from __future__ import annotations

import io
import json
import re
import tempfile
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import quote

import numpy as np
import pandas as pd
from fastapi import Depends, FastAPI, File, HTTPException, UploadFile, Query, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from scipy.interpolate import griddata, Rbf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sqlalchemy import String, cast, func, or_, select, text
from sqlalchemy.orm import Session

from coal_seam_blocks.aggregator import aggregate_boreholes, unify_columns
from api import calculate_key_strata_details, process_single_borehole_file
from coal_seam_blocks.modeling import build_block_models
from db import get_engine, get_records_table, get_session, reset_table_cache
from tunnel_support import TunnelSupportCalculator, batch_calculate_tunnel_support
from statistical_analysis import (
    analyze_descriptive_stats, analyze_correlation, analyze_regression,
    DescriptiveStatistics, CorrelationAnalysis, RegressionAnalysis, HypothesisTesting
)

# æ€§èƒ½ä¼˜åŒ–æ¨¡å—
from performance_config import (
    MAX_UPLOAD_SIZE_MB, MAX_RESOLUTION, CACHE_ENABLED,
    print_config_summary
)
from cache import cached, get_cache_stats, start_cache_cleanup_task, cache_database_query
from rate_limiter import RateLimitMiddleware, start_rate_limit_cleanup_task
from memory_utils import (
    optimize_dataframe_memory, check_memory_usage,
    memory_efficient_operation, clear_dataframe_cache, limit_dataframe_size
)

# ç®—æ³•ä¼˜åŒ–æ¨¡å—
from interpolation import get_interpolator, interpolate_smart
from data_validation import validate_geological_data, GeologicalDataValidator

APP_ROOT = Path(__file__).resolve().parent
DATA_DIR = APP_ROOT.parent / "data" / "input"
CHINA_JSON_CANDIDATES = [
    APP_ROOT.parent / "frontend" / "public" / "china.json",
    APP_ROOT.parent / "frontend" / "src" / "assets" / "china.json",
]

SEAM_COLUMN_CANDIDATES = ["ç…¤å±‚", "ç…¤å±‚åç§°", "å±‚ä½", "å²©å±‚", "å²©å±‚åç§°", "ç…¤å±‚å"]


class ModelingState:
    """In-memory storage for the geological modeling workflow."""

    def __init__(self) -> None:
        self.merged_df: Optional[pd.DataFrame] = None
        self.coords_df: Optional[pd.DataFrame] = None
        self.numeric_columns: List[str] = []
        self.text_columns: List[str] = []
        self.last_selected_seam_column: Optional[str] = None
        self.borehole_file_count: int = 0

    def ensure_loaded(self) -> None:
        if self.merged_df is None:
            raise HTTPException(status_code=400, detail="è¯·å…ˆä¸Šä¼ å¹¶åˆå¹¶é’»å­”ä¸åæ ‡æ•°æ®")


modeling_state = ModelingState()


class KeyStratumState:
    def __init__(self) -> None:
        self.files: Dict[str, pd.DataFrame] = {}
        self.filled: bool = False
        self.last_result: Optional[pd.DataFrame] = None

    def reset(self) -> None:
        self.files = {}
        self.filled = False
        self.last_result = None


key_stratum_state = KeyStratumState()


def _get_numeric_and_text_columns(df: pd.DataFrame) -> Dict[str, List[str]]:
    numeric_cols: List[str] = []
    text_cols: List[str] = []
    for col in df.columns:
        series = df[col]
        if pd.api.types.is_numeric_dtype(series):
            numeric_cols.append(col)
            continue
        # å°è¯•æ•°å­—åŒ–åˆ¤æ–­åˆ—ç±»å‹
        coerced = pd.to_numeric(series.dropna(), errors="coerce")
        if not coerced.empty and coerced.notna().mean() > 0.8:
            numeric_cols.append(col)
        else:
            text_cols.append(col)
    return {"numeric": numeric_cols, "text": text_cols}


def _get_records_table_or_500():
    """è·å– records è¡¨ï¼Œå¦‚æœå¤±è´¥åˆ™æŠ›å‡º HTTPException
    
    å·²åºŸå¼ƒï¼šå»ºè®®ç›´æ¥ä½¿ç”¨ get_records_table() å¹¶è‡ªè¡Œå¤„ç†å¼‚å¸¸
    """
    try:
        return get_records_table()
    except RuntimeError as exc:
        raise HTTPException(status_code=500, detail=str(exc)) from exc


def _get_records_table_safe():
    """å®‰å…¨è·å– records è¡¨ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    
    Returns:
        Table å¯¹è±¡æˆ– Noneï¼ˆå¦‚æœæ•°æ®åº“æœªåˆå§‹åŒ–ï¼‰
    """
    try:
        return get_records_table()
    except RuntimeError as e:
        print(f"[WARNING] è·å– records è¡¨å¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"[ERROR] è·å– records è¡¨æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
        return None


def _serialize_row(row: Dict[str, Any], columns: List[str]) -> Dict[str, Any]:
    payload = {}
    for column in columns:
        value = row.get(column)
        payload[column] = "" if value is None else value
    if "__rowid__" in row:
        payload["__rowid__"] = row["__rowid__"]
    return payload


def _build_optional_filters(table, search: Optional[str]):
    if not search:
        return None
    keyword = search.strip()
    if not keyword:
        return None
    pattern = f"%{keyword}%"
    conditions = []
    for column in table.columns:
        try:
            if column.type.python_type is str:
                conditions.append(column.ilike(pattern))
        except NotImplementedError:
            continue
    if not conditions:
        return None
    return or_(*conditions)


PROVINCE_COLUMN_CANDIDATES = [
    "çœä»½",
    "çœä»½åç§°",
    "æ‰€åœ¨çœä»½",
    "æ‰€å±çœä»½",
    "çœ",
    "çœå¸‚",
    "çœä»½/åœ°åŒº",
    "çœä»½(åœ°åŒº)",
    "è¡Œæ”¿åŒº",
    "ä»½",
]


DEFAULT_NUMERIC_COLUMNS = [
    "å¼¹æ€§æ¨¡é‡/GPa",
    "å®¹é‡/kNÂ·m-3",
    "æŠ—æ‹‰å¼ºåº¦/MPa",
    "æ³Šæ¾æ¯”",
    "å†…æ‘©æ“¦è§’",
    "ç²˜èšåŠ›/MPa",
]


def _resolve_column(column_map: Dict[str, Any], candidates: List[str]):
    for key in candidates:
        column = column_map.get(key)
        if column is not None:
            return column, key
    return None, None


def _normalize_province_name(name: str) -> str:
    if name is None:
        return ""
    value = str(name).strip()
    if not value:
        return ""

    replacements = {
        "å†…è’™å¤è‡ªæ²»åŒº": "å†…è’™å¤",
        "å¹¿è¥¿å£®æ—è‡ªæ²»åŒº": "å¹¿è¥¿",
        "å®å¤å›æ—è‡ªæ²»åŒº": "å®å¤",
        "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº": "æ–°ç–†",
        "è¥¿è—è‡ªæ²»åŒº": "è¥¿è—",
        "é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒº": "é¦™æ¸¯",
        "æ¾³é—¨ç‰¹åˆ«è¡Œæ”¿åŒº": "æ¾³é—¨",
        "é»‘é¾™æ±Ÿçœ": "é»‘é¾™æ±Ÿ",
    }
    if value in replacements:
        return replacements[value]

    suffixes = ["çœ", "å¸‚", "åœ°åŒº", "è‡ªæ²»åŒº", "ç‰¹åˆ«è¡Œæ”¿åŒº"]
    for suffix in suffixes:
        if value.endswith(suffix):
            value = value[: -len(suffix)]
            break
    return value.strip()


COAL_ALIASES = {"ç…¤"}
COAL_NORMALIZED_NAME = "ç…¤"


def _normalize_lithology_name(name: str) -> str:
    if name is None:
        return ""
    value = str(name).strip()
    if not value:
        return ""
    candidate = value.replace("ç…¤å±‚", COAL_NORMALIZED_NAME)
    simplified = re.sub(r"[0-9ï¼-ï¼™ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡ç‚¹Â·\-_/\\ï¼ˆï¼‰()\s]+", "", candidate)
    if simplified in COAL_ALIASES:
        return COAL_NORMALIZED_NAME
    return value


def _infer_numeric_columns(
    table,
    db_session: Session,
    exclude: Optional[set[str]] = None,
    sample_size: int = 200,
) -> List[str]:
    exclude_set = set(exclude or set())
    candidate_names: List[str] = [
        column.name
        for column in table.columns
        if column.name not in exclude_set
    ]
    if not candidate_names:
        return []

    stmt = select(*[table.c[name] for name in candidate_names]).limit(sample_size)
    rows = db_session.execute(stmt).all()
    if not rows:
        return []

    df = pd.DataFrame(rows, columns=candidate_names)
    inferred: List[str] = []
    for name in candidate_names:
        series = pd.to_numeric(df[name], errors="coerce")
        valid_ratio = series.notna().mean()
        if series.notna().sum() >= 3 and valid_ratio >= 0.5:
            inferred.append(name)

    if not inferred:
        return []

    preferred_order = {col: idx for idx, col in enumerate(DEFAULT_NUMERIC_COLUMNS)}
    inferred.sort(key=lambda col: preferred_order.get(col, len(preferred_order)))
    return inferred


def _load_china_geojson() -> Dict:
    for candidate in CHINA_JSON_CANDIDATES:
        if candidate.exists():
            with candidate.open("r", encoding="utf-8") as f:
                return json.load(f)
    raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°ä¸­å›½åœ°å›¾ JSON æ–‡ä»¶")


def _read_csv_bytes(data: bytes) -> pd.DataFrame:
    encodings = ["utf-8-sig", "utf-8", "gbk"]
    last_error: Optional[Exception] = None
    for enc in encodings:
        try:
            return pd.read_csv(io.BytesIO(data), encoding=enc)
        except Exception as exc:
            last_error = exc
            continue
    raise HTTPException(status_code=400, detail=f"CSVè¯»å–å¤±è´¥: {last_error}")


def _ensure_seam_column(df: pd.DataFrame) -> pd.DataFrame:
    for candidate in SEAM_COLUMN_CANDIDATES:
        if candidate in df.columns:
            if candidate != "ç…¤å±‚":
                df["ç…¤å±‚"] = df[candidate]
            break
    if "ç…¤å±‚" not in df.columns:
        df["ç…¤å±‚"] = ""
    return df


def _prepare_preview(dfs: Dict[str, pd.DataFrame], limit: int = 200) -> Dict[str, List[Dict]]:
    if not dfs:
        return {"columns": [], "rows": []}
    combined = pd.concat(list(dfs.values()), ignore_index=True, sort=False)
    combined = combined.fillna("")
    preview_df = combined.head(limit)
    columns = preview_df.columns.tolist()
    rows = json.loads(preview_df.to_json(orient="records", force_ascii=False))
    return {"columns": columns, "rows": rows}


def _fill_from_database(df: pd.DataFrame, stats_map: pd.DataFrame) -> pd.DataFrame:
    if stats_map.empty:
        return df
    df = df.copy()
    lithology_series = df.get("å²©å±‚åç§°")
    if lithology_series is None:
        return df
    numeric_cols = DEFAULT_NUMERIC_COLUMNS
    for idx, lith in lithology_series.items():
        key = str(lith).strip()
        if not key or key not in stats_map.index:
            continue
        stats_row = stats_map.loc[key]
        for col in numeric_cols:
            if col not in df.columns or col not in stats_row:
                continue
            current_value = df.at[idx, col]
            if pd.isna(current_value) or (isinstance(current_value, (int, float)) and float(current_value) == 0.0):
                df.at[idx, col] = stats_row[col]
    return df


def _normalize_key_columns(df: pd.DataFrame) -> pd.DataFrame:
    rename_map = {
        "å²©å±‚": "å²©å±‚åç§°",
        "å²©å±‚å": "å²©å±‚åç§°",
        "å±‚ä½": "å²©å±‚åç§°",
        "åç§°": "å²©å±‚åç§°",
        "åšåº¦": "åšåº¦/m",
        "åšåº¦m": "åšåº¦/m",
        "åšåº¦/M": "åšåº¦/m",
        "åšåº¦(ç±³)": "åšåº¦/m",
        "åšåº¦ï¼ˆmï¼‰": "åšåº¦/m",
        "å¼¹æ€§æ¨¡é‡": "å¼¹æ€§æ¨¡é‡/GPa",
        "å®¹é‡": "å®¹é‡/kNÂ·m-3",
        "æŠ—æ‹‰å¼ºåº¦": "æŠ—æ‹‰å¼ºåº¦/MPa",
    }
    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})
    return df


class ContourRequest(BaseModel):
    x_col: str
    y_col: str
    z_col: str
    method: str
    seams: Optional[List[str]] = None
    resolution: Optional[int] = 80


class BlockModelRequest(BaseModel):
    x_col: str
    y_col: str
    thickness_col: str
    seam_col: str
    selected_seams: List[str]
    method: str
    resolution: Optional[int] = 80
    base_level: Optional[float] = 0
    gap: Optional[float] = 0


class ExportRequest(BlockModelRequest):
    """å¯¼å‡ºè¯·æ±‚ï¼Œç»§æ‰¿è‡ª BlockModelRequest å¹¶å¢åŠ å¯¼å‡ºç±»å‹ã€æ–‡ä»¶åå’Œå¯¼å‡ºé€‰é¡¹å­—æ®µ"""
    export_type: str  # 'dxf' or 'flac3d'
    filename: Optional[str] = None
    options: Optional[Dict[str, Any]] = None  # å¯¼å‡ºé€‰é¡¹ï¼ˆå¦‚é™é‡‡æ ·å€æ•°ã€ä½“å—æ¨¡å¼ç­‰ï¼‰


class ComparisonRequest(BaseModel):
    x_col: str
    y_col: str
    z_col: str
    validation_ratio: float = 0.2
    seams: Optional[List[str]] = None


class ModelingValidationRequest(BaseModel):
    """å»ºæ¨¡å¯è¡Œæ€§éªŒè¯è¯·æ±‚"""
    x_col: str
    y_col: str
    thickness_col: str
    seam_col: str
    selected_seams: List[str]


app = FastAPI(title="Mining System API", version="0.1.0")

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ€§èƒ½ä¼˜åŒ–: è¯·æ±‚é™æµä¸­é—´ä»¶
rate_limit_middleware = RateLimitMiddleware(app, rate_per_minute=60)
app.add_middleware(RateLimitMiddleware, rate_per_minute=60)


# å¯åŠ¨äº‹ä»¶: åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–ç»„ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    print("\n" + "=" * 70)
    print("Mining System API å¯åŠ¨ä¸­...".center(70))
    print("=" * 70)

    # æ‰“å°æ€§èƒ½é…ç½®
    print_config_summary()

    # å¯åŠ¨ç¼“å­˜æ¸…ç†ä»»åŠ¡
    start_cache_cleanup_task()

    # å¯åŠ¨é™æµæ¸…ç†ä»»åŠ¡
    start_rate_limit_cleanup_task(rate_limit_middleware)

    # æ˜¾ç¤ºå†…å­˜çŠ¶æ€
    mem_usage = check_memory_usage()
    if "error" not in mem_usage:
        print(f"å½“å‰å†…å­˜ä½¿ç”¨: {mem_usage['process_mb']:.1f}MB")
        print(f"ç³»ç»Ÿæ€»å†…å­˜: {mem_usage['system_total_mb']:.1f}MB")
        print(f"ç³»ç»Ÿå¯ç”¨å†…å­˜: {mem_usage['system_available_mb']:.1f}MB")

    # æ£€æŸ¥å¹¶åˆå§‹åŒ–æ•°æ®åº“
    await check_and_init_database()

    print("=" * 70 + "\n")


async def check_and_init_database():
    """æ£€æŸ¥æ•°æ®åº“çŠ¶æ€ï¼Œå¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨å¯¼å…¥æ•°æ®"""
    try:
        from sqlalchemy import create_engine, inspect
        from db import DB_PATH, get_engine
        
        # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not DB_PATH.exists():
            print(f"âš ï¸  æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {DB_PATH}")
            print("ğŸ“¥ å°è¯•è‡ªåŠ¨åˆå§‹åŒ–æ•°æ®åº“...")
            await auto_import_csv()
            return
        
        # æ£€æŸ¥æ•°æ®åº“è¡¨å’Œæ•°æ®
        engine = get_engine()
        inspector = inspect(engine)
        
        if 'records' not in inspector.get_table_names():
            print("âš ï¸  æ•°æ®åº“è¡¨ä¸å­˜åœ¨")
            print("ğŸ“¥ å°è¯•è‡ªåŠ¨åˆå§‹åŒ–æ•°æ®åº“...")
            await auto_import_csv()
            return
        
        # æ£€æŸ¥è®°å½•æ•°
        with engine.connect() as conn:
            result = conn.execute(text('SELECT COUNT(*) FROM records'))
            count = result.fetchone()[0]
            
            if count == 0:
                print("âš ï¸  æ•°æ®åº“ä¸ºç©ºï¼ˆ0æ¡è®°å½•ï¼‰")
                print("ğŸ“¥ å°è¯•è‡ªåŠ¨åˆå§‹åŒ–æ•°æ®åº“...")
                await auto_import_csv()
            else:
                print(f"âœ“ æ•°æ®åº“å·²åŠ è½½ ({count} æ¡è®°å½•)")
                
    except Exception as e:
        print(f"âš ï¸  æ•°æ®åº“æ£€æŸ¥å¤±è´¥: {e}")
        print("ğŸ“¥ å°è¯•è‡ªåŠ¨åˆå§‹åŒ–æ•°æ®åº“...")
        await auto_import_csv()


async def auto_import_csv():
    """è‡ªåŠ¨å¯¼å…¥ CSV æ•°æ®åˆ°æ•°æ®åº“"""
    try:
        csv_path = APP_ROOT / "data" / "input" / "æ±‡æ€»è¡¨.csv"
        
        if not csv_path.exists():
            print(f"âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
            print(f"   æŸ¥æ‰¾è·¯å¾„: {csv_path}")
            print(f"   APP_ROOT: {APP_ROOT}")
            # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
            alternative_paths = [
                Path("/app/data/input/æ±‡æ€»è¡¨.csv"),
                APP_ROOT.parent / "data" / "input" / "æ±‡æ€»è¡¨.csv",
            ]
            for alt_path in alternative_paths:
                if alt_path.exists():
                    csv_path = alt_path
                    print(f"âœ“ åœ¨å¤‡ç”¨è·¯å¾„æ‰¾åˆ°: {csv_path}")
                    break
            else:
                return
        
        print(f"ğŸ“‚ æ‰¾åˆ° CSV æ–‡ä»¶: {csv_path}")
        print("ğŸ“Š å¼€å§‹å¯¼å…¥æ•°æ®...")
        
        # è¯»å– CSV
        df = pd.read_csv(csv_path, encoding='utf-8-sig')
        print(f"   è¯»å– {len(df)} æ¡è®°å½•")
        print(f"   åˆ—å: {list(df.columns)}")
        
        # å¯¼å…¥åˆ°æ•°æ®åº“
        from sqlalchemy import create_engine
        from db import DB_PATH
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        DB_PATH.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå¼•æ“å¹¶å¯¼å…¥
        engine = create_engine(f'sqlite:///{DB_PATH}')
        df.to_sql('records', engine, if_exists='replace', index=False)
        
        print(f"âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼å¯¼å…¥ {len(df)} æ¡è®°å½•åˆ° {DB_PATH}")
        
        # åˆ›å»ºç´¢å¼•ï¼ˆå°è¯•å¤šä¸ªå¯èƒ½çš„åˆ—åï¼‰
        try:
            with engine.connect() as conn:
                # è·å–å®é™…çš„åˆ—å
                actual_columns = df.columns.tolist()
                
                # ä¸ºå¸¸è§åˆ—åˆ›å»ºç´¢å¼•
                province_cols = ['çœä»½', 'Province', 'province', 'çœ', 'sheng']
                mine_cols = ['çŸ¿å', 'Mine', 'mine', 'çŸ¿', 'kuang']
                lithology_cols = ['å²©æ€§', 'Lithology', 'lithology', 'å²©', 'yan']
                
                for col_list, idx_name in [
                    (province_cols, 'idx_province'),
                    (mine_cols, 'idx_mine'),
                    (lithology_cols, 'idx_lithology')
                ]:
                    for col in col_list:
                        if col in actual_columns:
                            conn.execute(text(f'CREATE INDEX IF NOT EXISTS {idx_name} ON records ("{col}")'))
                            print(f"   åˆ›å»ºç´¢å¼•: {idx_name} on {col}")
                            break
                
                conn.commit()
        except Exception as idx_err:
            print(f"âš ï¸  åˆ›å»ºç´¢å¼•å¤±è´¥ï¼ˆä¸å½±å“ä½¿ç”¨ï¼‰: {idx_err}")
        
        # é‡ç½®è¡¨ç¼“å­˜
        reset_table_cache()
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶çš„æ¸…ç†"""
    print("\n[ç³»ç»Ÿ] æ­£åœ¨å…³é—­ï¼Œæ¸…ç†èµ„æº...")
    clear_dataframe_cache([modeling_state, key_stratum_state])
    print("[ç³»ç»Ÿ] èµ„æºæ¸…ç†å®Œæˆ\n")


@app.post("/api/modeling/columns")
async def load_modeling_columns(
    borehole_files: List[UploadFile] = File(..., description="å¤šä¸ªé’»å­”CSV"),
    coords_file: UploadFile = File(None, description="åæ ‡CSVï¼ˆå¯é€‰ï¼Œè‹¥æ•°æ®å·²åŒ…å«åæ ‡ï¼‰"),
    use_merged_data: bool = Form(False, description="æ˜¯å¦ä½¿ç”¨å·²åˆå¹¶çš„æ•°æ®"),
):
    if not borehole_files:
        raise HTTPException(status_code=400, detail="è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªé’»å­”æ–‡ä»¶")

    try:
        with tempfile.TemporaryDirectory() as tmp_dir:
            tmp_path = Path(tmp_dir)
            borehole_paths: List[str] = []
            
            # ä¿å­˜é’»å­”æ–‡ä»¶
            for idx, file in enumerate(borehole_files):
                try:
                    data = await file.read()
                    if not data:
                        raise ValueError(f"æ–‡ä»¶ {file.filename} ä¸ºç©º")
                    
                    # ä½¿ç”¨ç´¢å¼•é¿å…æ–‡ä»¶åå†²çª
                    filename = file.filename or f"borehole_{idx}.csv"
                    target = tmp_path / filename
                    target.write_bytes(data)
                    borehole_paths.append(str(target))
                    print(f"[DEBUG] ä¿å­˜é’»å­”æ–‡ä»¶: {filename}, å¤§å°: {len(data)} bytes")
                except Exception as e:
                    print(f"[ERROR] ä¿å­˜é’»å­”æ–‡ä»¶å¤±è´¥: {file.filename}, é”™è¯¯: {e}")
                    raise HTTPException(status_code=400, detail=f"ä¿å­˜é’»å­”æ–‡ä»¶å¤±è´¥: {file.filename} - {str(e)}")

            # æ ¹æ®æ˜¯å¦ä½¿ç”¨å·²åˆå¹¶æ•°æ®å†³å®šå¤„ç†æ–¹å¼
            if use_merged_data:
                # å…¨å±€æ•°æ®æ¨¡å¼ï¼šæ•°æ®å·²åŒ…å«åæ ‡ä¿¡æ¯å’Œé’»å­”åï¼Œç›´æ¥åŠ è½½å³å¯
                print(f"[DEBUG] ========== å…¨å±€æ•°æ®æ¨¡å¼ ==========")
                
                # åŠ è½½æ‰€æœ‰é’»å­”æ–‡ä»¶å¹¶åˆå¹¶
                from coal_seam_blocks.aggregator import load_borehole_csv, unify_columns
                merged_frames = []
                for file_path in borehole_paths:
                    df = load_borehole_csv(file_path)
                    df = unify_columns(df)
                    merged_frames.append(df)
                
                merged_df = pd.concat(merged_frames, ignore_index=True)
                print(f"[DEBUG] å…¨å±€æ•°æ®åŠ è½½æˆåŠŸï¼Œè®°å½•æ•°: {len(merged_df)}")
                print(f"[DEBUG] æ•°æ®åˆ—: {list(merged_df.columns)}")
                
                # æ£€æŸ¥é’»å­”ååˆ†å¸ƒ
                if "é’»å­”å" in merged_df.columns:
                    unique_boreholes = merged_df["é’»å­”å"].nunique()
                    print(f"[DEBUG] åŒ…å« {unique_boreholes} ä¸ªä¸åŒçš„é’»å­”")
                    sample_boreholes = merged_df["é’»å­”å"].unique()[:5].tolist()
                    print(f"[DEBUG] é’»å­”åæ ·æœ¬: {sample_boreholes}")
                
                # éªŒè¯æ•°æ®ä¸­æ˜¯å¦åŒ…å«åæ ‡åˆ—
                coord_candidates = ['X', 'x', 'Xåæ ‡', 'xåæ ‡', 'Y', 'y', 'Yåæ ‡', 'yåæ ‡']
                found_coords = [col for col in merged_df.columns if any(cand in col for cand in coord_candidates)]
                
                if len(found_coords) < 2:
                    raise HTTPException(
                        status_code=400, 
                        detail=f"æ•°æ®ä¸­æœªæ‰¾åˆ°è¶³å¤Ÿçš„åæ ‡åˆ—ã€‚æ‰¾åˆ°: {found_coords}ï¼Œéœ€è¦è‡³å°‘2ä¸ªåæ ‡åˆ—ï¼ˆXå’ŒYï¼‰"
                    )
                
                coords_df = None  # å·²åˆå¹¶æ•°æ®ä¸éœ€è¦å•ç‹¬çš„åæ ‡æ–‡ä»¶
                
            else:
                # ä¼ ç»Ÿæ¨¡å¼ï¼šéœ€è¦åæ ‡æ–‡ä»¶è¿›è¡Œåˆå¹¶
                if not coords_file:
                    raise HTTPException(status_code=400, detail="æœªä½¿ç”¨å·²åˆå¹¶æ•°æ®æ—¶ï¼Œå¿…é¡»æä¾›åæ ‡æ–‡ä»¶")
                
                # ä¿å­˜åæ ‡æ–‡ä»¶
                try:
                    coords_data = await coords_file.read()
                    if not coords_data:
                        raise ValueError("åæ ‡æ–‡ä»¶ä¸ºç©º")
                    coords_path = tmp_path / (coords_file.filename or "coordinates.csv")
                    coords_path.write_bytes(coords_data)
                    print(f"[DEBUG] ä¿å­˜åæ ‡æ–‡ä»¶: {coords_file.filename}, å¤§å°: {len(coords_data)} bytes")
                except Exception as e:
                    print(f"[ERROR] ä¿å­˜åæ ‡æ–‡ä»¶å¤±è´¥: {e}")
                    raise HTTPException(status_code=400, detail=f"ä¿å­˜åæ ‡æ–‡ä»¶å¤±è´¥: {str(e)}")

                # èšåˆæ•°æ®
                try:
                    print(f"[DEBUG] å¼€å§‹èšåˆæ•°æ®ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰ï¼Œé’»å­”æ–‡ä»¶æ•°: {len(borehole_paths)}")
                    
                    # å…ˆåŠ è½½é’»å­”æ•°æ®ç»Ÿè®¡
                    from coal_seam_blocks.aggregator import load_borehole_csv, unify_columns
                    total_borehole_records = 0
                    for path in borehole_paths:
                        df_temp = load_borehole_csv(path)
                        total_borehole_records += len(df_temp)
                    print(f"[DEBUG] é’»å­”æ–‡ä»¶æ€»è®°å½•æ•°: {total_borehole_records}")
                    
                    # åŠ è½½åæ ‡æ–‡ä»¶ç»Ÿè®¡
                    coords_temp = load_borehole_csv(str(coords_path))
                    print(f"[DEBUG] åæ ‡æ–‡ä»¶è®°å½•æ•°: {len(coords_temp)}")
                    print(f"[DEBUG] åæ ‡æ–‡ä»¶åˆ—: {list(coords_temp.columns)}")
                    
                    # æ‰§è¡Œèšåˆ
                    merged_df, coords_df = aggregate_boreholes(borehole_paths, str(coords_path))
                    print(f"[DEBUG] æ•°æ®èšåˆæˆåŠŸï¼Œåˆå¹¶åè®°å½•æ•°: {len(merged_df)}")
                    
                    # è®¡ç®—æ•°æ®æŸå¤±
                    if total_borehole_records > len(merged_df):
                        lost_records = total_borehole_records - len(merged_df)
                        loss_percent = (lost_records / total_borehole_records) * 100
                        print(f"[WARNING] inner join å¯¼è‡´æ•°æ®ä¸¢å¤±: {lost_records} æ¡ ({loss_percent:.1f}%)")
                        print(f"[WARNING] è¿™å¯èƒ½æ˜¯å› ä¸ºé’»å­”æ–‡ä»¶å’Œåæ ‡æ–‡ä»¶çš„é’»å­”åä¸åŒ¹é…")
                    
                except Exception as e:
                    print(f"[ERROR] æ•°æ®èšåˆå¤±è´¥: {e}")
                    raise HTTPException(status_code=400, detail=f"æ•°æ®èšåˆå¤±è´¥: {str(e)}")

        # æ•°æ®ä¸€è‡´æ€§éªŒè¯å’Œç»Ÿè®¡
        print(f"[DEBUG] ========== æ•°æ®åŠ è½½æ‘˜è¦ ==========")
        print(f"[DEBUG] æ•°æ®æ¨¡å¼: {'å…¨å±€æ•°æ®ï¼ˆå·²åˆå¹¶ï¼‰' if use_merged_data else 'ä¸Šä¼ æ–‡ä»¶ï¼ˆéœ€åˆå¹¶ï¼‰'}")
        print(f"[DEBUG] æœ€ç»ˆè®°å½•æ•°: {len(merged_df)}")
        print(f"[DEBUG] æœ€ç»ˆåˆ—æ•°: {len(merged_df.columns)}")
        
        # æ£€æŸ¥å…³é”®åˆ—
        required_cols = ["é’»å­”å"]
        missing_cols = [col for col in required_cols if col not in merged_df.columns]
        if missing_cols:
            print(f"[WARNING] æ•°æ®ç¼ºå°‘å…³é”®åˆ—: {missing_cols}")
        
        # æ£€æŸ¥é’»å­”åˆ†å¸ƒï¼ˆç”¨äºå¯¹æ¯”ä¸¤ç§æ¨¡å¼ï¼‰
        if "é’»å­”å" in merged_df.columns:
            unique_boreholes = merged_df["é’»å­”å"].nunique()
            print(f"[DEBUG] æœ€ç»ˆæ•°æ®åŒ…å« {unique_boreholes} ä¸ªä¸åŒçš„é’»å­”")
        
        modeling_state.merged_df = merged_df
        modeling_state.coords_df = coords_df
        modeling_state.borehole_file_count = len(borehole_files)
        columns_info = _get_numeric_and_text_columns(merged_df)
        modeling_state.numeric_columns = columns_info["numeric"]
        modeling_state.text_columns = columns_info["text"]
        
        print(f"[DEBUG] æ•°å€¼åˆ— ({len(modeling_state.numeric_columns)}): {modeling_state.numeric_columns[:5]}...")
        print(f"[DEBUG] æ–‡æœ¬åˆ— ({len(modeling_state.text_columns)}): {modeling_state.text_columns[:5]}...")
        print(f"[DEBUG] ====================================")
        
        # å¦‚æœç¼ºå°‘å…³é”®åˆ—ï¼Œç»™å‡ºè­¦å‘Šä½†ä¸é˜»æ­¢
        if missing_cols:
            print(f"[WARNING] æ•°æ®ç»“æ„å¯èƒ½ä¸å®Œæ•´ï¼Œå»ºæ¨¡ç»“æœå¯èƒ½å—å½±å“")
        
        # è¾“å‡ºæ•°æ®æ ·æœ¬
        if len(merged_df) > 0:
            sample_cols = ['é’»å­”å'] + [col for col in merged_df.columns if 'X' in col or 'x' in col or 'Y' in col or 'y' in col][:4]
            sample_cols = [col for col in sample_cols if col in merged_df.columns]
            if sample_cols:
                print(f"[DEBUG] æ•°æ®æ ·æœ¬ (å‰3è¡Œ):")
                print(merged_df[sample_cols].head(3).to_string())
        print(f"[DEBUG] ====================================")

        return {
            "status": "success",
            "numeric_columns": modeling_state.numeric_columns,
            "text_columns": modeling_state.text_columns,
            "record_count": int(len(merged_df)),
            "data_mode": "merged" if use_merged_data else "traditional",
        }
    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] load_modeling_columns å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨é”™è¯¯: {str(e)}")


@app.get("/api/modeling/seams")
async def get_unique_seams(column: str = Query(..., description="å²©å±‚åˆ—å")):
    modeling_state.ensure_loaded()
    if column not in modeling_state.merged_df.columns:
        raise HTTPException(status_code=404, detail=f"åœ¨åˆå¹¶æ•°æ®ä¸­æœªæ‰¾åˆ°åˆ—: {column}")
    
    df = modeling_state.merged_df
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åºå·åˆ—ï¼ˆç”¨äºç¡®å®šåœ°å±‚é¡ºåºï¼‰
    sequence_col = None
    for possible_col in ['åºå·', 'åºå·(ä»ä¸‹åˆ°ä¸Š)', 'å±‚åº', 'ç¼–å·', 'sequence', 'order']:
        if possible_col in df.columns:
            sequence_col = possible_col
            break
    
    # è·å–å”¯ä¸€çš„å²©å±‚åç§°
    values = (
        df[column]
        .dropna()
        .astype(str)
        .map(str.strip)
        .replace({"nan": ""})
    )
    unique_values = [v for v in values.unique() if v]
    
    # å¦‚æœæœ‰åºå·åˆ—ï¼ŒæŒ‰åºå·æ’åºï¼ˆä»å°åˆ°å¤§ï¼Œå³ä»ä¸‹åˆ°ä¸Šï¼‰
    if sequence_col:
        try:
            # ä¸ºæ¯ä¸ªå²©å±‚æ‰¾åˆ°æœ€å°çš„åºå·ï¼ˆä»£è¡¨è¯¥å²©å±‚æœ€åº•éƒ¨çš„ä½ç½®ï¼‰
            seam_order = {}
            for seam in unique_values:
                seam_data = df[df[column].astype(str).str.strip() == seam]
                if not seam_data.empty and sequence_col in seam_data.columns:
                    # ä½¿ç”¨æœ€å°åºå·ï¼ˆæœ€åº•å±‚ï¼‰ä½œä¸ºè¯¥å²©å±‚çš„æ’åºä¾æ®
                    min_seq = pd.to_numeric(seam_data[sequence_col], errors='coerce').min()
                    if pd.notna(min_seq):
                        seam_order[seam] = min_seq
            
            # æŒ‰åºå·æ’åº
            if seam_order:
                unique_values = sorted(unique_values, key=lambda x: seam_order.get(x, float('inf')))
                print(f"[DEBUG] æŒ‰åºå·åˆ—'{sequence_col}'æ’åºå²©å±‚: {unique_values}")
            else:
                # å¦‚æœæ— æ³•æå–æœ‰æ•ˆåºå·ï¼Œå›é€€åˆ°å­—æ¯æ’åº
                unique_values = sorted(unique_values)
                print(f"[DEBUG] åºå·åˆ—'{sequence_col}'æ— æœ‰æ•ˆæ•°æ®ï¼Œä½¿ç”¨å­—æ¯æ’åº")
        except Exception as e:
            print(f"[WARNING] æŒ‰åºå·æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨å­—æ¯æ’åº")
            unique_values = sorted(unique_values)
    else:
        # æ²¡æœ‰åºå·åˆ—ï¼Œä½¿ç”¨å­—æ¯æ’åº
        unique_values = sorted(unique_values)
        print(f"[DEBUG] æœªæ‰¾åˆ°åºå·åˆ—ï¼Œä½¿ç”¨å­—æ¯æ’åº: {unique_values}")
    
    modeling_state.last_selected_seam_column = column
    return {"status": "success", "values": unique_values}


def _filter_dataframe_by_seams(df: pd.DataFrame, seams: Optional[List[str]], seam_column: Optional[str]) -> pd.DataFrame:
    if not seams or not seam_column or seam_column not in df.columns:
        return df
    seam_set = {str(s) for s in seams}
    return df[df[seam_column].astype(str).isin(seam_set)]


@app.post("/api/modeling/contour")
async def generate_contour(data: ContourRequest):
    modeling_state.ensure_loaded()
    df = modeling_state.merged_df.copy()
    
    print(f"[CONTOUR] ========== ç­‰å€¼çº¿ç”Ÿæˆå¼€å§‹ ==========")
    print(f"[CONTOUR] åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
    print(f"[CONTOUR] è¯·æ±‚åˆ—: X={data.x_col}, Y={data.y_col}, Z={data.z_col}")
    print(f"[CONTOUR] å²©å±‚è¿‡æ»¤: {data.seams}")

    seam_col = modeling_state.last_selected_seam_column
    df = _filter_dataframe_by_seams(df, data.seams, seam_col)
    print(f"[CONTOUR] è¿‡æ»¤åæ•°æ®: {len(df)} æ¡è®°å½•")

    if df.empty:
        raise HTTPException(status_code=400, detail="è¿‡æ»¤åçš„æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆç­‰å€¼çº¿")

    required_cols = [data.x_col, data.y_col, data.z_col]
    if not all(col in df.columns for col in required_cols):
        missing = [col for col in required_cols if col not in df.columns]
        raise HTTPException(status_code=404, detail=f"æ•°æ®é›†ä¸­ç¼ºå°‘åˆ—: {missing}")

    subset = df.dropna(subset=required_cols).copy()
    if subset.empty:
        raise HTTPException(status_code=400, detail="æœ‰æ•ˆæ•°æ®ç‚¹ä¸è¶³ä»¥è¿›è¡Œæ’å€¼")
    
    print(f"[CONTOUR] å»é™¤NAå: {len(subset)} æ¡è®°å½•")

    x = pd.to_numeric(subset[data.x_col], errors="coerce").dropna()
    y = pd.to_numeric(subset[data.y_col], errors="coerce").dropna()
    z = pd.to_numeric(subset[data.z_col], errors="coerce").dropna()
    valid_length = min(len(x), len(y), len(z))
    
    print(f"[CONTOUR] æœ‰æ•ˆæ•°æ®ç‚¹: {valid_length}")
    print(f"[CONTOUR] XèŒƒå›´: [{x.min():.2f}, {x.max():.2f}]")
    print(f"[CONTOUR] YèŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
    print(f"[CONTOUR] ZèŒƒå›´: [{z.min():.2f}, {z.max():.2f}]")
    
    if valid_length < 4:
        raise HTTPException(status_code=400, detail="è‡³å°‘éœ€è¦4ä¸ªæœ‰æ•ˆæ•°æ®ç‚¹æ¥ç”Ÿæˆç­‰å€¼çº¿")

    x = x.iloc[:valid_length]
    y = y.iloc[:valid_length]
    z = z.iloc[:valid_length]

    resolution = max(int(data.resolution or 80), 20)
    xi = np.linspace(x.min(), x.max(), resolution)
    yi = np.linspace(y.min(), y.max(), resolution)
    XI, YI = np.meshgrid(xi, yi)
    
    # ä½¿ç”¨å¢å¼ºçš„æ’å€¼æ¨¡å—
    try:
        from interpolation import interpolate
        method = data.method.lower()
        zi = interpolate(x.values, y.values, z.values, XI, YI, method)
    except Exception as e:
        print(f"[WARNING] æ’å€¼å¤±è´¥: {e}, ä½¿ç”¨çº¿æ€§æ’å€¼")
        try:
            zi = griddata((x, y), z, (XI, YI), method="linear")
        except Exception:
            zi = griddata((x, y), z, (XI, YI), method="nearest")

    zi = np.nan_to_num(zi, nan=float(np.nanmean(z)))

    return {
        "status": "success",
        "grid": {
            "x": xi.tolist(),
            "y": yi.tolist(),
            "z": zi.tolist(),
        },
        "points": {
            "x": x.tolist(),
            "y": y.tolist(),
            "z": z.tolist(),
        },
    }


@app.post("/api/modeling/block_model")
async def generate_block_model(payload: BlockModelRequest):
    modeling_state.ensure_loaded()
    df = modeling_state.merged_df
    
    print(f"[3D_MODEL] ========== 3Då»ºæ¨¡å¼€å§‹ ==========")
    print(f"[3D_MODEL] åŸå§‹æ•°æ®: {len(df)} æ¡è®°å½•")
    print(f"[3D_MODEL] è¯·æ±‚å‚æ•°:")
    print(f"[3D_MODEL]   Xåˆ—: {payload.x_col}")
    print(f"[3D_MODEL]   Yåˆ—: {payload.y_col}")
    print(f"[3D_MODEL]   åšåº¦åˆ—: {payload.thickness_col}")
    print(f"[3D_MODEL]   å²©å±‚åˆ—: {payload.seam_col}")
    print(f"[3D_MODEL]   é€‰æ‹©å²©å±‚: {payload.selected_seams}")
    print(f"[3D_MODEL]   æ’å€¼æ–¹æ³•: {payload.method}")
    print(f"[3D_MODEL]   åˆ†è¾¨ç‡: {payload.resolution}")
    print(f"[3D_MODEL]   åŸºåº•é«˜ç¨‹: {payload.base_level}")
    print(f"[3D_MODEL]   å±‚é—´é—´éš”: {payload.gap}")

    for col in [payload.x_col, payload.y_col, payload.thickness_col, payload.seam_col]:
        if col not in df.columns:
            raise HTTPException(status_code=404, detail=f"æ•°æ®é›†ä¸­ç¼ºå°‘åˆ—: {col}")
    
    # è¾“å‡ºæ¯ä¸ªé€‰æ‹©å²©å±‚çš„æ•°æ®ç‚¹æ•°
    for seam in payload.selected_seams:
        seam_data = df[df[payload.seam_col].astype(str) == str(seam)]
        print(f"[3D_MODEL] å²©å±‚ '{seam}': {len(seam_data)} æ¡è®°å½•")

    def interpolation_wrapper(x, y, z, xi_flat, yi_flat):
        """æ™ºèƒ½æ’å€¼åŒ…è£…å‡½æ•°,ä½¿ç”¨å¢å¼ºçš„interpolationæ¨¡å—"""
        from interpolation import interpolate
        
        num_points = len(x)
        original_method = payload.method.lower()
        method_key = original_method
        
        print(f"[INTERP] ğŸ”§ æ’å€¼è°ƒç”¨: æ•°æ®ç‚¹={num_points}, è¯·æ±‚æ–¹æ³•={original_method}")
        
        # æ•°æ®éªŒè¯
        if num_points <= 3:
            print(f"[INTERP] âš ï¸ æ•°æ®ç‚¹å¤ªå°‘ ({num_points}), å¼ºåˆ¶ä½¿ç”¨ nearest")
            method_key = 'nearest'
        
        # æ£€æŸ¥ç‚¹æ˜¯å¦å…±çº¿æˆ–æ¥è¿‘å…±çº¿
        if num_points >= 3:
            try:
                x_range = np.max(x) - np.min(x)
                y_range = np.max(y) - np.min(y)
                
                print(f"[INTERP] ğŸ“Š æ•°æ®åˆ†å¸ƒ: XèŒƒå›´={x_range:.2f}m, YèŒƒå›´={y_range:.2f}m")
                
                # å¦‚æœç‚¹åœ¨ä¸€æ¡çº¿ä¸Š(æŸä¸ªæ–¹å‘çš„èŒƒå›´éå¸¸å°)
                if x_range < 1e-6 or y_range < 1e-6:
                    print(f"[INTERP] âš ï¸ æ•°æ®ç‚¹æ¥è¿‘å…±çº¿, å¼ºåˆ¶ä½¿ç”¨ nearest")
                    method_key = 'nearest'
            except Exception as e:
                print(f"[INTERP] âš ï¸ æ•°æ®èŒƒå›´æ£€æŸ¥å¤±è´¥: {e}")
        
        if method_key != original_method:
            print(f"[INTERP] ğŸ”„ æ–¹æ³•å·²æ”¹å˜: {original_method} â†’ {method_key}")
        else:
            print(f"[INTERP] âœ… ä½¿ç”¨è¯·æ±‚çš„æ–¹æ³•: {method_key}")
        
        # ä½¿ç”¨å¢å¼ºçš„æ’å€¼æ¨¡å—æ‰§è¡Œæ’å€¼
        try:
            result = interpolate(x, y, z, xi_flat, yi_flat, method_key)
            
            # âš ï¸ å¤„ç†NaN/Infå€¼ - ä¸èƒ½è½¬ä¸º0,ä¼šå¯¼è‡´åšåº¦ä¸º0!
            if isinstance(result, np.ndarray):
                invalid_mask = ~np.isfinite(result)
                invalid_count = np.sum(invalid_mask)
                if invalid_count > 0:
                    print(f"[INTERP] ğŸ”§ å¤„ç†äº† {invalid_count} ä¸ªæ— æ•ˆå€¼(NaN/Inf)")
                    # ç”¨åŸå§‹æ•°æ®çš„ä¸­ä½æ•°å¡«å……,è€Œé0
                    fill_value = float(np.median(z)) if len(z) > 0 else 0.0
                    result = np.where(np.isfinite(result), result, fill_value)
                    print(f"[INTERP] ğŸ“Š å¡«å……å€¼: {fill_value:.2f} (æ•°æ®ä¸­ä½æ•°)")
            
            print(f"[INTERP] âœ… æ’å€¼å®Œæˆ: ç»“æœå½¢çŠ¶={result.shape}")
            return result
            
        except Exception as e:
            print(f"[INTERP] âŒ æ’å€¼å¤±è´¥: {method_key} â†’ {str(e)[:100]}")
            print(f"[INTERP] ğŸ”„ å›é€€åˆ° nearest")
            try:
                result = griddata((x, y), z, (xi_flat, yi_flat), method='nearest')
                # ç”¨ä¸­ä½æ•°å¡«å……æ— æ•ˆå€¼
                fill_value = float(np.median(z)) if len(z) > 0 else 0.0
                result = np.where(np.isfinite(result), result, fill_value)
                return result
            except Exception as fallback_error:
                print(f"[INTERP] âŒ nearest ä¹Ÿå¤±è´¥: {fallback_error}")
                # è¿”å›é›¶æ•°ç»„ä½œä¸ºæœ€åçš„å›é€€
                return np.zeros_like(xi_flat)

    try:
        block_models, skipped, (XI, YI) = build_block_models(
            merged_df=df,
            seam_column=payload.seam_col,
            x_col=payload.x_col,
            y_col=payload.y_col,
            thickness_col=payload.thickness_col,
            selected_seams=payload.selected_seams,
            method_callable=interpolation_wrapper,
            resolution=int(payload.resolution or 80),
            base_level=float(payload.base_level or 0.0),
            gap_value=float(payload.gap or 0.0),
        )
        print(f"[DEBUG] å—ä½“å»ºæ¨¡å®Œæˆ: æˆåŠŸ {len(block_models)} ä¸ª, è·³è¿‡ {len(skipped)} ä¸ª")
        print(f"[DEBUG] ç½‘æ ¼å°ºå¯¸: XI.shape={XI.shape}, YI.shape={YI.shape}")
        if skipped:
            print(f"[DEBUG] è·³è¿‡çš„å²©å±‚: {skipped}")
    except Exception as exc:
        print(f"[ERROR] å—ä½“å»ºæ¨¡å¤±è´¥: {exc}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=400, detail=str(exc))

    models_payload = []
    print(f"[DEBUG] å‡†å¤‡è½¬æ¢ {len(block_models)} ä¸ªæ¨¡å‹æ•°æ®...")
    print(f"[DEBUG] ç½‘æ ¼ç»´åº¦: {XI.shape[0]} x {XI.shape[1]} = {XI.shape[0] * XI.shape[1]} ä¸ªç‚¹")
    
    for model in block_models:
        # æ–¹æ¡ˆ1: ä½¿ç”¨äºŒç»´æ•°ç»„æ ¼å¼ (parametric surface)
        # echarts-gl çš„ surface å¯ä»¥æ¥å— data: { type: 'xyz', value: [x_arr, y_arr, z_arr] }
        # æˆ–è€… data: [[x,y,z], ...] æ ¼å¼
        
        # æå–ç½‘æ ¼çš„ x, y åæ ‡(åªéœ€è¦ä¸€æ¬¡)
        x_grid = XI[0, :].tolist()  # X åæ ‡æ•°ç»„ (ç¬¬ä¸€è¡Œ)
        y_grid = YI[:, 0].tolist()  # Y åæ ‡æ•°ç»„ (ç¬¬ä¸€åˆ—)
        
        # Z å€¼ä»¥äºŒç»´æ•°ç»„å½¢å¼æä¾›
        z_top = model.top_surface.tolist()
        z_bottom = model.bottom_surface.tolist()
        
        print(f"[DEBUG] å²©å±‚ '{model.name}': xç»´åº¦={len(x_grid)}, yç»´åº¦={len(y_grid)}, zç»´åº¦={len(z_top)}x{len(z_top[0]) if z_top else 0}")
        
        models_payload.append(
            {
                "name": model.name,
                "points": int(model.points),
                # ä½¿ç”¨ parametric æ ¼å¼: åˆ†åˆ«æä¾› x, y, z æ•°ç»„
                "grid_x": x_grid,
                "grid_y": y_grid,
                "top_surface_z": z_top,
                "bottom_surface_z": z_bottom,
                "avg_thickness": float(model.avg_thickness),
                "max_thickness": float(model.max_thickness),
                "avg_height": float(model.avg_height),
            }
        )
        
        # æ‰“å°ç¬¬ä¸€ä¸ªæ¨¡å‹çš„æ•°æ®æ ·æœ¬ç”¨äºè°ƒè¯•
        if len(models_payload) == 1:
            print(f"[DEBUG] ç¬¬ä¸€ä¸ªæ¨¡å‹æ•°æ®æ ·æœ¬:")
            print(f"  - x_gridå‰3ä¸ªå€¼: {x_grid[:3]}")
            print(f"  - y_gridå‰3ä¸ªå€¼: {y_grid[:3]}")
            print(f"  - z_top[0]å‰3ä¸ªå€¼: {z_top[0][:3] if z_top and z_top[0] else 'N/A'}")

    return {
        "status": "success",
        "grid": {"x": XI[0].tolist(), "y": YI[:, 0].tolist()},
        "models": models_payload,
        "skipped": skipped,
    }


@app.post("/api/modeling/comparison")
async def compare_interpolation(payload: ComparisonRequest):
    modeling_state.ensure_loaded()
    df = modeling_state.merged_df.copy()
    seam_col = modeling_state.last_selected_seam_column
    df = _filter_dataframe_by_seams(df, payload.seams, seam_col)

    required_cols = [payload.x_col, payload.y_col, payload.z_col]
    if not all(col in df.columns for col in required_cols):
        missing = [col for col in required_cols if col not in df.columns]
        raise HTTPException(status_code=404, detail=f"æ•°æ®é›†ä¸­ç¼ºå°‘åˆ—: {missing}")

    df = df.dropna(subset=required_cols)
    if len(df) < 12:
        raise HTTPException(status_code=400, detail="æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæ’å€¼å¯¹æ¯”")

    X = df[[payload.x_col, payload.y_col]].apply(pd.to_numeric, errors="coerce").dropna()
    y = pd.to_numeric(df[payload.z_col], errors="coerce")
    y = y.loc[X.index]

    if len(X) < 12:
        raise HTTPException(status_code=400, detail="æœ‰æ•ˆæ•°æ®ç‚¹ä¸è¶³ä»¥è¿›è¡Œæ’å€¼å¯¹æ¯”")

    X_train, X_test, y_train, y_test = train_test_split(
        X.values,
        y.values,
        test_size=min(max(payload.validation_ratio, 0.1), 0.5),
        random_state=42,
    )

    methods = {
        "linear": "çº¿æ€§ (Linear)",
        "cubic": "ä¸‰æ¬¡æ ·æ¡ (Cubic)",
        "nearest": "æœ€è¿‘é‚» (Nearest)",
        "multiquadric": "å¤šé‡äºŒæ¬¡ (Multiquadric)",
        "inverse": "åè·ç¦» (Inverse)",
        "gaussian": "é«˜æ–¯ (Gaussian)",
        "thin_plate": "è–„æ¿æ ·æ¡ (Thin Plate)",
    }

    results = []
    for key, label in methods.items():
        try:
            if key in {"linear", "cubic", "nearest"}:
                preds = griddata(
                    (X_train[:, 0], X_train[:, 1]),
                    y_train,
                    (X_test[:, 0], X_test[:, 1]),
                    method=key,
                )
            else:
                rbf = Rbf(X_train[:, 0], X_train[:, 1], y_train, function=key)
                preds = rbf(X_test[:, 0], X_test[:, 1])
            if preds is None:
                continue
            mask = ~np.isnan(preds)
            if not np.any(mask):
                continue
            mae = float(mean_absolute_error(y_test[mask], preds[mask]))
            rmse = float(np.sqrt(mean_squared_error(y_test[mask], preds[mask])))
            r2 = float(r2_score(y_test[mask], preds[mask]))
            results.append({"method": label, "mae": round(mae, 4), "rmse": round(rmse, 4), "r2": round(r2, 4)})
        except Exception:
            continue

    if not results:
        raise HTTPException(status_code=400, detail="æ‰€æœ‰æ’å€¼æ–¹æ³•éƒ½è®¡ç®—å¤±è´¥")

    results.sort(key=lambda item: item["r2"], reverse=True)
    return {"status": "success", "results": results}


@app.get("/api/database/overview")
async def get_database_overview(
    limit: int = Query(40, ge=1, le=200),
    db: Session = Depends(get_session),
):
    """è·å–æ•°æ®åº“æ¦‚è§ˆ (å¸¦é”™è¯¯å¤„ç†)"""
    try:
        # å°è¯•è·å–æ•°æ®åº“è¡¨
        try:
            table = get_records_table()
        except RuntimeError as e:
            # æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºæ•°æ®
            print(f"[WARNING] æ•°æ®åº“æœªåˆå§‹åŒ–: {e}")
            return {
                "status": "success",
                "stats": {
                    "records": 0,
                    "provinces": 0,
                    "mines": 0,
                    "lithologies": 0,
                },
                "distribution": [],
                "message": "æ•°æ®åº“å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆå¯¼å…¥æ•°æ®"
            }
        
        column_map = {column.name: column for column in table.columns}

        stats = {
            "records": int(db.execute(select(func.count()).select_from(table)).scalar() or 0),
            "provinces": 0,
            "mines": 0,
            "lithologies": 0,
        }

        province_column, _ = _resolve_column(column_map, PROVINCE_COLUMN_CANDIDATES)
        if "çŸ¿å" in column_map:
            stats["mines"] = int(
                db.execute(select(func.count(func.distinct(column_map["çŸ¿å"])))).scalar() or 0
            )
        if "å²©æ€§" in column_map:
            distinct_stmt = (
                select(column_map["å²©æ€§"])
                .where(column_map["å²©æ€§"].is_not(None))
                .distinct()
            )
            normalized_set: set[str] = set()
            for row in db.execute(distinct_stmt):
                raw_value = row[0]
                normalized_value = _normalize_lithology_name(raw_value)
                if normalized_value:
                    normalized_set.add(normalized_value)
            stats["lithologies"] = len(normalized_set)

        distribution: List[Dict[str, Any]] = []
        if province_column is not None:
            province_stmt = (
                select(
                    province_column.label("raw_name"),
                    func.count().label("value"),
                )
                .where(province_column.is_not(None))
                .group_by(province_column)
                .order_by(func.count().desc())
            )
            province_rows = db.execute(province_stmt).all()
            unique_normalized: set[str] = set()
            for row in province_rows:
                raw_name = str(row.raw_name).strip() if row.raw_name is not None else ""
                normalized = _normalize_province_name(raw_name) or "æœªçŸ¥"
                unique_normalized.add(normalized if normalized != "æœªçŸ¥" else "")
                if len(distribution) < limit:
                    distribution.append(
                        {
                            "name": normalized,
                            "label": raw_name or normalized,
                            "value": int(row.value or 0),
                        }
                    )

            stats["provinces"] = len({name for name in unique_normalized if name})

        return {"status": "success", "stats": stats, "distribution": distribution}
    
    except Exception as e:
        print(f"[ERROR] get_database_overview å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        # è¿”å›ç©ºæ•°æ®è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return {
            "status": "success",
            "stats": {
                "records": 0,
                "provinces": 0,
                "mines": 0,
                "lithologies": 0,
            },
            "distribution": [],
            "message": "è·å–æ•°æ®åº“ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯"
        }


@app.get("/api/database/records")
async def get_database_records(
    page: int = Query(1, ge=1),
    page_size: int = Query(100, ge=1, le=1000),
    search: Optional[str] = Query(None, description="æ¨¡ç³Šæœç´¢å…³é”®å­—"),
    province: Optional[str] = Query(None, description="æŒ‰çœä»½è¿‡æ»¤"),
    db: Session = Depends(get_session),
):
    """è·å–æ•°æ®åº“è®°å½•åˆ—è¡¨ (å¸¦é”™è¯¯å¤„ç†)"""
    try:
        table = _get_records_table_safe()
        if table is None:
            # æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºç»“æœ
            return {
                "status": "success",
                "columns": [],
                "records": [],
                "page": page,
                "page_size": page_size,
                "total": 0,
                "message": "æ•°æ®åº“å°šæœªåˆå§‹åŒ–"
            }
        
        columns = [column.name for column in table.columns]
    except Exception as e:
        print(f"[ERROR] get_database_records è·å–è¡¨ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "status": "success",
            "columns": [],
            "records": [],
            "page": page,
            "page_size": page_size,
            "total": 0,
            "message": "è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥"
        }

    province_column, _ = _resolve_column({column.name: column for column in table.columns}, PROVINCE_COLUMN_CANDIDATES)
    province_clause = None
    if province and province_column is not None:
        raw_value = province.strip()
        normalized_value = _normalize_province_name(raw_value)
        candidates: List[str] = []
        if normalized_value:
            candidates.append(normalized_value)
        if raw_value and raw_value not in candidates:
            candidates.append(raw_value)
        suffixes = ["çœ", "å¸‚", "è‡ªæ²»åŒº", "ç‰¹åˆ«è¡Œæ”¿åŒº", "åœ°åŒº"]
        expanded_candidates: List[str] = []
        for base in candidates:
            expanded_candidates.append(base)
            for suffix in suffixes:
                expanded_candidates.append(f"{base}{suffix}")
        patterns = [item for item in expanded_candidates if item]
        if patterns:
            province_clause = or_(
                *[province_column.ilike(f"%{pattern}%") for pattern in patterns]
            )

    filters = _build_optional_filters(table, search)

    base_select = select(*[table.c[column] for column in columns], text("ROWID as __rowid__"))
    if filters is not None:
        base_select = base_select.where(filters)
    if province_clause is not None:
        base_select = base_select.where(province_clause)

    count_query = select(func.count()).select_from(table)
    if filters is not None:
        count_query = count_query.where(filters)
    if province_clause is not None:
        count_query = count_query.where(province_clause)

    total = int(db.execute(count_query).scalar() or 0)
    offset = (page - 1) * page_size

    rows = (
        db.execute(
            base_select.order_by(text("ROWID")).offset(offset).limit(page_size)
        )
        .mappings()
        .all()
    )

    records = []
    for row in rows:
        row_dict = dict(row)
        records.append(_serialize_row(row_dict, columns))

    return {
        "status": "success",
        "columns": columns,
        "records": records,
        "page": page,
        "page_size": page_size,
        "total": total,
    }


class SaveDatabaseRequest(BaseModel):
    updated: List[Dict[str, Any]] = []
    inserted: List[Dict[str, Any]] = []
    deleted: List[int] = []


@app.post("/api/database/save")
async def save_database(request: SaveDatabaseRequest, db: Session = Depends(get_session)):
    table = _get_records_table_or_500()
    columns = [column.name for column in table.columns]

    try:
        if request.inserted:
            payloads = []
            for row in request.inserted:
                payload = {column: row.get(column) for column in columns}
                payloads.append(payload)
            if payloads:
                db.execute(table.insert(), payloads)

        if request.updated:
            for row in request.updated:
                rowid = row.get("__rowid__")
                if rowid is None:
                    continue
                values = {column: row.get(column) for column in columns}
                stmt = table.update().where(text("ROWID = :rowid")).values(**values)
                params = {"rowid": rowid, **values}
                db.execute(stmt, params)

        if request.deleted:
            for rowid in request.deleted:
                db.execute(text("DELETE FROM records WHERE ROWID = :rowid"), {"rowid": rowid})

        db.commit()
        reset_table_cache()
    except Exception as exc:  # pragma: no cover - database failure fallback
        db.rollback()
        raise HTTPException(status_code=500, detail=f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {exc}") from exc

    return {"status": "success", "message": "æ•°æ®åº“å·²æ›´æ–°"}


@app.get("/api/database/lithologies")
async def get_lithology_summary(db: Session = Depends(get_session)):
    """è·å–å²©æ€§æ‘˜è¦ (å¸¦é”™è¯¯å¤„ç†)"""
    try:
        table = _get_records_table_safe()
        if table is None:
            return {
                "status": "success",
                "lithologies": [],
                "numeric_columns": [],
                "counts": {},
                "message": "æ•°æ®åº“å°šæœªåˆå§‹åŒ–"
            }
        
        column_map = {column.name: column for column in table.columns}

        lithology_col_name = "å²©æ€§"
        if lithology_col_name not in column_map:
            return {
                "status": "success",
                "lithologies": [],
                "numeric_columns": [],
                "counts": {},
                "message": "æ•°æ®åº“ç¼ºå°‘å²©æ€§åˆ—"
            }
    except Exception as e:
        print(f"[ERROR] get_lithology_summary å‘ç”Ÿé”™è¯¯: {e}")
        return {
            "status": "success",
            "lithologies": [],
            "numeric_columns": [],
            "counts": {},
            "message": "è·å–å²©æ€§æ•°æ®å¤±è´¥"
        }

    available_numeric = _infer_numeric_columns(table, db, exclude={lithology_col_name}) or [
        col for col in DEFAULT_NUMERIC_COLUMNS if col in column_map
    ]

    stmt = (
        select(
            column_map[lithology_col_name].label("name"),
            func.count().label("count"),
        )
        .where(column_map[lithology_col_name].is_not(None))
        .group_by(column_map[lithology_col_name])
        .order_by(func.count().desc())
    )

    aggregated_counts: Dict[str, int] = {}
    for row in db.execute(stmt):
        raw_name = str(row.name).strip() if row.name is not None else ""
        if not raw_name:
            continue
        normalized_name = _normalize_lithology_name(raw_name)
        if not normalized_name:
            continue
        aggregated_counts[normalized_name] = aggregated_counts.get(normalized_name, 0) + int(row.count or 0)

    sorted_items = sorted(aggregated_counts.items(), key=lambda item: item[1], reverse=True)
    lithologies = [name for name, _ in sorted_items]
    counts = {name: count for name, count in sorted_items}

    return {
        "status": "success",
        "lithologies": lithologies,
        "numeric_columns": available_numeric,
        "counts": counts,
    }


@app.get("/api/database/lithology-data")
async def get_lithology_data(
    lithology: str = Query(..., description="å²©æ€§åç§°"),
    search: Optional[str] = Query(None, description="å¯é€‰æ¨¡ç³Šæœç´¢"),
    db: Session = Depends(get_session),
):
    """è·å–æŒ‡å®šå²©æ€§çš„æ•°æ® (å¸¦é”™è¯¯å¤„ç†)"""
    try:
        table = _get_records_table_safe()
        if table is None:
            return {
                "status": "success",
                "values": {},
                "count": 0,
                "stats": {},
                "message": "æ•°æ®åº“å°šæœªåˆå§‹åŒ–"
            }
        
        column_map = {column.name: column for column in table.columns}

        lithology_col_name = "å²©æ€§"
        if lithology_col_name not in column_map:
            return {
                "status": "success",
                "values": {},
                "count": 0,
                "stats": {},
                "message": "æ•°æ®åº“ç¼ºå°‘å²©æ€§åˆ—"
            }

        name = lithology.strip()
        if not name:
            raise HTTPException(status_code=400, detail="å²©æ€§åç§°ä¸èƒ½ä¸ºç©º")
        normalized_name = _normalize_lithology_name(name)
        if not normalized_name:
            raise HTTPException(status_code=400, detail="å²©æ€§åç§°ä¸èƒ½ä¸ºç©º")

        available_numeric = _infer_numeric_columns(table, db, exclude={lithology_col_name}) or [
            col for col in DEFAULT_NUMERIC_COLUMNS if col in column_map
        ]
        if not available_numeric:
            return {"status": "success", "values": {}, "count": 0, "stats": {}}

        comparator = column_map[lithology_col_name]
        if normalized_name == COAL_NORMALIZED_NAME:
            base_condition = comparator.like("%ç…¤%")
        else:
            base_condition = comparator == normalized_name

        stmt = select(*[column_map[col] for col in available_numeric]).where(base_condition)
        filters = _build_optional_filters(table, search)
        if filters is not None:
            stmt = stmt.where(filters)
        rows = db.execute(stmt).all()
        if not rows:
            return {"status": "success", "values": {}, "count": 0, "stats": {}}

        data_frame = pd.DataFrame(rows, columns=available_numeric)
        values: Dict[str, List[float]] = {}
        stats: Dict[str, Dict[str, float]] = {}
        for col in available_numeric:
            series = pd.to_numeric(data_frame[col], errors="coerce").dropna()
            if series.empty:
                continue
            values[col] = series.tolist()
            stats[col] = {
                "count": int(series.count()),
                "min": float(series.min()),
                "max": float(series.max()),
                "mean": float(series.mean()),
                "median": float(series.median()),
                "std": float(series.std(ddof=0)) if series.count() > 1 else 0.0,
            }

        count_stmt = select(func.count()).select_from(table).where(base_condition)
        if filters is not None:
            count_stmt = count_stmt.where(filters)
        count = int(db.execute(count_stmt).scalar() or 0)

        return {"status": "success", "values": values, "count": count, "stats": stats}
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] get_lithology_data å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return {
            "status": "success",
            "values": {},
            "count": 0,
            "stats": {},
            "message": "è·å–å²©æ€§æ•°æ®å¤±è´¥"
        }


@app.get("/api/health")
async def health_check():
    return {"status": "ok"}


@app.get("/api/performance/stats")
async def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    mem_usage = check_memory_usage()
    cache_stats = get_cache_stats()

    return {
        "status": "success",
        "memory": mem_usage,
        "cache": cache_stats,
        "config": {
            "max_upload_mb": MAX_UPLOAD_SIZE_MB,
            "max_resolution": MAX_RESOLUTION,
            "cache_enabled": CACHE_ENABLED
        }
    }


@app.get("/api/dashboard/stats")
async def get_dashboard_stats(db: Session = Depends(get_session)):
    """è·å–ä»ªè¡¨æ¿ç»Ÿè®¡ä¿¡æ¯ (å¸¦é”™è¯¯å¤„ç†)"""
    try:
        def query_rock_db_count():
            try:
                table = get_records_table()
                return int(db.execute(select(func.count()).select_from(table)).scalar() or 0)
            except (RuntimeError, Exception) as e:
                print(f"[WARNING] æŸ¥è¯¢å²©çŸ³æ•°æ®åº“è®°å½•æ•°å¤±è´¥: {e}")
                return 0

        # ä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢ç¼“å­˜
        rock_db_count = cache_database_query("rock_count", query_rock_db_count, ttl=180)

        modeling_record_count = 0
        if modeling_state.merged_df is not None:
            modeling_record_count = int(len(modeling_state.merged_df))

        borehole_file_count = int(modeling_state.borehole_file_count or 0)

        return {
            "status": "success",
            "stats": {
                "rock_db_count": rock_db_count,
                "borehole_file_count": borehole_file_count,
                "modeling_record_count": modeling_record_count,
            },
        }
    except Exception as e:
        print(f"[ERROR] get_dashboard_stats å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        # è¿”å›é»˜è®¤å€¼è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
        return {
            "status": "success",
            "stats": {
                "rock_db_count": 0,
                "borehole_file_count": 0,
                "modeling_record_count": 0,
            },
        }


@app.get("/api/borehole-data")
async def get_borehole_data(db: Session = Depends(get_session)):
    """è·å–é’»å­”æ•°æ® (ä»æ•°æ®åº“ä¸­è¯»å–æ‰€æœ‰è®°å½•)"""
    try:
        table = _get_records_table_safe()
        if table is None:
            return []
        
        # è·å–æ‰€æœ‰è®°å½•
        columns = [column.name for column in table.columns]
        stmt = select(*[table.c[col] for col in columns])
        rows = db.execute(stmt).mappings().all()
        
        records = []
        for row in rows:
            row_dict = dict(row)
            records.append(_serialize_row(row_dict, columns))
        
        return records
    except Exception as e:
        print(f"[ERROR] get_borehole_data å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return []


@app.get("/api/summary-data")
async def get_summary_data(db: Session = Depends(get_session)):
    """è·å–æ±‡æ€»æ•°æ® (ä»æ•°æ®åº“ä¸­è¯»å–å¹¶æŒ‰çŸ¿ååˆ†ç»„)"""
    try:
        table = _get_records_table_safe()
        if table is None:
            return []
        
        column_map = {column.name: column for column in table.columns}
        
        # å¦‚æœæœ‰çŸ¿ååˆ—ï¼ŒæŒ‰çŸ¿ååˆ†ç»„ç»Ÿè®¡
        if "çŸ¿å" in column_map:
            stmt = (
                select(
                    column_map["çŸ¿å"].label("mine_name"),
                    func.count().label("record_count")
                )
                .where(column_map["çŸ¿å"].is_not(None))
                .group_by(column_map["çŸ¿å"])
            )
            rows = db.execute(stmt).all()
            
            summary = []
            for row in rows:
                summary.append({
                    "çŸ¿å": row.mine_name,
                    "è®°å½•æ•°": int(row.record_count)
                })
            return summary
        else:
            # å¦‚æœæ²¡æœ‰çŸ¿ååˆ—ï¼Œè¿”å›ç©ºæ•°ç»„
            return []
    except Exception as e:
        print(f"[ERROR] get_summary_data å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return []


@app.get("/api/coal-seam-data")
async def get_coal_seam_data(db: Session = Depends(get_session)):
    """è·å–ç…¤å±‚æ•°æ® (ä»æ•°æ®åº“ä¸­ç­›é€‰åŒ…å«"ç…¤"çš„å²©æ€§è®°å½•)"""
    try:
        table = _get_records_table_safe()
        if table is None:
            return []
        
        column_map = {column.name: column for column in table.columns}
        columns = [column.name for column in table.columns]
        
        # å¦‚æœæœ‰å²©æ€§åˆ—ï¼Œç­›é€‰åŒ…å«"ç…¤"çš„è®°å½•
        if "å²©æ€§" in column_map:
            stmt = (
                select(*[table.c[col] for col in columns])
                .where(column_map["å²©æ€§"].like("%ç…¤%"))
            )
            rows = db.execute(stmt).mappings().all()
            
            records = []
            for row in rows:
                row_dict = dict(row)
                records.append(_serialize_row(row_dict, columns))
            return records
        else:
            # å¦‚æœæ²¡æœ‰å²©æ€§åˆ—ï¼Œå°è¯•å²©å±‚åç§°åˆ—
            if "å²©å±‚åç§°" in column_map:
                stmt = (
                    select(*[table.c[col] for col in columns])
                    .where(column_map["å²©å±‚åç§°"].like("%ç…¤%"))
                )
                rows = db.execute(stmt).mappings().all()
                
                records = []
                for row in rows:
                    row_dict = dict(row)
                    records.append(_serialize_row(row_dict, columns))
                return records
            else:
                return []
    except Exception as e:
        print(f"[ERROR] get_coal_seam_data å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return []


@app.post("/api/get_feasibility_evaluation_levels")
async def get_feasibility_evaluation_levels():
    """è·å–ä¸Šè¡Œå¼€é‡‡å¯è¡Œæ€§ç­‰çº§è¯„ä¼°æ ‡å‡†"""
    return {
        "status": "success",
        "data": {
            "levels": [
                {
                    "level": "Içº§ (ä¸å¯è¡Œ/æå›°éš¾)",
                    "omega_range": "0-2",
                    "description": "ç…¤å±‚åŸºæœ¬å¤„äºå®è½å¸¦å†…ï¼Œå®Œæ•´åº¦ç ´åéå¸¸ä¸¥é‡ï¼Œä¸Šè¡Œå¼€é‡‡ä¸åˆç†æˆ–è¿‡äºå±é™©ã€‚"
                },
                {
                    "level": "IIçº§ (å›°éš¾)",
                    "omega_range": "2-4",
                    "description": "ä¸Šè¡Œå¼€é‡‡éš¾åº¦å¤§ï¼Œæ˜“å‡ºç°é¡¶æ¿é—®é¢˜å’Œå··é“æ”¯æŠ¤å›°éš¾ï¼Œéœ€è¦é‡å‹æ”¯æŠ¤æˆ–å……å¡«ã€‚"
                },
                {
                    "level": "IIIçº§ (å¯è¡Œï¼Œéœ€æ”¯æŠ¤)",
                    "omega_range": "4-6",
                    "description": "ä¸­ç­‰ç ´åç¨‹åº¦ï¼Œé¡¶æ¿å’Œç…¤å±‚å°‘é‡ç ´ç¢ï¼Œä½†è£‚éš™å‘è‚²ç¨‹åº¦å¤§ã€‚æŠ€æœ¯ä¸Šå¯è¡Œï¼Œå±€éƒ¨éœ€åŠ å¼ºæ”¯æŠ¤ã€‚"
                },
                {
                    "level": "IVçº§ (è‰¯å¥½)",
                    "omega_range": "6-8",
                    "description": "è½»å¾®ç ´åï¼Œç…¤å±‚å®Œæ•´æ€§è‰¯å¥½ï¼Œé¡¶æ¿æœ‰å°‘é‡è£‚éš™ï¼Œä¸‹æ²‰é‡å¾®å°ï¼Œä¸Šè¡Œå¼€é‡‡æ•ˆæœè¾ƒå¥½ã€‚"
                },
                {
                    "level": "Vçº§ (ä¼˜è‰¯)",
                    "omega_range": "8ä»¥ä¸Š",
                    "description": "ç…¤å±‚åŸºæœ¬ä¸å—ä¸‹ç…¤å±‚å¼€é‡‡çš„å½±å“ï¼Œç…¤å±‚é—´çš„ç›¸äº’ä½œç”¨åŸºæœ¬ä¸å­˜åœ¨ï¼Œä¸Šè¡Œå¼€é‡‡ä¸å­˜åœ¨å›°éš¾ã€‚"
                }
            ]
        }
    }


@app.post("/api/calculate_upward_mining_feasibility")
async def calculate_upward_mining_feasibility(request: dict):
    """è®¡ç®—å•ä¸ªé’»å­”çš„ä¸Šè¡Œå¼€é‡‡å¯è¡Œåº¦"""
    try:
        from upward_mining_feasibility import process_borehole_csv_for_feasibility
        
        # éªŒè¯å¿…éœ€å‚æ•°
        required_params = ['csv_file_path', 'bottom_coal_name', 'upper_coal_name']
        for param in required_params:
            if param not in request or not request[param]:
                raise HTTPException(status_code=400, detail=f'ç¼ºå°‘å¿…éœ€å‚æ•°: {param}')
        
        csv_file_path = request['csv_file_path']
        bottom_coal_name = request['bottom_coal_name']
        upper_coal_name = request['upper_coal_name']
        lamda = request.get('lamda', 4.95)
        C = request.get('C', -0.84)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(csv_file_path).exists():
            raise HTTPException(status_code=404, detail=f'æ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}')
        
        # è°ƒç”¨è®¡ç®—å‡½æ•°
        result = process_borehole_csv_for_feasibility(
            csv_file_path, bottom_coal_name, upper_coal_name, lamda, C
        )
        
        if "error" in result:
            raise HTTPException(status_code=400, detail=result['error'])
        
        return {"status": "success", "data": result}
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] calculate_upward_mining_feasibility å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f'è®¡ç®—å¤±è´¥: {str(e)}')


@app.post("/api/batch_calculate_upward_mining_feasibility")
async def batch_calculate_upward_mining_feasibility(request: dict):
    """æ‰¹é‡è®¡ç®—é’»å­”çš„ä¸Šè¡Œå¼€é‡‡å¯è¡Œåº¦"""
    try:
        from upward_mining_feasibility import batch_process_borehole_files
        
        # éªŒè¯å¿…éœ€å‚æ•°
        required_params = ['csv_file_paths', 'bottom_coal_name', 'upper_coal_name']
        for param in required_params:
            if param not in request or not request[param]:
                raise HTTPException(status_code=400, detail=f'ç¼ºå°‘å¿…éœ€å‚æ•°: {param}')
        
        csv_file_paths = request['csv_file_paths']
        bottom_coal_name = request['bottom_coal_name']
        upper_coal_name = request['upper_coal_name']
        lamda = request.get('lamda', 4.95)
        C = request.get('C', -0.84)
        
        # éªŒè¯æ–‡ä»¶åˆ—è¡¨
        if not isinstance(csv_file_paths, list) or len(csv_file_paths) == 0:
            raise HTTPException(status_code=400, detail='CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨ä¸èƒ½ä¸ºç©º')
        
        # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for file_path in csv_file_paths:
            if not Path(file_path).exists():
                raise HTTPException(status_code=404, detail=f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
        
        # è°ƒç”¨æ‰¹é‡è®¡ç®—å‡½æ•°
        result = batch_process_borehole_files(
            csv_file_paths, bottom_coal_name, upper_coal_name, lamda, C
        )
        
        return {"status": "success", "data": result}
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] batch_calculate_upward_mining_feasibility å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f'æ‰¹é‡è®¡ç®—å¤±è´¥: {str(e)}')


@app.post("/api/auto_calibrate_upward_mining_coefficients")
async def auto_calibrate_upward_mining_coefficients(request: dict):
    """è‡ªåŠ¨æ ‡å®šä¸Šè¡Œå¼€é‡‡è®¡ç®—çš„ç³»æ•°(Î»å’ŒC)"""
    try:
        from upward_mining_feasibility import auto_calibrate_coefficients
        
        # éªŒè¯å¿…éœ€å‚æ•°
        required_params = ['csv_file_paths', 'bottom_coal_name', 'upper_coal_name']
        for param in required_params:
            if param not in request or not request[param]:
                raise HTTPException(status_code=400, detail=f'ç¼ºå°‘å¿…éœ€å‚æ•°: {param}')
        
        csv_file_paths = request['csv_file_paths']
        bottom_coal_name = request['bottom_coal_name']
        upper_coal_name = request['upper_coal_name']
        initial_lamda = request.get('initial_lamda', 4.95)
        initial_C = request.get('initial_C', -0.84)
        
        # éªŒè¯æ–‡ä»¶åˆ—è¡¨
        if not isinstance(csv_file_paths, list) or len(csv_file_paths) == 0:
            raise HTTPException(status_code=400, detail='CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨ä¸èƒ½ä¸ºç©º')
        
        # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for file_path in csv_file_paths:
            if not Path(file_path).exists():
                raise HTTPException(status_code=404, detail=f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
        
        # è°ƒç”¨è‡ªåŠ¨æ ‡å®šå‡½æ•°
        result = auto_calibrate_coefficients(
            csv_file_paths, bottom_coal_name, upper_coal_name, initial_lamda, initial_C
        )
        
        return {"status": "success", "data": result}
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] auto_calibrate_upward_mining_coefficients å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f'æ ‡å®šå¤±è´¥: {str(e)}')


@app.post("/api/csv/columns")
async def get_csv_columns(file: UploadFile = File(...)):
    if not file.filename:
        raise HTTPException(status_code=400, detail="æœªæä¾›CSVæ–‡ä»¶")
    data = await file.read()
    df = _read_csv_bytes(data)
    return {"status": "success", "columns": df.columns.tolist()}


@app.post("/api/csv/transform")
async def transform_csv(
    file: UploadFile = File(...),
    mapping: str = Form(...),
    target_columns: str = Form(...),
):
    if not file.filename:
        raise HTTPException(status_code=400, detail="æœªæä¾›CSVæ–‡ä»¶")

    try:
        mapping_dict: Dict[str, str] = json.loads(mapping or "{}")
        targets: List[str] = json.loads(target_columns or "[]")
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="æ˜ å°„æˆ–åˆ—ä¿¡æ¯æ ¼å¼é”™è¯¯")

    data = await file.read()
    df = _read_csv_bytes(data)

    output = pd.DataFrame()
    for target in targets:
        source_col = mapping_dict.get(target)
        if source_col and source_col in df.columns:
            output[target] = df[source_col]
        else:
            output[target] = ["" for _ in range(len(df))]

    filename = file.filename or "formatted.csv"
    if "." in filename:
        stem, ext = filename.rsplit(".", 1)
        download_name = f"{stem}_formatted.{ext}"
    else:
        download_name = f"{filename}_formatted.csv"

    csv_content = output.to_csv(index=False, encoding="utf-8-sig")
    buffer = io.BytesIO(csv_content.encode("utf-8-sig"))
    buffer.seek(0)

    return StreamingResponse(
        buffer,
        media_type="text/csv",
        headers={"Content-Disposition": f"attachment; filename={download_name}"},
    )


@app.post("/api/borehole/analyze")
async def analyze_borehole_files(files: List[UploadFile] = File(...)):
    if not files:
        raise HTTPException(status_code=400, detail="è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªé’»å­”æ–‡ä»¶")

    summary = {
        "total_files": len(files),
        "successful_files": 0,
        "failed_files": 0,
        "warning_files": 0,
        "total_coal_records": 0,
        "error_details": [],
        "warning_details": [],
        "file_details": [],
        "processing_time": 0.0,
    }

    combined_records: List[Dict[str, Any]] = []
    start_time = time.perf_counter()

    for upload in files:
        filename = upload.filename or "æœªå‘½å.csv"
        detail = {"file_name": filename, "status": "pending", "message": "", "coal_records": 0}
        try:
            file_bytes = await upload.read()
            if not file_bytes:
                raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©º")

            records, message, level = process_single_borehole_file(file_bytes, filename)
            detail.update({
                "status": level,
                "message": message,
                "coal_records": len(records),
            })

            if level == "error":
                summary["failed_files"] += 1
                summary["error_details"].append(f"{filename}: {message}")
            elif level == "warning":
                summary["warning_files"] += 1
                summary["warning_details"].append(f"{filename}: {message}")
                if records:
                    combined_records.extend(records)
                    summary["total_coal_records"] += len(records)
            else:
                summary["successful_files"] += 1
                if records:
                    combined_records.extend(records)
                    summary["total_coal_records"] += len(records)
        except Exception as exc:  # pragma: no cover - best effort for malformed files
            detail.update({
                "status": "error",
                "message": str(exc),
                "coal_records": 0,
            })
            summary["failed_files"] += 1
            summary["error_details"].append(f"{filename}: {exc}")
        finally:
            summary["file_details"].append(detail)

    summary["processing_time"] = round(time.perf_counter() - start_time, 3)

    columns: List[str] = []
    if combined_records:
        seen: List[str] = []
        for record in combined_records:
            for key in record.keys():
                if key not in seen:
                    seen.append(key)
        columns = seen

    message = "åˆ†æå®Œæˆ" if summary["successful_files"] else "æœªæˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶"
    if summary["total_coal_records"] == 0:
        message = "æœªæå–åˆ°æœ‰æ•ˆç…¤å±‚è®°å½•"

    return {
        "status": "success",
        "message": message,
        "columns": columns,
        "records": combined_records,
        "summary": summary,
    }


@app.post("/api/keystratum/files")
async def upload_keystratum_files(files: List[UploadFile] = File(...)):
    if not files:
        raise HTTPException(status_code=400, detail="è¯·ä¸Šä¼ è‡³å°‘ä¸€ä¸ªå²©å±‚æ•°æ®æ–‡ä»¶")

    key_stratum_state.reset()

    total = len(files)
    valid = 0
    errors: List[str] = []
    stored: Dict[str, pd.DataFrame] = {}

    for upload in files:
        filename = upload.filename or "æœªå‘½å.csv"
        try:
            content = await upload.read()
            if not content:
                raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©º")
            df = _read_csv_bytes(content)
            df = unify_columns(df)
            df = _normalize_key_columns(df)
            
            # å¦‚æœCSVä¸­å·²ç»æœ‰é’»å­”ååˆ—ï¼Œåˆ é™¤å®ƒ(å› ä¸ºä¼šä»æ–‡ä»¶åé‡æ–°ç”Ÿæˆ)
            if "é’»å­”å" in df.columns:
                df = df.drop(columns=["é’»å­”å"])
            
            # åŒæ ·åˆ é™¤æ•°æ®æ¥æºåˆ—(ä¼šä»æ–‡ä»¶åé‡æ–°ç”Ÿæˆ)
            if "æ•°æ®æ¥æº" in df.columns:
                df = df.drop(columns=["æ•°æ®æ¥æº"])
            
            if "å²©å±‚åç§°" not in df.columns:
                raise ValueError("ç¼ºå°‘åˆ—: å²©å±‚åç§°")
            if "åšåº¦/m" not in df.columns:
                raise ValueError("ç¼ºå°‘åˆ—: åšåº¦/m")
            df["å²©å±‚åç§°"] = df["å²©å±‚åç§°"].astype(str).str.strip()
            df = _ensure_seam_column(df)
            stored[filename] = df
            valid += 1
        except Exception as exc:
            errors.append(f"{filename}: {exc}")
            continue

    if not stored:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼")

    key_stratum_state.files = stored
    preview = _prepare_preview(stored)

    return {
        "status": "success",
        "file_count": total,
        "valid_count": valid,
        "errors": errors,
        "preview": preview,
    }


@app.post("/api/keystratum/fill")
async def fill_keystratum_from_database():
    if not key_stratum_state.files:
        raise HTTPException(status_code=400, detail="è¯·å…ˆä¸Šä¼ å²©å±‚æ•°æ®æ–‡ä»¶")

    key_stratum_state.last_result = None

    table = _get_records_table_or_500()
    columns = [column.name for column in table.columns]
    lithology_col = "å²©æ€§"
    if lithology_col not in columns:
        raise HTTPException(status_code=400, detail="å²©æ€§æ•°æ®åº“ç¼ºå°‘ 'å²©æ€§' åˆ—")

    numeric_cols = [col for col in DEFAULT_NUMERIC_COLUMNS if col in columns]

    available_numeric = numeric_cols
    if not available_numeric:
        raise HTTPException(status_code=400, detail="æ•°æ®åº“ç¼ºå°‘å¯ç”¨äºå¡«å……çš„æ•°å€¼åˆ—")

    query_columns = [table.c[lithology_col]] + [table.c[col] for col in available_numeric]
    with get_engine().connect() as connection:
        db_df = pd.read_sql(select(*query_columns), connection)

    if db_df.empty:
        raise HTTPException(status_code=400, detail="æ•°æ®åº“ä¸­æ²¡æœ‰å¯ç”¨äºå¡«å……çš„å²©æ€§è®°å½•")

    db_df[lithology_col] = db_df[lithology_col].astype(str).str.strip()
    db_df[lithology_col] = db_df[lithology_col].map(_normalize_lithology_name)
    db_df = db_df[db_df[lithology_col].astype(bool)]
    stats_map = (
        db_df.groupby(lithology_col)[available_numeric]
        .median(numeric_only=True)
        .dropna(how="all")
    )
    stats_map = stats_map.reindex(columns=numeric_cols, fill_value=np.nan)

    updated: Dict[str, pd.DataFrame] = {}
    for filename, df in key_stratum_state.files.items():
        updated[filename] = _fill_from_database(df, stats_map)

    key_stratum_state.files = updated
    key_stratum_state.filled = True
    preview = _prepare_preview(updated)

    return {
        "status": "success",
        "message": "å·²æ ¹æ®æ•°æ®åº“å¡«å……ç¼ºå¤±å‚æ•°",
        "preview": preview,
    }


@app.get("/api/keystratum/coals")
async def get_keystratum_coals():
    if not key_stratum_state.files:
        raise HTTPException(status_code=400, detail="è¯·å…ˆä¸Šä¼ å²©å±‚æ•°æ®æ–‡ä»¶")

    coals: set[str] = set()
    for df in key_stratum_state.files.values():
        column = df.get("å²©å±‚åç§°")
        if column is None:
            continue
        coals.update(
            value
            for value in column.astype(str).str.strip()
            if value and ("ç…¤" in value or "ç…¤å±‚" in value)
        )

    sorted_coals = sorted(coals)
    return {"status": "success", "coals": sorted_coals}


class KeyStratumRequest(BaseModel):
    coal: str


@app.post("/api/keystratum/process")
async def process_keystratum(request: KeyStratumRequest):
    if not key_stratum_state.files:
        raise HTTPException(status_code=400, detail="è¯·å…ˆä¸Šä¼ å²©å±‚æ•°æ®æ–‡ä»¶")
    coal_name = request.coal.strip() if request.coal else ""
    if not coal_name:
        raise HTTPException(status_code=400, detail="è¯·é€‰æ‹©ç›®æ ‡å²©å±‚")

    key_stratum_state.last_result = None

    print(f"[DEBUG] å¼€å§‹å¤„ç†å…³é”®å±‚, ç›®æ ‡ç…¤å±‚: {coal_name}")
    print(f"[DEBUG] æ–‡ä»¶æ•°é‡: {len(key_stratum_state.files)}")
    for filename in key_stratum_state.files.keys():
        print(f"[DEBUG] æ–‡ä»¶: {filename}")
        df = key_stratum_state.files[filename]
        print(f"[DEBUG]   è¡Œæ•°: {len(df)}, åˆ—: {df.columns.tolist()}")
        if "å²©å±‚åç§°" in df.columns:
            unique_names = df["å²©å±‚åç§°"].unique().tolist()
            print(f"[DEBUG]   å²©å±‚åç§°: {unique_names[:10]}")  # åªæ‰“å°å‰10ä¸ª
        else:
            print(f"[DEBUG]   ç¼ºå°‘'å²©å±‚åç§°'åˆ—!")

    processed: List[pd.DataFrame] = []
    errors: List[str] = []
    processed_count = 0

    for filename, df in key_stratum_state.files.items():
        print(f"[DEBUG] å¤„ç†æ–‡ä»¶: {filename}, è¡Œæ•°: {len(df)}")
        try:
            working_df = df.copy()
            if "å²©å±‚åç§°" not in working_df.columns:
                raise ValueError("ç¼ºå°‘åˆ—: å²©å±‚åç§°")
            
            # æŸ¥æ‰¾ç…¤å±‚
            mask = working_df["å²©å±‚åç§°"].astype(str).str.strip() == coal_name
            print(f"[DEBUG]   æŸ¥æ‰¾ç…¤å±‚ '{coal_name}', åŒ¹é…è¡Œæ•°: {mask.sum()}")
            
            if not mask.any():
                errors.append(f"{filename}: æœªæ‰¾åˆ°ç›®æ ‡å²©å±‚ {coal_name}")
                continue
            coal_indices = working_df[mask].index.tolist()
            coal_idx = coal_indices[0]
            coal_seam_df = working_df.loc[[coal_idx]]
            df_above = working_df.loc[: coal_idx - 1].copy()
            if df_above.empty:
                errors.append(f"{filename}: ç›®æ ‡å²©å±‚ä¸Šæ–¹æ— å²©å±‚")
                continue

            required_cols = ["åšåº¦/m", "å¼¹æ€§æ¨¡é‡/GPa", "å®¹é‡/kNÂ·m-3", "æŠ—æ‹‰å¼ºåº¦/MPa"]
            for col in required_cols:
                if col not in df_above.columns:
                    raise ValueError(f"ç¼ºå°‘åˆ—: {col}")

            key_info = calculate_key_strata_details(df_above, coal_seam_df)

            result_df = working_df.copy()
            result_df.insert(0, "é’»å­”å", Path(filename).stem)
            result_df["å…³é”®å±‚æ ‡è®°"] = "-"
            result_df["è·ç…¤å±‚è·ç¦»/m"] = 0.0
            result_df.loc[coal_idx, "å…³é”®å±‚æ ‡è®°"] = "ç…¤å±‚"

            cumulative_above = 0.0
            for row_idx in range(len(df_above) - 1, -1, -1):
                actual_idx = df_above.index[row_idx]
                thickness = pd.to_numeric(df_above.iloc[row_idx]["åšåº¦/m"], errors="coerce")
                thickness = float(thickness) if pd.notna(thickness) else 0.0
                distance = round(cumulative_above + thickness / 2, 2)
                result_df.loc[actual_idx, "è·ç…¤å±‚è·ç¦»/m"] = distance
                cumulative_above += thickness

            if coal_idx < len(result_df) - 1:
                cumulative_below = 0.0
                for row_idx in range(coal_idx + 1, len(result_df)):
                    thickness = pd.to_numeric(result_df.iloc[row_idx]["åšåº¦/m"], errors="coerce")
                    thickness = float(thickness) if pd.notna(thickness) else 0.0
                    cumulative_below += thickness
                    result_df.iat[row_idx, result_df.columns.get_loc("è·ç…¤å±‚è·ç¦»/m")] = round(-cumulative_below, 2)

            above_mask = result_df.index < coal_idx
            for item in key_info:
                lithology_name = str(item.get("å²©æ€§", "")).replace("(PKS)", "").strip()
                if not lithology_name:
                    continue
                distance = float(item.get("è·ç…¤å±‚è·ç¦»", 0))
                match_mask = (
                    above_mask
                    & (result_df["å²©å±‚åç§°"].astype(str).str.strip() == lithology_name)
                    & np.isclose(result_df["è·ç…¤å±‚è·ç¦»/m"], distance, atol=0.1)
                )
                if match_mask.any():
                    result_df.loc[match_mask, "å…³é”®å±‚æ ‡è®°"] = item.get("SK_Label", "-")

            processed.append(result_df)
            processed_count += 1
        except Exception as exc:
            errors.append(f"{filename}: {exc}")
            continue

    if not processed:
        key_stratum_state.last_result = None
        print(f"[DEBUG] å¤„ç†å¤±è´¥! é”™è¯¯ä¿¡æ¯: {errors}")
        return {
            "status": "error",
            "message": "æœªæˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶",
            "errors": errors,
        }

    combined_df = pd.concat(processed, ignore_index=True, sort=False).fillna("")
    columns = combined_df.columns.tolist()
    records = json.loads(combined_df.to_json(orient="records", force_ascii=False))

    key_stratum_state.last_result = combined_df

    print(f"[DEBUG] å¤„ç†æˆåŠŸ! å¤„ç†äº† {processed_count} ä¸ªæ–‡ä»¶")
    return {
        "status": "success",
        "message": "å…³é”®å±‚è®¡ç®—å®Œæˆ",
        "columns": columns,
        "records": records,
        "processed_count": processed_count,
        "errors": errors,
    }


@app.get("/api/keystratum/export")
async def export_keystratum_results(format: str = Query("xlsx", regex="^(xlsx|csv)$")):
    df = key_stratum_state.last_result
    if df is None or df.empty:
        raise HTTPException(status_code=400, detail="å½“å‰æ²¡æœ‰å¯å¯¼å‡ºçš„å…³é”®å±‚è®¡ç®—ç»“æœ")

    buffer = io.BytesIO()
    if format == "csv":
        df.to_csv(buffer, index=False, encoding="utf-8-sig")
        filename = "å…³é”®å±‚è®¡ç®—ç»“æœ.csv"
        media_type = "text/csv"
    else:
        with pd.ExcelWriter(buffer) as writer:
            df.to_excel(writer, index=False, sheet_name="å…³é”®å±‚åˆ†æç»“æœ")
        filename = "å…³é”®å±‚è®¡ç®—ç»“æœ.xlsx"
        media_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"

    buffer.seek(0)
    quoted_name = quote(filename)
    headers = {
        "Content-Disposition": f"attachment; filename*=UTF-8''{quoted_name}"
    }
    return StreamingResponse(buffer, media_type=media_type, headers=headers)


# ==================== åŸå§‹æ•°æ®å¯¼å…¥æ¥å£(ä»…ç”¨äºå…¨å±€æ•°æ®ç®¡ç†) ====================
@app.post("/api/raw/import")
async def import_raw_stratum_data(files: List[UploadFile] = File(...)):
    """
    å¯¼å…¥åŸå§‹å²©å±‚æ•°æ®(ä¸åšä»»ä½•ä¸šåŠ¡å¤„ç†,ç›´æ¥è¯»å–CSV)
    ç”¨äºDashboardå…¨å±€æ•°æ®ç®¡ç†
    è¿‡æ»¤æ‰å…³é”®å±‚è®¡ç®—ç›¸å…³çš„å­—æ®µ,åªä¿ç•™åŸå§‹å²©å±‚å±æ€§
    """
    if not files:
        raise HTTPException(status_code=400, detail="è¯·ä¸Šä¼ è‡³å°‘ä¸€ä¸ªCSVæ–‡ä»¶")

    # å®šä¹‰éœ€è¦æ’é™¤çš„åˆ—åæ¨¡å¼(å…³é”®å±‚è®¡ç®—ç›¸å…³å­—æ®µ)
    # æ³¨æ„: "å…³é”®å±‚æ ‡è®°"åˆ—ä¸æ’é™¤,ä¼šè¢«æ”¹åä¸º"å²©å±‚"
    exclude_patterns = [
        r'^å…³é”®å±‚\d+',           # å…³é”®å±‚1, å…³é”®å±‚2, å…³é”®å±‚1å²©æ€§ç­‰
        r'è·ç…¤å±‚è·ç¦»',
        r'è·.*ç…¤.*é¡¶',           # è·ç…¤å±‚é¡¶æ¿, è·é¡¶ç…¤ç­‰
        r'å…³é”®å±‚.*åšåº¦',
        r'å…³é”®å±‚.*å²©æ€§',
        r'å…³é”®å±‚.*è·',
    ]
    
    import re
    def should_exclude_column(col_name):
        """åˆ¤æ–­åˆ—åæ˜¯å¦åº”è¯¥è¢«æ’é™¤"""
        for pattern in exclude_patterns:
            if re.search(pattern, str(col_name)):
                return True
        return False

    total = len(files)
    valid = 0
    errors: List[str] = []
    all_records = []
    columns_set = set()
    excluded_columns = set()

    for upload in files:
        filename = upload.filename or "æœªå‘½å.csv"
        try:
            content = await upload.read()
            if not content:
                raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©º")
            
            # ç®€å•è¯»å–CSV,ä¸åšä»»ä½•ä¸šåŠ¡å¤„ç†
            df = _read_csv_bytes(content)
            
            # è¿‡æ»¤æ‰è®¡ç®—å­—æ®µ
            original_columns = df.columns.tolist()
            filtered_columns = [col for col in original_columns if not should_exclude_column(col)]
            excluded = [col for col in original_columns if should_exclude_column(col)]
            excluded_columns.update(excluded)
            
            # åªä¿ç•™åŸå§‹å­—æ®µ
            df = df[filtered_columns]
            
            # åªåšåŸºæœ¬çš„åˆ—åç»Ÿä¸€(åšåº¦/mç­‰)
            df = unify_columns(df)
            
            # æ ‡å‡†åŒ–å…³é”®åˆ—å(åç§°â†’å²©å±‚åç§°, åšåº¦â†’åšåº¦/mç­‰)
            df = _normalize_key_columns(df)
            
            # æå–é’»å­”å(ä»æ–‡ä»¶åä¸­,å»æ‰æ‰©å±•å)
            import os
            borehole_name = os.path.splitext(filename)[0]
            
            # æ·»åŠ é’»å­”ååˆ—(æ”¾åœ¨æœ€å‰é¢)
            df.insert(0, 'é’»å­”å', borehole_name)
            
            # å°†"ç…¤å±‚"åˆ—æ”¹åä¸º"å²©å±‚"(å¦‚æœå­˜åœ¨)
            if 'ç…¤å±‚' in df.columns:
                df = df.rename(columns={'ç…¤å±‚': 'å²©å±‚'})
            
            # æ·»åŠ æ•°æ®æ¥æºåˆ—
            df['æ•°æ®æ¥æº'] = filename
            
            # æ”¶é›†æ‰€æœ‰åˆ—å
            columns_set.update(df.columns.tolist())
            
            # è½¬ä¸ºè®°å½•
            records = json.loads(df.to_json(orient="records", force_ascii=False))
            all_records.extend(records)
            
            valid += 1
        except Exception as exc:
            errors.append(f"{filename}: {str(exc)}")
            continue

    if not all_records:
        raise HTTPException(status_code=400, detail="æœªèƒ½æˆåŠŸè§£æä»»ä½•æ–‡ä»¶")

    columns = sorted(list(columns_set))
    
    result = {
        "status": "success",
        "file_count": total,
        "valid_count": valid,
        "errors": errors,
        "records": all_records,
        "columns": columns,
        "record_count": len(all_records),
    }
    
    # æ·»åŠ æ’é™¤å­—æ®µä¿¡æ¯(ç”¨äºè°ƒè¯•)
    if excluded_columns:
        result["excluded_columns"] = sorted(list(excluded_columns))
    
    return result


# ==============================================================================
# å··é“æ”¯æŠ¤è®¡ç®— API
# ==============================================================================

class TunnelSupportInput(BaseModel):
    """å··é“æ”¯æŠ¤è®¡ç®—è¾“å…¥å‚æ•°"""
    B: float  # å··é“å®½åº¦ (m)
    H: float  # å··é“é«˜åº¦ (m)
    K: float  # åº”åŠ›é›†ä¸­ç³»æ•°
    depth: float  # åŸ‹æ·± (m)
    gamma: float  # å®¹é‡ (kN/mÂ³)
    C: float  # ç²˜èšåŠ› (MPa)
    phi: float  # å†…æ‘©æ“¦è§’ (åº¦)
    f_top: Optional[float] = 2.0  # é¡¶æ¿æ™®æ°ç³»æ•° (é»˜è®¤ 2.0)


class TunnelSupportBatchRequest(BaseModel):
    """æ‰¹é‡è®¡ç®—è¯·æ±‚"""
    data: List[TunnelSupportInput]
    constants: Optional[Dict[str, float]] = None


@app.post("/api/tunnel-support/calculate")
async def calculate_tunnel_support(params: TunnelSupportInput):
    """
    å•ä¸ªå··é“æ”¯æŠ¤è®¡ç®—
    
    åŸºäºã€Šå··é“æ”¯æŠ¤ç†è®ºå…¬å¼.docxã€‹å®ç°å®Œæ•´è®¡ç®—æµç¨‹
    """
    try:
        # è°ƒè¯•ï¼šæ‰“å°æ¥æ”¶åˆ°çš„å‚æ•°
        params_dict = params.dict()
        print(f"[DEBUG] æ¥æ”¶åˆ°çš„å‚æ•°: {params_dict}")
        print(f"[DEBUG] f_top å€¼: {params_dict.get('f_top', 'æœªæä¾›')}")
        
        calculator = TunnelSupportCalculator()
        result = calculator.calculate_complete(params_dict)
        
        # è°ƒè¯•ï¼šæ‰“å°è®¡ç®—ç»“æœä¸­çš„ hat
        print(f"[DEBUG] è®¡ç®—ç»“æœ hat: {result['basic']['hat']}")
        
        return {
            "status": "success",
            "result": result
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"è®¡ç®—å¤±è´¥: {str(e)}")


@app.post("/api/tunnel-support/batch-calculate")
async def batch_calculate_tunnel_support_api(request: TunnelSupportBatchRequest):
    """
    æ‰¹é‡å··é“æ”¯æŠ¤è®¡ç®—
    
    æ”¯æŒæ‰¹é‡å¤„ç†å¤šç»„å‚æ•°ï¼Œå¯è‡ªå®šä¹‰è®¡ç®—å¸¸é‡
    """
    try:
        data_list = [item.dict() for item in request.data]
        
        df_result = batch_calculate_tunnel_support(data_list, request.constants)
        
        return {
            "status": "success",
            "count": len(df_result),
            "results": json.loads(df_result.to_json(orient="records", force_ascii=False))
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ‰¹é‡è®¡ç®—å¤±è´¥: {str(e)}")


@app.post("/api/tunnel-support/parse-excel")
async def parse_tunnel_support_excel(file: UploadFile = File(...)):
    """
    è§£æä¸Šä¼ çš„å··é“å‚æ•°Excelæ–‡ä»¶
    
    è¿”å›è§£æåçš„æ•°æ®ï¼Œç”¨äºå‰ç«¯å±•ç¤ºå’Œç¼–è¾‘
    """
    if not file.filename or not (file.filename.endswith('.xlsx') or file.filename.endswith('.xls')):
        raise HTTPException(status_code=400, detail="è¯·ä¸Šä¼ Excelæ–‡ä»¶ (.xlsx æˆ– .xls)")
    
    try:
        content = await file.read()
        df = pd.read_excel(io.BytesIO(content))
        
        # éªŒè¯å¿…éœ€åˆ—
        required = ['B', 'H', 'åº”åŠ›é›†ä¸­ç³»æ•°K', 'åŸ‹æ·±', 'å®¹é‡', 'ç²˜èšåŠ›', 'å†…æ‘©æ“¦è§’']
        missing = [col for col in required if col not in df.columns]
        
        if missing:
            raise HTTPException(
                status_code=400, 
                detail=f"Excelæ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {', '.join(missing)}"
            )
        
        # æ ‡å‡†åŒ–åˆ—å
        column_map = {
            'åº”åŠ›é›†ä¸­ç³»æ•°K': 'K',
            'åŸ‹æ·±': 'depth',
            'å®¹é‡': 'gamma',
            'ç²˜èšåŠ›': 'C',
            'å†…æ‘©æ“¦è§’': 'phi'
        }
        
        df_renamed = df.rename(columns=column_map)
        
        return {
            "status": "success",
            "count": len(df_renamed),
            "columns": df_renamed.columns.tolist(),
            "data": json.loads(df_renamed.to_json(orient="records", force_ascii=False))
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")


@app.get("/api/tunnel-support/default-constants")
async def get_default_constants():
    """
    è·å–é»˜è®¤è®¡ç®—å¸¸é‡
    
    è¿”å›ç³»ç»Ÿé¢„è®¾çš„è®¡ç®—å‚æ•°ï¼Œç”¨äºå‰ç«¯æ˜¾ç¤ºå’Œç”¨æˆ·å‚è€ƒ
    """
    return {
        "status": "success",
        "constants": TunnelSupportCalculator.DEFAULT_CONSTANTS,
        "descriptions": {
            "Sn": "é”šç´¢æˆªé¢ç§¯ (mmÂ²)",
            "Rm_anchor": "é”šç´¢æŠ—æ‹‰å¼ºåº¦ (MPa)",
            "Rm_rod": "é”šæ†æŠ—æ‹‰å¼ºåº¦ (MPa)",
            "Q_anchor": "é”šç´¢è®¾è®¡è·è½½ (kN)",
            "Q_rod": "é”šæ†è®¾è®¡è·è½½ (kN)",
            "c0": "æ ‘è„‚é”šå›ºåŠ› (MPa)",
            "tau_rod": "é”šæ†é”šå›ºåŠ› (MPa)",
            "R_mm": "é”šç´¢åŠå¾„ (mm)",
            "D_mm": "é”šæ†ç›´å¾„ (mm)",
            "safety_K": "å®‰å…¨ç³»æ•°",
            "m": "é”šæ†(ç´¢)å·¥ä½œçŠ¶æ€ç³»æ•°",
            "n": "æ ¹æ•°"
        }
    }


# ============================================================================
# ç»Ÿè®¡åˆ†æ API ç«¯ç‚¹
# ============================================================================

class StatisticalAnalysisRequest(BaseModel):
    """ç»Ÿè®¡åˆ†æè¯·æ±‚æ¨¡å‹"""
    data: Dict[str, List[float]]
    columns: Optional[List[str]] = None


class CorrelationRequest(BaseModel):
    """ç›¸å…³æ€§åˆ†æè¯·æ±‚æ¨¡å‹"""
    data: Dict[str, List[float]]
    method: str = 'pearson'  # 'pearson' æˆ– 'spearman'
    columns: Optional[List[str]] = None


class RegressionRequest(BaseModel):
    """å›å½’åˆ†æè¯·æ±‚æ¨¡å‹"""
    x: List[float]
    y: List[float]
    x_label: Optional[str] = 'X'
    y_label: Optional[str] = 'Y'
    regression_type: str = 'linear'  # 'linear' æˆ– 'polynomial'
    polynomial_degree: int = 2


@app.post("/api/statistics/descriptive")
async def descriptive_statistics(request: StatisticalAnalysisRequest):
    """
    æè¿°æ€§ç»Ÿè®¡åˆ†æ
    
    è¿”å›æ¯ä¸ªå˜é‡çš„è¯¦ç»†ç»Ÿè®¡æŒ‡æ ‡ï¼šå‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ã€ååº¦ã€å³°åº¦ç­‰
    """
    try:
        result = analyze_descriptive_stats(request.data)
        return {
            "status": "success",
            "data": result
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç»Ÿè®¡åˆ†æå¤±è´¥: {str(e)}")


@app.post("/api/statistics/correlation")
async def correlation_analysis(request: CorrelationRequest):
    """
    ç›¸å…³æ€§åˆ†æ
    
    è®¡ç®—å˜é‡é—´çš„ç›¸å…³ç³»æ•°çŸ©é˜µï¼ˆPearson æˆ– Spearmanï¼‰
    è¿”å›ç›¸å…³ç³»æ•°çŸ©é˜µã€på€¼çŸ©é˜µå’Œæ˜¾è‘—ç›¸å…³å¯¹
    """
    try:
        if request.method not in ['pearson', 'spearman']:
            raise HTTPException(status_code=400, detail="method å¿…é¡»æ˜¯ 'pearson' æˆ– 'spearman'")
        
        result = analyze_correlation(request.data, method=request.method)
        return {
            "status": "success",
            "data": result
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç›¸å…³æ€§åˆ†æå¤±è´¥: {str(e)}")


@app.post("/api/statistics/regression")
async def regression_analysis(request: RegressionRequest):
    """
    å›å½’åˆ†æ
    
    æ”¯æŒçº¿æ€§å›å½’å’Œå¤šé¡¹å¼å›å½’
    è¿”å›å›å½’æ–¹ç¨‹ã€RÂ²ã€RMSEã€ç½®ä¿¡åŒºé—´ã€é¢„æµ‹åŒºé—´ç­‰
    """
    try:
        if len(request.x) != len(request.y):
            raise HTTPException(status_code=400, detail="x å’Œ y çš„é•¿åº¦å¿…é¡»ç›¸åŒ")
        
        if len(request.x) < 3:
            raise HTTPException(status_code=400, detail="æ•°æ®ç‚¹æ•°é‡ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦3ä¸ªç‚¹ï¼‰")
        
        result = analyze_regression(
            request.x, 
            request.y, 
            regression_type=request.regression_type,
            polynomial_degree=request.polynomial_degree
        )
        
        if 'error' in result:
            raise HTTPException(status_code=400, detail=result['error'])
        
        # æ·»åŠ æ ‡ç­¾ä¿¡æ¯
        result['x_label'] = request.x_label
        result['y_label'] = request.y_label
        
        return {
            "status": "success",
            "data": result
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å›å½’åˆ†æå¤±è´¥: {str(e)}")


@app.get("/api/statistics/methods")
async def get_statistical_methods():
    """
    è·å–æ”¯æŒçš„ç»Ÿè®¡æ–¹æ³•åˆ—è¡¨
    """
    return {
        "status": "success",
        "methods": {
            "descriptive": {
                "name": "æè¿°æ€§ç»Ÿè®¡",
                "description": "è®¡ç®—å‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ã€ååº¦ã€å³°åº¦ç­‰ç»Ÿè®¡æŒ‡æ ‡",
                "metrics": [
                    "count", "missing", "mean", "median", "mode", 
                    "std", "variance", "range", "iqr", "cv",
                    "min", "max", "q25", "q50", "q75",
                    "skewness", "kurtosis", "sem", "ci_lower", "ci_upper"
                ]
            },
            "correlation": {
                "name": "ç›¸å…³æ€§åˆ†æ",
                "description": "è®¡ç®—å˜é‡é—´çš„ç›¸å…³ç³»æ•°çŸ©é˜µ",
                "methods": ["pearson", "spearman"],
                "output": ["ç›¸å…³ç³»æ•°çŸ©é˜µ", "på€¼çŸ©é˜µ", "æ˜¾è‘—ç›¸å…³å¯¹"]
            },
            "regression": {
                "name": "å›å½’åˆ†æ",
                "description": "æ‹Ÿåˆå›å½’æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹",
                "types": ["linear", "polynomial"],
                "output": ["æ–¹ç¨‹", "RÂ²", "RMSE", "ç½®ä¿¡åŒºé—´", "é¢„æµ‹åŒºé—´", "æ®‹å·®åˆ†æ"]
            }
        }
    }

@app.post("/api/modeling/validate")
async def validate_modeling_endpoint(payload: ModelingValidationRequest):
    """
    éªŒè¯å»ºæ¨¡å¯è¡Œæ€§ï¼šæ£€æŸ¥æ•°æ®æ˜¯å¦æ»¡è¶³å»ºæ¨¡è¦æ±‚ã€‚
    è¿”å›è¯¦ç»†çš„éªŒè¯ç»“æœï¼ŒåŒ…æ‹¬æ•°æ®ç‚¹æ•°ã€åæ ‡å”¯ä¸€å€¼ã€å„ç…¤å±‚ç‚¹æ•°ç­‰ã€‚
    """
    try:
        modeling_state.ensure_loaded()
    except HTTPException as e:
        return {
            "valid": False,
            "error": e.detail,
            "details": {}
        }
    
    df = modeling_state.merged_df
    
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    missing_cols = []
    for col in [payload.x_col, payload.y_col, payload.thickness_col, payload.seam_col]:
        if col not in df.columns:
            missing_cols.append(col)
    
    if missing_cols:
        return {
            "valid": False,
            "error": f"æ•°æ®é›†ä¸­ç¼ºå°‘åˆ—: {', '.join(missing_cols)}",
            "details": {
                "available_columns": list(df.columns)
            }
        }
    
    # æ£€æŸ¥æœ‰æ•ˆæ•°æ®ç‚¹
    required_cols = [payload.x_col, payload.y_col, payload.thickness_col, payload.seam_col]
    valid_data = df.dropna(subset=required_cols).copy()
    total_valid_points = len(valid_data)
    
    if total_valid_points < 8:
        return {
            "valid": False,
            "error": f"ç”Ÿæˆå—ä½“è‡³å°‘éœ€è¦8ä¸ªæœ‰æ•ˆæ•°æ®ç‚¹ï¼Œå½“å‰ä»…æœ‰ {total_valid_points} ä¸ª",
            "details": {
                "total_rows": len(df),
                "valid_points": total_valid_points,
                "min_required": 8
            }
        }
    
    # æ£€æŸ¥åæ ‡å”¯ä¸€å€¼
    valid_data[payload.seam_col] = valid_data[payload.seam_col].astype(str)
    x_vals = valid_data[payload.x_col].astype(float)
    y_vals = valid_data[payload.y_col].astype(float)
    
    x_unique = x_vals.nunique()
    y_unique = y_vals.nunique()
    
    if x_unique < 2 or y_unique < 2:
        return {
            "valid": False,
            "error": f"X æˆ– Y åæ ‡å–å€¼è¿‡å°‘ï¼Œæ— æ³•æ„å»ºç½‘æ ¼ï¼ˆX: {x_unique} ä¸ªå”¯ä¸€å€¼, Y: {y_unique} ä¸ªå”¯ä¸€å€¼ï¼‰",
            "details": {
                "x_unique": x_unique,
                "y_unique": y_unique,
                "min_required": 2
            }
        }
    
    # æ£€æŸ¥å„ç…¤å±‚ç‚¹æ•°
    seam_stats = {}
    has_valid_seam = False
    
    for seam_name in payload.selected_seams:
        seam_df = valid_data[valid_data[payload.seam_col] == str(seam_name)]
        if seam_df.empty:
            seam_stats[seam_name] = {
                "points": 0,
                "valid": False,
                "message": "æ— æ•°æ®ç‚¹"
            }
        else:
            thickness_points = pd.to_numeric(seam_df[payload.thickness_col], errors='coerce')
            valid_mask = ~pd.isna(thickness_points)
            valid_count = valid_mask.sum()
            
            if valid_count < 1:
                seam_stats[seam_name] = {
                    "points": 0,
                    "valid": False,
                    "message": "åšåº¦æ•°æ®å…¨éƒ¨æ— æ•ˆ"
                }
            else:
                seam_stats[seam_name] = {
                    "points": int(valid_count),
                    "valid": True,
                    "message": "å¯å»ºæ¨¡"
                }
                has_valid_seam = True
    
    if not has_valid_seam:
        return {
            "valid": False,
            "error": "æ‰€é€‰å²©å±‚å‡æ— æœ‰æ•ˆæ•°æ®ç‚¹",
            "details": {
                "total_valid_points": total_valid_points,
                "seam_stats": seam_stats
            }
        }
    
    # å…¨éƒ¨éªŒè¯é€šè¿‡
    return {
        "valid": True,
        "message": "æ•°æ®æ»¡è¶³å»ºæ¨¡è¦æ±‚",
        "details": {
            "total_valid_points": total_valid_points,
            "x_unique": x_unique,
            "y_unique": y_unique,
            "seam_stats": seam_stats
        }
    }

@app.post("/api/export")
async def export_model_endpoint(payload: ExportRequest):
    """
    é€šç”¨å¯¼å‡ºæ¥å£ï¼šç”Ÿæˆ DXF æˆ– FLAC3D æ–‡ä»¶å¹¶ä½œä¸ºé™„ä»¶è¿”å›ã€‚
    å‰ç«¯å¯ä»¥ç›´æ¥ POST JSON åˆ°æ­¤æ¥å£ä»¥ä¸‹è½½æ–‡ä»¶ï¼ˆé€‚ç”¨äº web é¡µé¢ï¼‰ã€‚
    """
    modeling_state.ensure_loaded()
    df = modeling_state.merged_df

    # å‚æ•°éªŒè¯
    for col in [payload.x_col, payload.y_col, payload.thickness_col, payload.seam_col]:
        if col not in df.columns:
            raise HTTPException(status_code=404, detail=f"æ•°æ®é›†ä¸­ç¼ºå°‘åˆ—: {col}")

    # æ„é€ æ’å€¼åŒ…è£…å™¨
    def interpolation_wrapper(x, y, z, xi_flat, yi_flat):
        from interpolation import interpolate

        num_points = len(x)
        method_key = payload.method.lower()
        if num_points <= 3:
            method_key = 'nearest'
        
        # æ·»åŠ æ—¥å¿—ç”¨äºè°ƒè¯•
        print(f"[Export] ä½¿ç”¨æ’å€¼æ–¹æ³•: {method_key} (åŸå§‹è¯·æ±‚: {payload.method})")
        
        try:
            return interpolate(x, y, z, xi_flat, yi_flat, method_key)
        except Exception as e:
            print(f"[Export] æ’å€¼å¤±è´¥,å›é€€åˆ°nearest: {e}")
            return griddata((x, y), z, (xi_flat, yi_flat), method='nearest')

    # ç”Ÿæˆå—ä½“æ¨¡å‹
    try:
        block_models_objs, skipped, (XI, YI) = build_block_models(
            merged_df=df,
            seam_column=payload.seam_col,
            x_col=payload.x_col,
            y_col=payload.y_col,
            thickness_col=payload.thickness_col,
            selected_seams=payload.selected_seams,
            method_callable=interpolation_wrapper,
            resolution=payload.resolution or 80,
            base_level=payload.base_level or 0,
            gap_value=payload.gap or 0,
        )
    except ValueError as e:
        # å¸¸è§çš„å»ºæ¨¡è¾“å…¥é”™è¯¯ï¼ˆä¾‹å¦‚æ•°æ®ç‚¹ä¸è¶³ã€ç½‘æ ¼ä¸åŒ¹é…ç­‰ï¼‰ç”¨ 400 è¿”å›ï¼Œå¹¶å°†åŸå§‹é”™è¯¯æ¶ˆæ¯æš´éœ²ç»™å‰ç«¯
        # ä½¿ç”¨ str(e) ä¿è¯æ¶ˆæ¯ä¸ºå¯åºåˆ—åŒ–çš„å­—ç¬¦ä¸²
        raise HTTPException(status_code=400, detail=f"å»ºæ¨¡å¤±è´¥: {str(e)}")
    except Exception as e:
        # å…¶ä»–æœªé¢„æœŸå¼‚å¸¸è®°å½•åˆ°æ—¥å¿—å¹¶è¿”å› 500
        try:
            import traceback
            with open("export_error.log", "a", encoding="utf-8") as f:
                f.write(f"[{datetime.now()}] Unhandled build_block_models error: {str(e)}\n")
                traceback.print_exc(file=f)
        except Exception:
            pass
        raise HTTPException(status_code=500, detail=f"å»ºæ¨¡å¤±è´¥ï¼ˆå†…éƒ¨é”™è¯¯ï¼‰: {str(e)}")

    if not block_models_objs:
        raise HTTPException(status_code=400, detail="æœªèƒ½ç”Ÿæˆæ¨¡å‹æ•°æ®, æ— æ³•å¯¼å‡º")

    export_data = {"layers": []}
    for model in block_models_objs:
        if model.top_surface is None:
            continue
        
        # ç¡®ä¿æœ‰å®Œæ•´çš„åº•æ¿å’Œåšåº¦æ•°æ®ï¼ˆFLAC3D éœ€è¦ï¼‰
        bottom_surface = model.bottom_surface
        thickness = model.thickness_grid
        
        # å¦‚æœä¸¤è€…éƒ½ç¼ºå¤±ï¼Œä½¿ç”¨é»˜è®¤åšåº¦
        if bottom_surface is None and thickness is None:
            print(f"[Export] è­¦å‘Š: {model.name} ç¼ºå°‘åº•æ¿/åšåº¦æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤åšåº¦ 5.0m")
            thickness = np.full_like(model.top_surface, 5.0)
            bottom_surface = model.top_surface - thickness
        elif bottom_surface is None:
            # æœ‰åšåº¦ï¼Œè®¡ç®—åº•æ¿
            print(f"[Export] {model.name}: ä½¿ç”¨åšåº¦è®¡ç®—åº•æ¿")
            bottom_surface = model.top_surface - thickness
        elif thickness is None:
            # æœ‰åº•æ¿ï¼Œè®¡ç®—åšåº¦
            print(f"[Export] {model.name}: ä½¿ç”¨åº•æ¿è®¡ç®—åšåº¦")
            thickness = model.top_surface - bottom_surface
        
        export_data["layers"].append({
            "name": model.name,
            "grid_x": XI,
            "grid_y": YI,
            "grid_z": model.top_surface,
            "grid_z_bottom": bottom_surface,
            "thickness": thickness,
        })

    # ç¡®å®šå¯¼å‡ºå™¨
    export_type = (payload.export_type or 'dxf').lower()
    from exporters.dxf_exporter import DXFExporter
    from exporters.flac3d_exporter import FLAC3DExporter
    from exporters.stl_exporter import STLExporter
    from exporters.layered_stl_exporter import LayeredSTLExporter
    from datetime import datetime
    import traceback

    if payload.filename:
        filename = payload.filename
        # å¦‚æœæ–‡ä»¶ååŒ…å«ä¸­æ–‡ï¼Œè½¬æ¢ä¸ºæ‹¼éŸ³æˆ–è‹±æ–‡ï¼ˆé¿å…FLAC3Dä¹±ç ï¼‰
        if any('\u4e00' <= c <= '\u9fff' for c in filename):
            # åŒ…å«ä¸­æ–‡ï¼Œä½¿ç”¨è‹±æ–‡é»˜è®¤å
            if export_type == 'flac3d':
                ext = 'f3grid'
            elif export_type in ['stl', 'stl_single']:
                ext = 'stl'
            elif export_type == 'stl_layered':
                ext = 'zip'
            else:
                ext = 'dxf'
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"geological_model_{timestamp}.{ext}"
            print(f"[Export] æ£€æµ‹åˆ°ä¸­æ–‡æ–‡ä»¶åï¼Œè‡ªåŠ¨è½¬æ¢ä¸º: {filename}")
    else:
        if export_type == 'flac3d':
            ext = 'f3grid'
        elif export_type in ['stl', 'stl_single']:
            ext = 'stl'
        elif export_type == 'stl_layered':
            ext = 'zip'
        else:
            ext = 'dxf'
        filename = f"geological_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{ext}"

    output_dir = APP_ROOT.parent / 'data' / 'output'
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = str(output_dir / filename)

    try:
        # æå–å¯¼å‡ºé€‰é¡¹
        export_options = {}
        if hasattr(payload, 'options') and payload.options:
            export_options = payload.options
            print(f"[Export] ä½¿ç”¨è‡ªå®šä¹‰å¯¼å‡ºé€‰é¡¹: {export_options}")
        
        if export_type == 'dxf':
            exporter = DXFExporter()
            print(f"[Export] å¼€å§‹å¯¼å‡º DXF æ ¼å¼ï¼Œè¾“å‡ºè·¯å¾„: {output_path}")
            final_path = exporter.export(export_data, output_path, options=export_options)
        elif export_type == 'flac3d':
            exporter = FLAC3DExporter()
            print(f"[Export] å¼€å§‹å¯¼å‡º FLAC3D æ ¼å¼ï¼Œè¾“å‡ºè·¯å¾„: {output_path}")
            final_path = exporter.export(export_data, output_path, options=export_options)
        elif export_type in ['stl', 'stl_single']:
            # å•æ–‡ä»¶STLå¯¼å‡ºï¼ˆæ‰€æœ‰åœ°å±‚åˆå¹¶ï¼‰
            exporter = STLExporter()
            print(f"[Export] å¼€å§‹å¯¼å‡º STL æ ¼å¼ï¼ˆå•æ–‡ä»¶ï¼‰ï¼Œè¾“å‡ºè·¯å¾„: {output_path}")
            final_path = exporter.export(export_data, output_path, options=export_options)
        elif export_type == 'stl_layered':
            # åˆ†å±‚STLå¯¼å‡ºï¼ˆæ¯å±‚ä¸€ä¸ªæ–‡ä»¶ï¼Œæ‰“åŒ…ä¸ºZIPï¼‰
            exporter = LayeredSTLExporter()
            print(f"[Export] å¼€å§‹å¯¼å‡º STL æ ¼å¼ï¼ˆåˆ†å±‚ï¼‰ï¼Œè¾“å‡ºè·¯å¾„: {output_path}")
            final_path = exporter.export_layered(export_data, output_path, options=export_options)
        else:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„å¯¼å‡ºç±»å‹: {export_type}")

        print(f"[Export] å¯¼å‡ºå®Œæˆ: {final_path}")
    except ImportError as ie:
        # è®°å½•è¯¦ç»†é”™è¯¯
        error_msg = str(ie)
        print(f"[Export Error] ImportError: {error_msg}")
        try:
            with open("export_error.log", "a", encoding="utf-8") as f:
                f.write(f"[{datetime.now()}] ImportError: {error_msg}\n")
                traceback.print_exc(file=f)
        except:
            pass
        raise HTTPException(
            status_code=500, 
            detail=f"ä¾èµ–åº“ç¼ºå¤±: {error_msg}\n\nè¯·åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œ: pip install ezdxf==1.3.0"
        )
    except ValueError as ve:
        # æ•°æ®éªŒè¯é”™è¯¯
        error_msg = str(ve)
        print(f"[Export Error] ValueError: {error_msg}")
        try:
            with open("export_error.log", "a", encoding="utf-8") as f:
                f.write(f"[{datetime.now()}] ValueError: {error_msg}\n")
                traceback.print_exc(file=f)
        except:
            pass
        raise HTTPException(status_code=400, detail=f"æ•°æ®éªŒè¯å¤±è´¥: {error_msg}")
    except Exception as e:
        # è®°å½•è¯¦ç»†é”™è¯¯
        error_msg = str(e)
        print(f"[Export Error] Exception: {error_msg}")
        try:
            with open("export_error.log", "a", encoding="utf-8") as f:
                f.write(f"[{datetime.now()}] Export Error: {error_msg}\n")
                traceback.print_exc(file=f)
        except:
            pass
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºå¤±è´¥: {error_msg}")

    # è¿”å›æ–‡ä»¶æµ
    try:
        file_like = open(final_path, 'rb')
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ— æ³•è¯»å–å¯¼å‡ºæ–‡ä»¶: {e}")

    media_type = 'application/octet-stream'
    if final_path.lower().endswith('.dxf'):
        media_type = 'application/dxf'

    # ä½¿ç”¨ RFC 2231 ç¼–ç å¤„ç†ä¸­æ–‡æ–‡ä»¶å
    from urllib.parse import quote
    filename_encoded = quote(Path(final_path).name)
    headers = {
        'Content-Disposition': f'attachment; filename="{filename_encoded}"; filename*=UTF-8\'\'{filename_encoded}'
    }

    return StreamingResponse(file_like, media_type=media_type, headers=headers)
