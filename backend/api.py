# backend/api.py
import io
import os
import sys
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
import threading
import json
from scipy.interpolate import griddata, Rbf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# å¯¼å…¥ä¼˜åŒ–åçš„æ¨¡å—
from key_strata_calculator import calculate_key_strata_details as calculate_key_strata_optimized
from data_validation import validate_geological_data, GeologicalDataValidator
from upward_mining_feasibility import UpwardMiningFeasibility, process_borehole_csv_for_feasibility, batch_process_borehole_files, auto_calibrate_coefficients
from exporters.dxf_exporter import DXFExporter
from exporters.flac3d_exporter import FLAC3DExporter

# ==============================================================================
#  æ ¸å¿ƒè®¡ç®—å‡½æ•° - ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
# ==============================================================================
def calculate_key_strata_details(df_strata_above_coal, coal_seam_properties_df):
    """
    è®¡ç®—ç»™å®šç…¤å±‚ä¸Šè¦†å²©å±‚çš„å…³é”®å±‚ä¿¡æ¯ã€‚
    (ä½¿ç”¨ä¼˜åŒ–åçš„æ¨¡å— - æ€§èƒ½æå‡30%+)
    """
    # ç›´æ¥è°ƒç”¨ä¼˜åŒ–åçš„å®ç°
    return calculate_key_strata_optimized(df_strata_above_coal, coal_seam_properties_df)


def _read_csv_from_bytes(file_bytes: bytes) -> pd.DataFrame:
    """Read CSV content from raw bytes with robust encoding fallbacks."""
    encodings = ["utf-8-sig", "utf-8", "gbk"]
    last_error: Optional[Exception] = None
    for encoding in encodings:
        try:
            return pd.read_csv(io.BytesIO(file_bytes), encoding=encoding)
        except Exception as exc:  # pragma: no cover - relies on external files
            last_error = exc
            continue
    raise ValueError(f"æ— æ³•è§£æCSVæ–‡ä»¶: {last_error}")


def process_single_borehole_file(file_bytes: bytes, filename: str) -> Tuple[List[Dict[str, Any]], str, str]:
    """Process a single borehole CSV file and extract coal seam metrics."""

    processed_records: List[Dict[str, Any]] = []
    borehole_name = os.path.splitext(os.path.basename(filename or ""))[0] or "æœªçŸ¥é’»å­”"

    try:
        df = _read_csv_from_bytes(file_bytes)
    except Exception as exc:
        return [], f"æ–‡ä»¶ '{filename}' è¯»å–å¤±è´¥: {exc}", "error"

    df.dropna(how="all", inplace=True)
    if df.empty:
        return [], f"æ–‡ä»¶ '{filename}' ä¸ºç©ºã€‚", "warning"

    std_cols_map = {
        "åç§°": "å²©å±‚åç§°",
        "å²©å±‚åç§°": "å²©å±‚åç§°",
        "å²©å±‚": "å²©å±‚åç§°",
        "å²©æ€§": "å²©å±‚åç§°",
        "åšåº¦/m": "åšåº¦/m",
        "åšåº¦": "åšåº¦/m",
        "å¼¹æ€§æ¨¡é‡/GPa": "å¼¹æ€§æ¨¡é‡/GPa",
        "å¼¹æ€§æ¨¡é‡/Gpa": "å¼¹æ€§æ¨¡é‡/GPa",
        "å¼¹æ€§æ¨¡é‡": "å¼¹æ€§æ¨¡é‡/GPa",
        "å®¹é‡/kNÂ·m-3": "å®¹é‡/kNÂ·m-3",
        "å®¹é‡/kN*m-3": "å®¹é‡/kNÂ·m-3",
        "å®¹é‡": "å®¹é‡/kNÂ·m-3",
        "æŠ—æ‹‰å¼ºåº¦/MPa": "æŠ—æ‹‰å¼ºåº¦/MPa",
        "æŠ—æ‹‰å¼ºåº¦": "æŠ—æ‹‰å¼ºåº¦/MPa",
    }
    df.rename(columns=lambda c: std_cols_map.get(str(c).strip(), str(c).strip()), inplace=True)

    if "å²©å±‚åç§°" not in df.columns or "åšåº¦/m" not in df.columns:
        return [], f"æ–‡ä»¶ '{filename}' ç¼ºå°‘'å²©å±‚åç§°'æˆ–'åšåº¦/m'åˆ—ã€‚", "error"

    coal_indices = df[df["å²©å±‚åç§°"].astype(str).str.contains("ç…¤", na=False)].index.tolist()
    if not coal_indices:
        return [], f"åœ¨æ–‡ä»¶ '{filename}' ä¸­æœªæ‰¾åˆ°ç…¤å±‚ã€‚", "warning"

    for coal_idx in coal_indices:
        coal_row = df.iloc[coal_idx]
        coal_name = str(coal_row.get("å²©å±‚åç§°", "")).strip() or "æœªçŸ¥ç…¤å±‚"
        coal_thickness = pd.to_numeric(coal_row.get("åšåº¦/m"), errors="coerce")
        coal_thickness_val = float(coal_thickness) if pd.notna(coal_thickness) else None

        direct_roof_name = "N/A"
        direct_roof_thickness = None
        if coal_idx > 0:
            roof_row = df.iloc[coal_idx - 1]
            direct_roof_name = str(roof_row.get("å²©å±‚åç§°", "")).strip() or "N/A"
            roof_thickness = pd.to_numeric(roof_row.get("åšåº¦/m"), errors="coerce")
            direct_roof_thickness = float(roof_thickness) if pd.notna(roof_thickness) else None

        record: Dict[str, Any] = {
            "é’»å­”å": borehole_name,
            "ç…¤å±‚": coal_name,
            "ç…¤å±‚åšåº¦": round(coal_thickness_val, 2) if coal_thickness_val is not None else "N/A",
            "ç›´æ¥é¡¶å²©æ€§": direct_roof_name,
            "ç›´æ¥é¡¶åšåº¦": round(direct_roof_thickness, 2) if direct_roof_thickness is not None else "N/A",
        }

        df_above = df.iloc[:coal_idx].copy()
        coal_props_df = df.iloc[[coal_idx]].copy()

        key_info = []
        required_cols = ["å²©å±‚åç§°", "åšåº¦/m", "å¼¹æ€§æ¨¡é‡/GPa", "å®¹é‡/kNÂ·m-3", "æŠ—æ‹‰å¼ºåº¦/MPa"]
        if not df_above.empty and all(col in df_above.columns for col in required_cols):
            try:
                key_info = calculate_key_strata_details(df_above, coal_props_df)
            except Exception:
                key_info = []

        for index, info in enumerate(key_info[:4], start=1):
            record[f"å…³é”®å±‚{index}åšåº¦"] = info.get("åšåº¦", "N/A")
            record[f"å…³é”®å±‚{index}å²©æ€§"] = info.get("å²©æ€§", "N/A")
            record[f"å…³é”®å±‚{index}è·ç…¤å±‚çš„è·ç¦»"] = info.get("è·ç…¤å±‚è·ç¦»", "N/A")

        for fallback_index in range(len(key_info) + 1, 5):
            record.setdefault(f"å…³é”®å±‚{fallback_index}åšåº¦", "N/A")
            record.setdefault(f"å…³é”®å±‚{fallback_index}å²©æ€§", "N/A")
            record.setdefault(f"å…³é”®å±‚{fallback_index}è·ç…¤å±‚çš„è·ç¦»", "N/A")

        processed_records.append(record)

    return processed_records, f"æ–‡ä»¶ '{filename}' å¤„ç†å®Œæˆã€‚", "info"


def fill_missing_properties(df: pd.DataFrame, rock_db: pd.DataFrame, stat_preference: str = "median") -> Tuple[pd.DataFrame, int, List[str]]:
    """Fill missing mechanical properties using statistics from rock database."""

    if rock_db is None or rock_db.empty or df is None or df.empty:
        return df, 0, []

    stat_preference = (stat_preference or "median").lower()
    if stat_preference not in {"mean", "median"}:
        stat_preference = "median"

    lithology_col = None
    if "å²©å±‚åç§°" in df.columns:
        lithology_col = "å²©å±‚åç§°"
    elif "å²©æ€§" in df.columns:
        lithology_col = "å²©æ€§"
    if lithology_col is None or "å²©æ€§" not in rock_db.columns:
        return df, 0, []

    stats_group = rock_db.copy()
    stats_group["å²©æ€§"] = stats_group["å²©æ€§"].astype(str).str.strip()
    stats_group = stats_group[stats_group["å²©æ€§"].astype(bool)]

    aggregation: Dict[str, str] = {}
    numeric_columns = []
    for column in stats_group.columns:
        if column == "å²©æ€§":
            continue
        if pd.api.types.is_numeric_dtype(stats_group[column]):
            numeric_columns.append(column)
            aggregation[column] = stat_preference

    if not numeric_columns:
        return df, 0, []

    stats_map = stats_group.groupby("å²©æ€§").agg(aggregation)

    filled_df = df.copy()
    filled_count = 0
    filled_cols: set[str] = set()

    for idx, row in filled_df.iterrows():
        lithology = str(row.get(lithology_col, "")).strip()
        match_row = None
        if lithology:
            exact = stats_map.loc[stats_map.index == lithology]
            if not exact.empty:
                match_row = exact.iloc[0]
            elif "ç…¤" in lithology:
                coal_candidates = stats_map.loc[stats_map.index.str.contains("ç…¤")]
                if not coal_candidates.empty:
                    match_row = coal_candidates.iloc[0]

        if match_row is None:
            continue

        for column in numeric_columns:
            if column not in filled_df.columns:
                continue
            current = filled_df.at[idx, column]
            needs_fill = pd.isna(current)
            if not needs_fill:
                try:
                    needs_fill = float(current) == 0.0
                except Exception:
                    needs_fill = False
            if needs_fill:
                value = match_row.get(column)
                if pd.notna(value):
                    filled_df.at[idx, column] = value
                    filled_count += 1
                    filled_cols.add(column)

    return filled_df, filled_count, sorted(filled_cols)

# ==============================================================================
#  API ç±»
# ==============================================================================
class Api:
    def __init__(self):
        self.dfs = {}
        self.merged_df_modeling = None
        self.rock_db_cache = None
        self.china_geojson_cache = None
        # æ–°å¢ï¼šæ’å€¼æ–¹æ³•æ˜ å°„ - å®Œæ•´ç‰ˆæœ¬
        self.interpolation_methods = {
            # åŸºç¡€griddataæ–¹æ³•
            "linear": "çº¿æ€§ (Linear)",
            "cubic": "ä¸‰æ¬¡æ ·æ¡ (Cubic)",
            "nearest": "æœ€è¿‘é‚» (Nearest)",
            # RBFå¾„å‘åŸºå‡½æ•°æ–¹æ³•
            "multiquadric": "å¤šé‡äºŒæ¬¡ (Multiquadric)",
            "inverse": "åè·ç¦» (Inverse)",
            "gaussian": "é«˜æ–¯ (Gaussian)",
            "linear_rbf": "çº¿æ€§RBF (Linear RBF)",
            "cubic_rbf": "ä¸‰æ¬¡RBF (Cubic RBF)",
            "quintic_rbf": "äº”æ¬¡RBF (Quintic RBF)",
            "thin_plate": "è–„æ¿æ ·æ¡ (Thin Plate)",
            # é«˜çº§æ’å€¼æ–¹æ³•
            "modified_shepard": "ä¿®æ­£è°¢æ³¼å¾· (Modified Shepard)",
            "natural_neighbor": "è‡ªç„¶é‚»ç‚¹ (Natural Neighbor)",
            "radial_basis": "å¾„å‘åŸºå‡½æ•° (Radial Basis)",
            "ordinary_kriging": "æ™®é€šå…‹é‡Œé‡‘ (Ordinary Kriging)",
            "universal_kriging": "é€šç”¨å…‹é‡Œé‡‘ (Universal Kriging)",
            "bilinear": "åŒçº¿æ€§ (Bilinear)",
            "anisotropic": "å„å‘å¼‚æ€§ (Anisotropic)",
            "idw": "åè·ç¦»åŠ æƒ (IDW)"
        }

    def _get_raw_db_path(self):
        # ... (ä»£ç ä¸ä¹‹å‰ç›¸åŒ)
        pass

    def _read_raw_db(self):
        # ... (ä»£ç ä¸ä¹‹å‰ç›¸åŒ)
        pass

    def _perform_interpolation(self, x_train, y_train, z_train, x_val, y_val, method_key):
        """[ä¼˜åŒ–] ç»Ÿä¸€çš„æ’å€¼æ‰§è¡Œå‡½æ•° - å¢å¼ºç¨³å®šæ€§ï¼Œä½¿ç”¨interpolationæ¨¡å—"""
        from interpolation import interpolate
        
        # æ•°æ®éªŒè¯
        if x_train is None or y_train is None or z_train is None:
            raise ValueError("è®­ç»ƒæ•°æ®ä¸èƒ½ä¸ºNone")

        if len(x_train) == 0 or len(y_train) == 0 or len(z_train) == 0:
            raise ValueError("è®­ç»ƒæ•°æ®ä¸èƒ½ä¸ºç©º")

        if len(x_train) != len(y_train) or len(x_train) != len(z_train):
            raise ValueError(f"è®­ç»ƒæ•°æ®é•¿åº¦ä¸åŒ¹é…: x={len(x_train)}, y={len(y_train)}, z={len(z_train)}")

        # è¿‡æ»¤NaNå’ŒInfå€¼
        valid_mask = np.isfinite(x_train) & np.isfinite(y_train) & np.isfinite(z_train)
        if not np.any(valid_mask):
            raise ValueError("æ‰€æœ‰è®­ç»ƒæ•°æ®éƒ½æ˜¯æ— æ•ˆå€¼(NaNæˆ–Inf)")

        x_train = x_train[valid_mask]
        y_train = y_train[valid_mask]
        z_train = z_train[valid_mask]

        num_points = len(x_train)
        if num_points < 3:
            # æ•°æ®ç‚¹å¤ªå°‘,å¼ºåˆ¶ä½¿ç”¨æœ€è¿‘é‚»
            method_key = 'nearest'

        try:
            # ä½¿ç”¨å¢å¼ºçš„æ’å€¼æ¨¡å—
            result = interpolate(x_train, y_train, z_train, x_val, y_val, method_key)
            
            # æ£€æŸ¥ç»“æœ
            if result is None or (isinstance(result, np.ndarray) and result.size == 0):
                raise ValueError(f"{method_key}æ’å€¼è¿”å›ç©ºç»“æœ")
            
            # âš ï¸ å¤„ç†NaN/Infå€¼ - ä¸èƒ½è½¬ä¸º0,ä¼šå¯¼è‡´åšåº¦ä¸º0!
            if isinstance(result, np.ndarray):
                invalid_mask = ~np.isfinite(result)
                if np.any(invalid_mask):
                    # ç”¨åŸå§‹æ•°æ®çš„ä¸­ä½æ•°å¡«å……
                    fill_value = float(np.median(z_train)) if len(z_train) > 0 else 0.0
                    result = np.where(np.isfinite(result), result, fill_value)
            
            return result
            
        except Exception as e:
            # ä»»ä½•æ’å€¼å¤±è´¥éƒ½å›é€€åˆ°æœ€è¿‘é‚»
            print(f"[è­¦å‘Š] æ’å€¼æ–¹æ³• {method_key} å¤±è´¥: {e}, å›é€€åˆ°æœ€è¿‘é‚»æ’å€¼")
            try:
                result = griddata((x_train, y_train), z_train, (x_val, y_val), method='nearest')
                # ç”¨ä¸­ä½æ•°å¡«å……æ— æ•ˆå€¼
                fill_value = float(np.median(z_train)) if len(z_train) > 0 else 0.0
                result = np.where(np.isfinite(result), result, fill_value)
                return result
            except Exception as fallback_error:
                print(f"[é”™è¯¯] æœ€è¿‘é‚»æ’å€¼ä¹Ÿå¤±è´¥: {fallback_error}")
                # è¿”å›é›¶æ•°ç»„ä½œä¸ºæœ€åçš„å›é€€
                return np.zeros_like(x_val)

    def get_dashboard_stats(self) -> Dict[str, Any]:
        # ... (ä»£ç ä¸ä¹‹å‰ç›¸åŒ)
        pass
    
    def get_china_geojson(self) -> Dict[str, Any]:
        # ... (ä»£ç ä¸ä¹‹å‰ç›¸åŒ)
        pass

    # --- å…¶ä»–æ¨¡å—APIä¿æŒä¸å˜ ---

    # --- åœ°è´¨å»ºæ¨¡ API (æ–°å¢ä¸€ä¸ªæ–¹æ³•) ---
    def get_modeling_data_columns(self, filepaths: List[str], coords_path: str) -> Dict:
        # ... (ä»£ç ä¸ä¹‹å‰ç›¸åŒ)
        pass

    def get_unique_column_values(self, params: Dict) -> Dict:
        # ... (ä»£ç ä¸ä¹‹å‰ç›¸åŒ)
        pass

    def generate_contour_data(self, params: Dict) -> Dict:
        # ... (ä»£ç ä¸ä¹‹å‰ç›¸åŒ)
        pass

    def compare_interpolation_methods(self, params: Dict) -> Dict[str, Any]:
        """[æ–°å¢] å¯¹æ¯”æ‰€æœ‰æ’å€¼æ–¹æ³•çš„æ€§èƒ½"""
        if self.merged_df_modeling is None:
            return {'status': 'error', 'message': 'è¯·å…ˆåŠ è½½å¹¶åˆå¹¶æ•°æ®'}

        try:
            x_col, y_col, z_col = params['x_col'], params['y_col'], params['z_col']
            validation_ratio = params.get('validation_ratio', 0.2)
            
            df = self.merged_df_modeling.dropna(subset=[x_col, y_col, z_col])
            if len(df) < 10:
                return {'status': 'error', 'message': f'æ•°æ®ç‚¹ä¸è¶³ï¼ˆå°‘äº10ä¸ªï¼‰ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆéªŒè¯'}

            X = df[[x_col, y_col]].values
            y = df[z_col].values
            
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_ratio, random_state=42)

            x_train, y_train_coords = X_train[:, 0], X_train[:, 1]
            x_test, y_test_coords = X_test[:, 0], X_test[:, 1]

            results = []
            for key, name in self.interpolation_methods.items():
                try:
                    y_pred = self._perform_interpolation(x_train, y_train_coords, y_train, x_test, y_test_coords, key)
                    
                    # è¿‡æ»¤æ‰é¢„æµ‹å¤±è´¥çš„NaNå€¼
                    valid_mask = ~np.isnan(y_pred)
                    if not np.any(valid_mask):
                        continue

                    y_test_valid = y_test[valid_mask]
                    y_pred_valid = y_pred[valid_mask]

                    mae = mean_absolute_error(y_test_valid, y_pred_valid)
                    rmse = np.sqrt(mean_squared_error(y_test_valid, y_pred_valid))
                    r2 = r2_score(y_test_valid, y_pred_valid)
                    
                    results.append({
                        'method': name,
                        'mae': round(mae, 4),
                        'rmse': round(rmse, 4),
                        'r2': round(r2, 4),
                    })
                except Exception as e:
                    print(f"æ’å€¼æ–¹æ³• {name} å¤±è´¥: {e}")
                    continue
            
            if not results:
                return {'status': 'error', 'message': 'æ‰€æœ‰æ’å€¼æ–¹æ³•å‡è®¡ç®—å¤±è´¥'}

            # æŒ‰ RÂ² é™åºæ’åº
            results.sort(key=lambda x: x['r2'], reverse=True)
            
            return {'status': 'success', 'results': results}

        except Exception as e:
            return {'status': 'error', 'message': f'å¯¹æ¯”æ’å€¼æ–¹æ³•æ—¶å‡ºé”™: {e}'}

    def generate_block_model_data(self, params: Dict) -> Dict:
        """[ä¼˜åŒ–] ç”Ÿæˆ3Då—ä½“æ¨¡å‹æ•°æ® - å¢å¼ºç¨³å®šæ€§å’Œé”™è¯¯å¤„ç†"""
        try:
            # æ•°æ®éªŒè¯
            if self.merged_df_modeling is None:
                return {'status': 'error', 'message': 'æ•°æ®æœªåŠ è½½,è¯·å…ˆé€‰æ‹©æ–‡ä»¶'}

            # å‚æ•°éªŒè¯
            required_params = ['seam_col', 'x_col', 'y_col', 'thickness_col', 'selected_seams', 'method']
            for param in required_params:
                if param not in params or not params[param]:
                    return {'status': 'error', 'message': f'ç¼ºå°‘å¿…éœ€å‚æ•°: {param}'}

            if not isinstance(params['selected_seams'], list) or len(params['selected_seams']) == 0:
                return {'status': 'error', 'message': 'è‡³å°‘éœ€è¦é€‰æ‹©ä¸€ä¸ªå²©å±‚è¿›è¡Œå»ºæ¨¡'}

            from coal_seam_blocks.modeling import build_block_models

            def interpolation_wrapper(x, y, z, xi_flat, yi_flat):
                """åŒ…è£…æ’å€¼å‡½æ•°,å¢åŠ å¼‚å¸¸å¤„ç†"""
                method = params['method'].lower()
                print(f"[API] ğŸ¯ ç”¨æˆ·é€‰æ‹©çš„æ’å€¼æ–¹æ³•: {method}")
                try:
                    result = self._perform_interpolation(
                        x, y, z, xi_flat, yi_flat, method
                    )
                    print(f"[API] âœ… æ’å€¼æˆåŠŸ: method={method}, ç»“æœå½¢çŠ¶={result.shape if hasattr(result, 'shape') else len(result)}")
                    return result
                except Exception as e:
                    print(f"[API] âš ï¸ æ’å€¼å¤±è´¥: {e}, ä½¿ç”¨æœ€è¿‘é‚»æ–¹æ³•")
                    return self._perform_interpolation(
                        x, y, z, xi_flat, yi_flat, 'nearest'
                    )

            # è°ƒç”¨å»ºæ¨¡å‡½æ•°
            block_models_objs, skipped, (XI, YI) = build_block_models(
                merged_df=self.merged_df_modeling,
                seam_column=params['seam_col'],
                x_col=params['x_col'],
                y_col=params['y_col'],
                thickness_col=params['thickness_col'],
                selected_seams=params['selected_seams'],
                method_callable=interpolation_wrapper,
                resolution=params.get('resolution', 80),
                base_level=params.get('base_level', 0),
                gap_value=params.get('gap', 0)
            )

            # éªŒè¯ç»“æœ
            if not block_models_objs or len(block_models_objs) == 0:
                return {
                    'status': 'error',
                    'message': 'æœªèƒ½ç”Ÿæˆä»»ä½•å—ä½“æ¨¡å‹,å¯èƒ½æ˜¯æ•°æ®ç‚¹ä¸è¶³æˆ–æ’å€¼å¤±è´¥',
                    'skipped': skipped
                }

            # æ ¼å¼åŒ–æ¨¡å‹æ•°æ® - ä¿®å¤å‰åç«¯æ•°æ®æ ¼å¼ä¸åŒ¹é…é—®é¢˜
            models_data = []
            grid_x = XI[0].tolist() if XI.shape[0] > 0 else []
            grid_y = YI[:, 0].tolist() if YI.shape[1] > 0 else []

            for model in block_models_objs:
                # éªŒè¯æ¨¡å‹æ•°æ®
                if model.top_surface is None or model.bottom_surface is None:
                    print(f"[è­¦å‘Š] å²©å±‚ {model.name} çš„è¡¨é¢æ•°æ®ä¸ºç©º,è·³è¿‡")
                    continue

                # âš ï¸ å…³é”®ä¿®å¤ï¼šä¿ç•™NaNï¼Œä¸è¦è½¬æˆ0ï¼
                # åŸå› ï¼šå»ºæ¨¡æ—¶å„å±‚æ˜¯ä»åŸºå‡†é¢ç´¯åŠ çš„ï¼ŒNaNåº”è¯¥ä¿ç•™è®©å¯¼å‡ºå™¨å¤„ç†
                # å¦‚æœè½¬æˆ0ä¼šç ´åå±‚åºå…³ç³»ï¼Œå¯¼è‡´å±‚é—´äº¤é”™
                top_surface = model.top_surface.copy()
                bottom_surface = model.bottom_surface.copy()
                
                # åªå¤„ç†Infå€¼ï¼ˆä¿ç•™NaNï¼‰
                top_surface[np.isinf(top_surface)] = np.nan
                bottom_surface[np.isinf(bottom_surface)] = np.nan

                # ç¡®ä¿æ•°æ®ç»´åº¦æ­£ç¡®
                if top_surface.shape != XI.shape or bottom_surface.shape != XI.shape:
                    print(f"[è­¦å‘Š] å²©å±‚ {model.name} çš„è¡¨é¢æ•°æ®ç»´åº¦ä¸åŒ¹é…,è·³è¿‡")
                    continue

                # è½¬æ¢ä¸ºåˆ—è¡¨ï¼ŒNaNä¼šè¢«åºåˆ—åŒ–ä¸ºnullï¼ˆJSONæ ‡å‡†ï¼‰
                # å¯¼å‡ºå™¨ä¼šä½¿ç”¨æ’å€¼å¡«å……è¿™äº›nullå€¼
                models_data.append({
                    'name': str(model.name),
                    'points': int(model.points),
                    'grid_x': grid_x,  # å‰ç«¯æœŸå¾…çš„å­—æ®µå
                    'grid_y': grid_y,  # å‰ç«¯æœŸå¾…çš„å­—æ®µå
                    'top_surface_z': top_surface.tolist(),  # NaN â†’ nullï¼ˆä¿ç•™ç¼ºå¤±ä¿¡æ¯ï¼‰
                    'bottom_surface_z': bottom_surface.tolist(),  # NaN â†’ null
                    'avg_thickness': float(model.avg_thickness) if hasattr(model, 'avg_thickness') else 0.0,
                    'max_thickness': float(model.max_thickness) if hasattr(model, 'max_thickness') else 0.0,
                    'avg_height': float(model.avg_height) if hasattr(model, 'avg_height') else 0.0,
                })

            if len(models_data) == 0:
                return {
                    'status': 'error',
                    'message': 'æ‰€æœ‰æ¨¡å‹æ•°æ®éªŒè¯å¤±è´¥,æ— æ³•ç”Ÿæˆ3Dæ¨¡å‹',
                    'skipped': skipped
                }

            return {
                'status': 'success',
                'grid': {'x': grid_x, 'y': grid_y},
                'models': models_data,
                'skipped': skipped,
                'total_models': len(models_data),
                'total_skipped': len(skipped)
            }

        except ValueError as ve:
            return {'status': 'error', 'message': f'æ•°æ®éªŒè¯å¤±è´¥: {str(ve)}'}
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"[é”™è¯¯] 3Då»ºæ¨¡è®¡ç®—å¤±è´¥:\n{error_detail}")
            return {'status': 'error', 'message': f'3Då»ºæ¨¡è®¡ç®—å¤±è´¥: {str(e)}'}

    # --- ä¸Šè¡Œå¼€é‡‡å¯è¡Œåº¦è®¡ç®— API ---
    def calculate_upward_mining_feasibility(self, params: Dict) -> Dict:
        """
        è®¡ç®—å•ä¸ªé’»å­”çš„ä¸Šè¡Œå¼€é‡‡å¯è¡Œåº¦

        Args:
            params: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
                - csv_file_path: CSVæ–‡ä»¶è·¯å¾„
                - bottom_coal_name: å¼€é‡‡ç…¤å±‚åç§°
                - upper_coal_name: ä¸Šç…¤å±‚åç§°
                - lamda: å½±å“å› å­Î» (å¯é€‰, é»˜è®¤4.95)
                - C: åœ°è´¨å¸¸æ•°C (å¯é€‰, é»˜è®¤-0.84)

        Returns:
            è®¡ç®—ç»“æœå­—å…¸
        """
        try:
            # éªŒè¯å¿…éœ€å‚æ•°
            required_params = ['csv_file_path', 'bottom_coal_name', 'upper_coal_name']
            for param in required_params:
                if param not in params or not params[param]:
                    return {'status': 'error', 'message': f'ç¼ºå°‘å¿…éœ€å‚æ•°: {param}'}

            # è·å–å‚æ•°
            csv_file_path = params['csv_file_path']
            bottom_coal_name = params['bottom_coal_name']
            upper_coal_name = params['upper_coal_name']
            lamda = params.get('lamda', 4.95)
            C = params.get('C', -0.84)

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(csv_file_path):
                return {'status': 'error', 'message': f'æ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}'}

            # è°ƒç”¨æ‰°åŠ¨åº¦è®¡ç®—å‡½æ•°
            result = process_borehole_csv_for_feasibility(csv_file_path, bottom_coal_name, upper_coal_name, lamda, C)

            if "error" in result:
                return {'status': 'error', 'message': result['error']}

            return {
                'status': 'success',
                'data': result
            }

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"[é”™è¯¯] ä¸Šè¡Œå¼€é‡‡å¯è¡Œåº¦è®¡ç®—å¤±è´¥:\n{error_detail}")
            return {'status': 'error', 'message': f'è®¡ç®—å¤±è´¥: {str(e)}'}

    def batch_calculate_upward_mining_feasibility(self, params: Dict) -> Dict:
        """
        æ‰¹é‡è®¡ç®—é’»å­”çš„ä¸Šè¡Œå¼€é‡‡å¯è¡Œåº¦

        Args:
            params: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
                - csv_file_paths: CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨
                - bottom_coal_name: å¼€é‡‡ç…¤å±‚åç§°
                - upper_coal_name: ä¸Šç…¤å±‚åç§°
                - lamda: å½±å“å› å­Î» (å¯é€‰, é»˜è®¤4.95)
                - C: åœ°è´¨å¸¸æ•°C (å¯é€‰, é»˜è®¤-0.84)

        Returns:
            æ‰¹é‡è®¡ç®—ç»“æœå­—å…¸
        """
        try:
            # éªŒè¯å¿…éœ€å‚æ•°
            required_params = ['csv_file_paths', 'bottom_coal_name', 'upper_coal_name']
            for param in required_params:
                if param not in params or not params[param]:
                    return {'status': 'error', 'message': f'ç¼ºå°‘å¿…éœ€å‚æ•°: {param}'}

            # è·å–å‚æ•°
            csv_file_paths = params['csv_file_paths']
            bottom_coal_name = params['bottom_coal_name']
            upper_coal_name = params['upper_coal_name']
            lamda = params.get('lamda', 4.95)
            C = params.get('C', -0.84)

            # éªŒè¯æ–‡ä»¶åˆ—è¡¨
            if not isinstance(csv_file_paths, list) or len(csv_file_paths) == 0:
                return {'status': 'error', 'message': 'CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨ä¸èƒ½ä¸ºç©º'}

            # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            for file_path in csv_file_paths:
                if not os.path.exists(file_path):
                    return {'status': 'error', 'message': f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}'}

            # è°ƒç”¨æ‰¹é‡è®¡ç®—å‡½æ•°
            result = batch_process_borehole_files(csv_file_paths, bottom_coal_name, upper_coal_name, lamda, C)

            return {
                'status': 'success',
                'data': result
            }

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"[é”™è¯¯] æ‰¹é‡ä¸Šè¡Œå¼€é‡‡å¯è¡Œåº¦è®¡ç®—å¤±è´¥:\n{error_detail}")
            return {'status': 'error', 'message': f'æ‰¹é‡è®¡ç®—å¤±è´¥: {str(e)}'}

    def auto_calibrate_upward_mining_coefficients(self, params: Dict) -> Dict:
        """
        è‡ªåŠ¨æ ‡å®šä¸Šè¡Œå¼€é‡‡è®¡ç®—çš„ç³»æ•°(Î»å’ŒC)

        Args:
            params: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
                - csv_file_paths: CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨
                - bottom_coal_name: å¼€é‡‡ç…¤å±‚åç§°
                - upper_coal_name: ä¸Šç…¤å±‚åç§°
                - initial_lamda: åˆå§‹å½±å“å› å­Î» (å¯é€‰, é»˜è®¤4.95)
                - initial_C: åˆå§‹åœ°è´¨å¸¸æ•°C (å¯é€‰, é»˜è®¤-0.84)

        Returns:
            æ ‡å®šç»“æœå­—å…¸
        """
        try:
            # éªŒè¯å¿…éœ€å‚æ•°
            required_params = ['csv_file_paths', 'bottom_coal_name', 'upper_coal_name']
            for param in required_params:
                if param not in params or not params[param]:
                    return {'status': 'error', 'message': f'ç¼ºå°‘å¿…éœ€å‚æ•°: {param}'}

            # è·å–å‚æ•°
            csv_file_paths = params['csv_file_paths']
            bottom_coal_name = params['bottom_coal_name']
            upper_coal_name = params['upper_coal_name']
            initial_lamda = params.get('initial_lamda', 4.95)
            initial_C = params.get('initial_C', -0.84)

            # éªŒè¯æ–‡ä»¶åˆ—è¡¨
            if not isinstance(csv_file_paths, list) or len(csv_file_paths) == 0:
                return {'status': 'error', 'message': 'CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨ä¸èƒ½ä¸ºç©º'}

            # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            for file_path in csv_file_paths:
                if not os.path.exists(file_path):
                    return {'status': 'error', 'message': f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}'}

            # è°ƒç”¨è‡ªåŠ¨æ ‡å®šå‡½æ•°
            result = auto_calibrate_coefficients(csv_file_paths, bottom_coal_name, upper_coal_name, initial_lamda, initial_C)

            return {
                'status': 'success',
                'data': result
            }

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"[é”™è¯¯] è‡ªåŠ¨æ ‡å®šç³»æ•°å¤±è´¥:\n{error_detail}")
            return {'status': 'error', 'message': f'æ ‡å®šå¤±è´¥: {str(e)}'}

    def get_feasibility_evaluation_levels(self) -> Dict:
        """
        è·å–å¯è¡Œæ€§ç­‰çº§è¯„ä¼°æ ‡å‡†

        Returns:
            å¯è¡Œæ€§ç­‰çº§è¯„ä¼°æ ‡å‡†
        """
        return {
            'status': 'success',
            'levels': [
                {
                    'level': 'Içº§ (ä¸å¯è¡Œ/æå›°éš¾)',
                    'omega_range': '0-2',
                    'description': 'ç…¤å±‚åŸºæœ¬å¤„äºå®è½å¸¦å†…ï¼Œå®Œæ•´åº¦ç ´åéå¸¸ä¸¥é‡ï¼Œä¸Šè¡Œå¼€é‡‡ä¸åˆç†æˆ–è¿‡äºå±é™©ã€‚'
                },
                {
                    'level': 'IIçº§ (å›°éš¾)',
                    'omega_range': '2-4',
                    'description': 'ä¸Šè¡Œå¼€é‡‡éš¾åº¦å¤§ï¼Œæ˜“å‡ºç°é¡¶æ¿é—®é¢˜å’Œå··é“æ”¯æŠ¤å›°éš¾ï¼Œéœ€è¦é‡å‹æ”¯æŠ¤æˆ–å……å¡«ã€‚'
                },
                {
                    'level': 'IIIçº§ (å¯è¡Œï¼Œéœ€æ”¯æŠ¤)',
                    'omega_range': '4-6',
                    'description': 'ä¸­ç­‰ç ´åç¨‹åº¦ï¼Œé¡¶æ¿å’Œç…¤å±‚å°‘é‡ç ´ç¢ï¼Œä½†è£‚éš™å‘è‚²ç¨‹åº¦å¤§ã€‚æŠ€æœ¯ä¸Šå¯è¡Œï¼Œå±€éƒ¨éœ€åŠ å¼ºæ”¯æŠ¤ã€‚'
                },
                {
                    'level': 'IVçº§ (è‰¯å¥½)',
                    'omega_range': '6-8',
                    'description': 'è½»å¾®ç ´åï¼Œç…¤å±‚å®Œæ•´æ€§è‰¯å¥½ï¼Œé¡¶æ¿æœ‰å°‘é‡è£‚éš™ï¼Œä¸‹æ²‰é‡å¾®å°ï¼Œä¸Šè¡Œå¼€é‡‡æ•ˆæœè¾ƒå¥½ã€‚'
                },
                {
                    'level': 'Vçº§ (ä¼˜è‰¯)',
                    'omega_range': '8ä»¥ä¸Š',
                    'description': 'ç…¤å±‚åŸºæœ¬ä¸å—ä¸‹ç…¤å±‚å¼€é‡‡çš„å½±å“ï¼Œç…¤å±‚é—´çš„ç›¸äº’ä½œç”¨åŸºæœ¬ä¸å­˜åœ¨ï¼Œä¸Šè¡Œå¼€é‡‡ä¸å­˜åœ¨å›°éš¾ã€‚'
                }
            ]
        }

    def export_model(self, params: Dict) -> Dict:
        """
        å¯¼å‡ºåœ°è´¨æ¨¡å‹åˆ°å¤–éƒ¨æ ¼å¼ (DXF, FLAC3D)
        
        Args:
            params: åŒ…å«å»ºæ¨¡å‚æ•°å’Œå¯¼å‡ºé€‰é¡¹
                - export_type: 'dxf' æˆ– 'flac3d'
                - output_path: (å¯é€‰) è¾“å‡ºè·¯å¾„
                - ... (å…¶ä»–å»ºæ¨¡å‚æ•°åŒ generate_block_model_data)
        """
        try:
            # æ•°æ®éªŒè¯
            if self.merged_df_modeling is None:
                return {'status': 'error', 'message': 'æ•°æ®æœªåŠ è½½,è¯·å…ˆé€‰æ‹©æ–‡ä»¶'}

            # å‚æ•°éªŒè¯
            required_params = ['seam_col', 'x_col', 'y_col', 'thickness_col', 'selected_seams']
            for param in required_params:
                if param not in params or not params[param]:
                    return {'status': 'error', 'message': f'ç¼ºå°‘å¿…éœ€å‚æ•°: {param}'}

            from coal_seam_blocks.modeling import build_block_models

            def interpolation_wrapper(x, y, z, xi_flat, yi_flat):
                try:
                    return self._perform_interpolation(
                        x, y, z, xi_flat, yi_flat,
                        params.get('method', 'linear').lower()
                    )
                except Exception as e:
                    print(f"[è­¦å‘Š] å¯¼å‡ºæ—¶æ’å€¼å¤±è´¥: {e}, ä½¿ç”¨æœ€è¿‘é‚»æ–¹æ³•")
                    return self._perform_interpolation(
                        x, y, z, xi_flat, yi_flat, 'nearest'
                    )

            # è°ƒç”¨å»ºæ¨¡å‡½æ•°ç”Ÿæˆæ•°æ®
            block_models_objs, skipped, (XI, YI) = build_block_models(
                merged_df=self.merged_df_modeling,
                seam_column=params['seam_col'],
                x_col=params['x_col'],
                y_col=params['y_col'],
                thickness_col=params['thickness_col'],
                selected_seams=params['selected_seams'],
                method_callable=interpolation_wrapper,
                resolution=params.get('resolution', 80),
                base_level=params.get('base_level', 0),
                gap_value=params.get('gap', 0)
            )

            if not block_models_objs:
                return {
                    'status': 'error',
                    'message': 'æœªèƒ½ç”Ÿæˆæ¨¡å‹æ•°æ®, æ— æ³•å¯¼å‡º',
                    'skipped': skipped
                }

            # å‡†å¤‡å¯¼å‡ºæ•°æ®ç»“æ„
            export_data = {"layers": []}
            for model in block_models_objs:
                if model.top_surface is None:
                    continue
                    
                export_data["layers"].append({
                    "name": model.name,
                    "grid_x": XI,
                    "grid_y": YI,
                    "grid_z": model.top_surface,
                    "grid_z_bottom": model.bottom_surface,
                    "thickness": model.thickness_grid
                })

            # ç¡®å®šå¯¼å‡ºç±»å‹å’Œè·¯å¾„
            export_type = params.get("export_type", "dxf").lower()
            output_path = params.get("output_path")
            
            if not output_path:
                import os
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                ext = "f3grid" if export_type == "flac3d" else "dxf"
                
                user_filename = params.get("filename")
                if user_filename:
                    # ç§»é™¤å¯èƒ½åŒ…å«çš„è·¯å¾„ï¼Œåªä¿ç•™æ–‡ä»¶å
                    user_filename = os.path.basename(user_filename)
                    if not user_filename.lower().endswith(f".{ext}"):
                        user_filename += f".{ext}"
                    filename = user_filename
                else:
                    filename = f"model_export_{timestamp}.{ext}"

                # é»˜è®¤ä¿å­˜åˆ° backend/data/output æˆ–å½“å‰ç›®å½•
                output_dir = os.path.join(os.path.dirname(__file__), "data", "output")
                if not os.path.exists(output_dir):
                    os.makedirs(output_dir, exist_ok=True)
                output_path = os.path.join(output_dir, filename)

            # æ‰§è¡Œå¯¼å‡º
            exporter = None
            if export_type == "dxf":
                exporter = DXFExporter()
            elif export_type == "flac3d":
                exporter = FLAC3DExporter()
            else:
                return {'status': 'error', 'message': f'ä¸æ”¯æŒçš„å¯¼å‡ºç±»å‹: {export_type}'}
                
            final_path = exporter.export(export_data, output_path)
            
            return {
                'status': 'success', 
                'message': f'æ¨¡å‹å·²æˆåŠŸå¯¼å‡ºåˆ°: {final_path}',
                'file_path': final_path,
                'skipped_layers': skipped
            }
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            return {'status': 'error', 'message': f'å¯¼å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'}