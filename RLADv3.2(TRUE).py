"""
RLAD v3.1 (Optimized): åŸºäºSTL+LOFä¸å¼ºåŒ–å­¦ä¹ çš„äº¤äº’å¼æ¶²å‹æ”¯æ¶å·¥ä½œé˜»åŠ›å¼‚å¸¸æ£€æµ‹
é›†æˆäº†v2.4çš„å®Œæ•´å¯è§†åŒ–ä¸è¯„ä¼°æµç¨‹ï¼Œå¹¶ä¿ç•™äº†v3.0çš„æ ¸å¿ƒæ£€æµ‹é€»è¾‘ã€‚
"""

# åŸºç¡€åŠæ·±åº¦å­¦ä¹ åº“å¯¼å…¥
import os
import sys
import json
import time
import random
import warnings
import argparse
import traceback
from pathlib import Path
from datetime import datetime
from collections import deque, namedtuple
from typing import Optional, Tuple, List, Dict

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

# æ•°æ®å¤„ç†ä¸è¯„ä¼°åº“å¯¼å…¥
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (precision_score, recall_score, f1_score, confusion_matrix,
                           roc_curve, auc, precision_recall_curve, roc_auc_score,
                           average_precision_score, precision_recall_fscore_support)
from sklearn.manifold import TSNE
from sklearn.neighbors import LocalOutlierFactor
from statsmodels.tsa.seasonal import STL

# GUIåŠç»˜å›¾åº“å¯¼å…¥
import tkinter as tk
from tkinter import ttk, messagebox
import seaborn as sns
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

# =================================
# å…¨å±€é…ç½®
# =================================

# é…ç½®matplotlibä¸ºç§‘ç ”è®ºæ–‡é£æ ¼
plt.style.use('seaborn-v0_8-ticks')
plt.rcParams['figure.dpi'] = 150
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.family'] = 'serif'
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
plt.rcParams['legend.fontsize'] = 10
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.unicode_minus'] = False

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# =================================
# è¾…åŠ©å‡½æ•°
# =================================

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä¿è¯å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def convert_to_serializable(obj):
    """å°†numpy/torchç­‰å¯¹è±¡è½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„æ ¼å¼"""
    if isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, torch.Tensor):
        return obj.cpu().numpy().tolist()
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, Path):
        return str(obj)
    elif isinstance(obj, (datetime, pd.Timestamp)):
        return obj.isoformat()
    elif isinstance(obj, set):
        return list(obj)
    else:
        try:
            return str(obj) if not isinstance(obj, (int, float, bool, str, type(None))) else obj
        except Exception:
            return f"Unserializable object: {type(obj)}"

# =================================
# æ ¸å¿ƒæŒ‡æ ‡å¯è§†åŒ–ç±» (æ¥è‡ª v2.4)
# =================================

class CoreMetricsVisualizer:
    def __init__(self, output_dir="./output_visuals"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.colors = {
            'primary': '#0072B2', 'secondary': '#D55E00', 'tertiary': '#009E73',
            'accent': '#CC79A7', 'neutral': '#56B4E9', 'black': '#333333'
        }

    def _set_scientific_style(self, ax, title, xlabel, ylabel):
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.set_xlabel(xlabel, fontsize=12)
        ax.set_ylabel(ylabel, fontsize=12)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.grid(False)
        ax.tick_params(axis='both', which='major', labelsize=10)

    def plot_f1_score_training(self, training_history, save_path=None):
        fig, ax = plt.subplots(1, 1, figsize=(8, 5))
        episodes, val_f1 = training_history.get('episodes', []), training_history.get('val_f1', [])
        val_precision, val_recall = training_history.get('val_precision', []), training_history.get('val_recall', [])
        if not episodes or not val_f1: return
        ax.plot(episodes, val_f1, color=self.colors['black'], linestyle='-', linewidth=2, label='F1-Score')
        ax.plot(episodes, val_precision, color=self.colors['primary'], linestyle='--', linewidth=1.5, label='Precision')
        ax.plot(episodes, val_recall, color=self.colors['secondary'], linestyle=':', linewidth=1.5, label='Recall')
        self._set_scientific_style(ax, 'Validation Metrics During Training', 'Epoch', 'Score')
        ax.set_ylim(0, 1.05); ax.legend(loc='best', frameon=False)
        plt.tight_layout()
        if save_path is None: save_path = os.path.join(self.output_dir, 'training_metrics.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight'); plt.close()
        print(f"Training metrics plot saved to: {save_path}")

    def plot_roc_curve(self, y_true, y_scores, save_path=None):
        if len(np.unique(y_true)) < 2: return None
        fpr, tpr, _ = roc_curve(y_true, y_scores); roc_auc = auc(fpr, tpr)
        fig, ax = plt.subplots(1, 1, figsize=(6, 6))
        ax.plot(fpr, tpr, color=self.colors['primary'], lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')
        ax.plot([0, 1], [0, 1], color=self.colors['black'], lw=1.5, linestyle='--', label='Random Classifier')
        self._set_scientific_style(ax, 'Receiver Operating Characteristic (ROC)', 'False Positive Rate', 'True Positive Rate')
        ax.set_xlim([-0.05, 1.0]); ax.set_ylim([0.0, 1.05]); ax.legend(loc="lower right", frameon=False)
        plt.tight_layout()
        if save_path is None: save_path = os.path.join(self.output_dir, 'roc_curve.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight'); plt.close()
        print(f"ROC curve plot saved to: {save_path}"); return roc_auc

    def plot_final_metrics_bar(self, precision, recall, f1_score, auc_roc, save_path=None):
        metrics, values = ['AUC-ROC', 'F1-Score', 'Recall', 'Precision'], [auc_roc, f1_score, recall, precision]
        fig, ax = plt.subplots(figsize=(7, 5))
        bars = ax.barh(metrics, values, color=self.colors['primary'], height=0.6)
        self._set_scientific_style(ax, 'Final Model Performance', 'Score', 'Metric')
        ax.set_xlim(0, 1.0); ax.spines['left'].set_visible(False); ax.tick_params(axis='y', length=0)
        ax.grid(False)
        for bar in bars:
            width = bar.get_width()
            ax.text(width + 0.01, bar.get_y() + bar.get_height()/2, f'{width:.3f}', va='center', ha='left', fontsize=10)
        plt.tight_layout()
        if save_path is None: save_path = os.path.join(self.output_dir, 'final_metrics_summary.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight'); plt.close()
        print(f"Final metrics summary plot saved to: {save_path}")

    def plot_confusion_matrix(self, y_true, y_pred, save_path=None):
        cm = confusion_matrix(y_true, y_pred)
        fig, ax = plt.subplots(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False,
                    annot_kws={"size": 14}, linecolor='white', linewidths=1)
        self._set_scientific_style(ax, 'Confusion Matrix', 'Predicted Label', 'True Label')
        ax.set_xticklabels(['Normal', 'Anomaly']); ax.set_yticklabels(['Normal', 'Anomaly'], va='center', rotation=90)
        ax.grid(False)
        plt.tight_layout()
        if save_path is None: save_path = os.path.join(self.output_dir, 'confusion_matrix.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight'); plt.close()
        print(f"Confusion matrix plot saved to: {save_path}")

    def plot_precision_recall_curve(self, y_true, y_scores, save_path=None):
        if len(np.unique(y_true)) < 2: return
        precision, recall, _ = precision_recall_curve(y_true, y_scores)
        avg_precision = average_precision_score(y_true, y_scores)
        fig, ax = plt.subplots(figsize=(7, 6))
        ax.plot(recall, precision, color=self.colors['tertiary'], lw=2, label=f'PR Curve (AP = {avg_precision:.3f})')
        self._set_scientific_style(ax, 'Precision-Recall Curve', 'Recall', 'Precision')
        ax.set_xlim([0.0, 1.0]); ax.set_ylim([0.0, 1.05]); ax.legend(loc="best", frameon=False)
        plt.tight_layout()
        if save_path is None: save_path = os.path.join(self.output_dir, 'precision_recall_curve.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight'); plt.close()
        print(f"Precision-Recall curve plot saved to: {save_path}")

    def plot_prediction_scores_distribution(self, y_true, y_scores, save_path=None):
        fig, ax = plt.subplots(figsize=(8, 5))
        sns.kdeplot(y_scores[y_true == 0], ax=ax, color=self.colors['primary'], fill=True, label='Normal Scores')
        sns.kdeplot(y_scores[y_true == 1], ax=ax, color=self.colors['secondary'], fill=True, label='Anomaly Scores')
        self._set_scientific_style(ax, 'Prediction Score Distribution', 'Prediction Score (for Anomaly)', 'Density')
        ax.legend(frameon=False); plt.tight_layout()
        if save_path is None: save_path = os.path.join(self.output_dir, 'score_distribution.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight'); plt.close()
        print(f"Prediction score distribution plot saved to: {save_path}")

    def plot_tsne_features(self, features, y_true, save_path=None):
        print("Performing t-SNE... (this may take a moment)")
        if len(features) < 2: return
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)-1), n_jobs=-1)
        features_2d = tsne.fit_transform(np.array(features))
        df_tsne = pd.DataFrame({'t-SNE-1': features_2d[:, 0], 't-SNE-2': features_2d[:, 1], 'label': y_true})
        df_tsne['label'] = df_tsne['label'].map({0: 'Normal', 1: 'Anomaly'})
        fig, ax = plt.subplots(figsize=(8, 7))
        sns.scatterplot(data=df_tsne, x='t-SNE-1', y='t-SNE-2', hue='label',
                        palette={'Normal': self.colors['primary'], 'Anomaly': self.colors['secondary']},
                        style='label', ax=ax, s=50, hue_order=['Normal', 'Anomaly'])
        ax.legend(title='Class', frameon=False)
        self._set_scientific_style(ax, 't-SNE Visualization of Learned Features', 't-SNE Dimension 1', 't-SNE Dimension 2')
        plt.tight_layout()
        if save_path is None: save_path = os.path.join(self.output_dir, 'tsne_features.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight'); plt.close()
        print(f"t-SNE plot saved to: {save_path}")

    def plot_prediction_vs_actual(self, original_data, window_indices, true_labels, scores, window_size, save_path=None):
        fig, ax = plt.subplots(figsize=(15, 6))
        ax.plot(np.arange(len(original_data)), original_data, color=self.colors['black'], alpha=0.6, label='Original Signal', linewidth=1.0)
        window_centers = [idx + window_size // 2 for idx in window_indices]
        scatter = ax.scatter(window_centers, scores, c=scores, cmap='coolwarm', s=15, label='Anomaly Score', zorder=3)
        true_anomaly_indices = [i for i, label in enumerate(true_labels) if label == 1]
        for i in true_anomaly_indices:
            if i < len(window_indices):
                start_idx = window_indices[i]
                ax.axvspan(start_idx, start_idx + window_size, color=self.colors['secondary'], alpha=0.2, lw=0)
        cbar = plt.colorbar(scatter, ax=ax); cbar.set_label('Anomaly Score', fontsize=10)
        self._set_scientific_style(ax, 'Predicted Anomaly Score vs. Actual Anomalies', 'Time Step', 'Value')
        ax.legend(loc='upper right', frameon=False)
        plt.tight_layout()
        if save_path is None: save_path = os.path.join(self.output_dir, 'prediction_vs_actual.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight'); plt.close()
        print(f"Prediction vs. actual plot saved to: {save_path}")

    def plot_anomaly_heatmap(self, original_data, predictions, window_indices, window_size, save_path=None):
        heatmap_data = np.zeros(len(original_data)); count_map = np.zeros(len(original_data))
        for i, start_idx in enumerate(window_indices):
            score = predictions[i]
            end_idx = min(start_idx + window_size, len(heatmap_data))
            heatmap_data[start_idx:end_idx] += score
            count_map[start_idx:end_idx] += 1
        mask = count_map > 0
        heatmap_data[mask] /= count_map[mask]
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8), sharex=True, gridspec_kw={'height_ratios': [3, 1]})
        ax1.plot(original_data, color=self.colors['black'], alpha=0.7, linewidth=1.0)
        self._set_scientific_style(ax1, 'Original Time Series', '', 'Value')
        im = ax2.imshow(heatmap_data.reshape(1, -1), cmap='coolwarm', aspect='auto', interpolation='nearest', extent=[0, len(original_data), 0, 1])
        self._set_scientific_style(ax2, 'Anomaly Score Heatmap', 'Time Step', '')
        ax2.set_yticks([])
        cbar = fig.colorbar(im, ax=ax2, orientation='horizontal', pad=0.3); cbar.set_label('Anomaly Probability', fontsize=10)
        plt.tight_layout()
        if save_path is None: save_path = os.path.join(self.output_dir, 'anomaly_heatmap.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight'); plt.close()
        print(f"Anomaly detection heatmap saved to: {save_path}")

    def plot_attention_weights(self, agent, sample_data, device, save_path=None):
        agent.eval()
        with torch.no_grad():
            sample_tensor = torch.FloatTensor(sample_data).unsqueeze(0).to(device)
            _, _, attn_weights = agent(sample_tensor, return_features=True, return_attention_weights=True)
        agent.train()
        attn_weights = attn_weights.squeeze(0).mean(axis=0).cpu().numpy()
        fig, ax = plt.subplots(figsize=(12, 4))
        ax.bar(range(len(attn_weights)), attn_weights, color=self.colors['primary'])
        self._set_scientific_style(ax, 'Average Attention Weights Across Sequence', 'Sequence Position (Time Step)', 'Attention Weight')
        plt.tight_layout()
        if save_path is None: save_path = os.path.join(self.output_dir, 'attention_weights.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight'); plt.close()
        print(f"Attention weights visualization saved to: {save_path}")

    def plot_training_dashboard(self, training_history, save_path=None):
        if not training_history or 'episodes' not in training_history:
            print("âš ï¸ è®­ç»ƒå†å²ä¸ºç©ºï¼Œè·³è¿‡è®­ç»ƒé¢æ¿ç»˜åˆ¶")
            return
            
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        episodes = training_history['episodes']
        
        # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
        if not episodes:
            print("âš ï¸ æ²¡æœ‰episodeæ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–")
            return
            
        # 1. Training Loss - ä¿®å¤å­—æ®µå
        if 'losses' in training_history and training_history['losses']:
            # ç¡®ä¿losseså’Œepisodesé•¿åº¦åŒ¹é…
            losses = training_history['losses']
            loss_episodes = episodes[:len(losses)] if len(losses) < len(episodes) else episodes
            axes[0].plot(loss_episodes, losses, color=self.colors['secondary'])
            self._set_scientific_style(axes[0], 'Training Loss', 'Epoch', 'Loss')
        else:
            axes[0].text(0.5, 0.5, 'No Loss Data', ha='center', va='center', transform=axes[0].transAxes)
            axes[0].set_title('Training Loss (No Data)', fontsize=14)
            
        # 2. Validation Metrics
        if all(key in training_history for key in ['val_f1', 'val_precision', 'val_recall']):
            if training_history['val_f1']:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                axes[1].plot(episodes, training_history['val_f1'], color=self.colors['black'], label='F1-Score')
                axes[1].plot(episodes, training_history['val_precision'], color=self.colors['primary'], linestyle='--', label='Precision')
                axes[1].plot(episodes, training_history['val_recall'], color=self.colors['secondary'], linestyle=':', label='Recall')
                self._set_scientific_style(axes[1], 'Validation Metrics', 'Epoch', 'Score')
                axes[1].set_ylim(0, 1.05)
                axes[1].legend(frameon=False)
            else:
                axes[1].text(0.5, 0.5, 'No Validation Data', ha='center', va='center', transform=axes[1].transAxes)
                axes[1].set_title('Validation Metrics (No Data)', fontsize=14)
        else:
            axes[1].text(0.5, 0.5, 'No Validation Data', ha='center', va='center', transform=axes[1].transAxes)
            axes[1].set_title('Validation Metrics (No Data)', fontsize=14)
        
        # 3. Learning Rate
        if 'learning_rate' in training_history and training_history['learning_rate']:
            axes[2].plot(episodes, training_history['learning_rate'], color=self.colors['tertiary'], label='Learning Rate')
            axes[2].set_ylabel('Learning Rate', color=self.colors['tertiary'])
            axes[2].tick_params(axis='y', labelcolor=self.colors['tertiary'])
            self._set_scientific_style(axes[2], 'Learning Rate Schedule', 'Epoch', '')
            axes[2].set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦æ˜¾ç¤ºå­¦ä¹ ç‡
        else:
            axes[2].text(0.5, 0.5, 'No LR Data', ha='center', va='center', transform=axes[2].transAxes)
            axes[2].set_title('Learning Rate (No Data)', fontsize=14)
        
        # 4. AUC-ROC Evolution - ä¿®å¤å­—æ®µå¼•ç”¨
        if 'val_auc' in training_history and training_history['val_auc']:
            axes[3].plot(episodes, training_history['val_auc'], color=self.colors['primary'])
            self._set_scientific_style(axes[3], 'Validation AUC-ROC', 'Epoch', 'AUC')
            axes[3].set_ylim(0, 1.05)
        else:
            # å¦‚æœæ²¡æœ‰AUCæ•°æ®ï¼Œæ˜¾ç¤ºè®­ç»ƒè¿›åº¦ä¿¡æ¯
            best_f1 = max(training_history.get("val_f1", [0])) if training_history.get("val_f1") else 0
            axes[3].text(0.5, 0.5, f'Total Episodes: {len(episodes)}\nBest F1: {best_f1:.3f}', 
                        ha='center', va='center', transform=axes[3].transAxes, fontsize=12)
            axes[3].set_title('Training Summary', fontsize=14)
        
        plt.tight_layout()
        if save_path is None: 
            save_path = os.path.join(self.output_dir, 'training_dashboard.pdf')
        plt.savefig(save_path, format='pdf', bbox_inches='tight')
        plt.close()
        print(f"Training dashboard saved to: {save_path}")

    def generate_all_core_visualizations(self, training_history, final_metrics, original_data,
                                         window_indices, window_size, agent, sample_data, device):
        print("\nGenerating Core Metric Visualizations...")
        if training_history:
            self.plot_training_dashboard(training_history)
        
        y_true, y_scores, y_pred, features = final_metrics['labels'], final_metrics['probabilities'], final_metrics['predictions'], final_metrics['features']
        has_labels = len(y_true) > 0 and len(np.unique(y_true)) > 1
        
        auc_roc_value = None
        if has_labels:
            auc_roc_value = self.plot_roc_curve(y_true, y_scores)
            self.plot_precision_recall_curve(y_true, y_scores)
            self.plot_prediction_scores_distribution(y_true, y_scores)
            self.plot_confusion_matrix(y_true, y_pred)
            self.plot_tsne_features(features, y_true)
            self.plot_prediction_vs_actual(original_data, window_indices, y_true, final_metrics['all_probabilities'], window_size)

        self.plot_final_metrics_bar(final_metrics.get('precision', 0), final_metrics.get('recall', 0),
                                    final_metrics.get('f1', 0), auc_roc_value if auc_roc_value is not None else 0)
        
        scores_for_heatmap = final_metrics.get('all_probabilities', final_metrics.get('all_predictions'))
        if scores_for_heatmap is not None:
            self.plot_anomaly_heatmap(original_data, scores_for_heatmap, window_indices, window_size)
        
        self.plot_attention_weights(agent, sample_data, device)
        print("Core metric visualizations generated successfully!")

# =================================
# STL+LOFåŒå±‚å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ (æ¥è‡ª v3.0)
# =================================

class STLLOFAnomalyDetector:
    def __init__(self, period=24, seasonal=25, robust=True, n_neighbors=20, contamination=0.02):
        self.period = period
        # ç¡®ä¿seasonalæ˜¯å¥‡æ•°ä¸” >= 3
        if seasonal % 2 == 0:
            seasonal += 1
        self.seasonal = max(3, seasonal)
        
        # ç¡®ä¿seasonal > periodï¼Œè¿™æ˜¯STLçš„è¦æ±‚
        if self.seasonal <= self.period:
            self.seasonal = self.period + (2 - self.period % 2) + 1
            
        self.robust = robust
        self.n_neighbors = n_neighbors
        self.contamination = contamination
        
        print(f"ğŸ”§ STL+LOF Detector Initialized: STL(period={self.period}, seasonal={self.seasonal}), LOF(contamination={contamination})")
    def detect_anomalies(self, data):
        print("ğŸ”„ Running Enhanced STL+LOF point-wise anomaly detection...")
        series = pd.Series(data.flatten()).fillna(method='ffill').fillna(method='bfill')
        
        # ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿ
        if len(series) < 2 * self.period: 
            raise ValueError(f"Data length {len(series)} is too short for STL period {self.period}")
        
        try:
            # 1. STLåˆ†è§£
            stl_result = STL(series, seasonal=self.seasonal, period=self.period, robust=self.robust).fit()
            residuals = stl_result.resid.dropna()
            
            # ç¡®ä¿residualså’ŒåŸå§‹æ•°æ®é•¿åº¦ä¸€è‡´
            if len(residuals) != len(series):
                print(f"âš ï¸ STL residualsé•¿åº¦({len(residuals)})ä¸åŸå§‹æ•°æ®é•¿åº¦({len(series)})ä¸ä¸€è‡´ï¼Œè¿›è¡Œå¯¹é½...")
                # åˆ›å»ºä¸åŸå§‹æ•°æ®åŒé•¿åº¦çš„residuals
                aligned_residuals = pd.Series(index=series.index, dtype=float)
                aligned_residuals.loc[residuals.index] = residuals
                # ç”¨å‰åå€¼å¡«å……ç¼ºå¤±å€¼
                aligned_residuals = aligned_residuals.fillna(method='ffill').fillna(method='bfill')
                residuals = aligned_residuals
            
            # 2. å¤šé‡å¼‚å¸¸æ£€æµ‹ç­–ç•¥
            # ç­–ç•¥1: åŸºäºæ®‹å·®çš„LOF
            residuals_2d = residuals.values.reshape(-1, 1)
            if len(residuals_2d) < self.n_neighbors: 
                self.n_neighbors = max(5, len(residuals_2d) - 1)
            
            lof_model = LocalOutlierFactor(n_neighbors=self.n_neighbors, contamination=self.contamination)
            lof_labels = lof_model.fit_predict(residuals_2d)
            
            # ç­–ç•¥2: ç»Ÿè®¡é˜ˆå€¼æ³•ï¼ˆ3-sigmaè§„åˆ™å¢å¼ºç‰ˆï¼‰
            residual_mean = residuals.mean()
            residual_std = residuals.std()
            # ä½¿ç”¨åŠ¨æ€é˜ˆå€¼ï¼š2.5-sigmaåˆ°3.5-sigmaä¹‹é—´
            dynamic_threshold = residual_mean + 2.8 * residual_std
            statistical_anomalies = np.abs(residuals) > dynamic_threshold
            
            # ç­–ç•¥3: åŸºäºè¶‹åŠ¿å˜åŒ–çš„æ£€æµ‹
            trend = stl_result.trend.dropna()
            if len(trend) > 1:
                trend_diff = np.diff(trend)
                if len(trend_diff) > 0:
                    trend_threshold = np.percentile(np.abs(trend_diff), 95)  # 95%åˆ†ä½æ•°
                    trend_anomalies = np.abs(trend_diff) > trend_threshold
                    # å¯¹é½é•¿åº¦ï¼šåœ¨å¼€å¤´æ·»åŠ False
                    trend_anomalies = np.concatenate([[False], trend_anomalies])
                    
                    # å¦‚æœé•¿åº¦ä»ä¸åŒ¹é…ï¼Œæˆªæ–­æˆ–å¡«å……
                    if len(trend_anomalies) < len(residuals):
                        # å¡«å……Falseåˆ°æœ«å°¾
                        padding = np.zeros(len(residuals) - len(trend_anomalies), dtype=bool)
                        trend_anomalies = np.concatenate([trend_anomalies, padding])
                    elif len(trend_anomalies) > len(residuals):
                        # æˆªæ–­åˆ°åˆé€‚é•¿åº¦
                        trend_anomalies = trend_anomalies[:len(residuals)]
                else:
                    trend_anomalies = np.zeros(len(residuals), dtype=bool)
            else:
                trend_anomalies = np.zeros(len(residuals), dtype=bool)
            
            # ç»¼åˆå¤šç§ç­–ç•¥çš„ç»“æœ
            combined_scores = np.zeros(len(residuals))
            combined_scores += (lof_labels == -1).astype(float) * 0.4  # LOFæƒé‡40%
            combined_scores += statistical_anomalies.astype(float) * 0.35  # ç»Ÿè®¡æ–¹æ³•æƒé‡35%
            combined_scores += trend_anomalies.astype(float) * 0.25  # è¶‹åŠ¿å˜åŒ–æƒé‡25%
            
            # ä½¿ç”¨åŠ¨æ€é˜ˆå€¼ç¡®å®šæœ€ç»ˆå¼‚å¸¸
            final_threshold = 0.5  # ç»¼åˆåˆ†æ•°é˜ˆå€¼
            final_labels = (combined_scores > final_threshold).astype(int)
            
            # ç¡®ä¿è¿”å›ç»“æœä¸åŸå§‹æ•°æ®é•¿åº¦ä¸€è‡´
            if len(final_labels) != len(series):
                print(f"âš ï¸ æœ€ç»ˆæ ‡ç­¾é•¿åº¦({len(final_labels)})ä¸åŸå§‹æ•°æ®é•¿åº¦({len(series)})ä¸ä¸€è‡´ï¼Œè¿›è¡Œè°ƒæ•´...")
                full_labels = np.zeros(len(series), dtype=int)
                min_len = min(len(final_labels), len(series))
                full_labels[:min_len] = final_labels[:min_len]
                final_labels = full_labels
            
        except Exception as e:
            print(f"âš ï¸ STLåˆ†è§£è¿‡ç¨‹å‡ºé”™: {e}")
            # å®Œå…¨å¤‡ç”¨æ–¹æ³•ï¼šä»…ä½¿ç”¨ç»Ÿè®¡æ£€æµ‹
            data_mean = np.mean(series)
            data_std = np.std(series)
            threshold = data_mean + 3 * data_std
            final_labels = (np.abs(series - data_mean) > threshold).astype(int)
        
        anomaly_count = np.sum(final_labels)
        anomaly_rate = anomaly_count / len(final_labels)
        print(f"âœ… Enhanced STL+LOF detection complete. Found {anomaly_count} anomaly points ({anomaly_rate:.2%})")
        
        return final_labels
# =================================
# GUIäº¤äº’å¼æ ‡æ³¨ç•Œé¢ (æ¥è‡ª v3.0)
# =================================

class AnnotationGUI:
    def __init__(self, window_size=288):
        self.window_size = window_size
        self.result = None
        self.root = None

    def create_gui(self, window_data, window_idx, original_data_segment=None, auto_predicted_label=None):
        self.result = None
        if self.root:
            try: self.root.destroy()
            except: pass
        
        self.root = tk.Tk()
        self.root.title(f"Anomaly Annotation - Window #{window_idx}")
        self.root.geometry("1200x900")
        self.root.configure(bg='#f0f0f0')
        self.root.lift(); self.root.attributes('-topmost', True); self.root.after_idle(self.root.attributes, '-topmost', False)
        
        main_container = tk.Frame(self.root, bg='#f0f0f0'); main_container.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        tk.Label(main_container, text=f"Annotate Window #{window_idx}", font=('Arial', 18, 'bold'), bg='#f0f0f0', fg='#2c3e50').pack(pady=(0,10))
        
        # Chart
        fig = Figure(figsize=(11, 6), dpi=100); fig.patch.set_facecolor('#f0f0f0')
        ax1 = fig.add_subplot(211)
        ax1.plot(window_data.flatten(), 'b-', lw=1.5, label='Standardized Data'); ax1.set_title('Standardized Data', fontsize=11); ax1.grid(True, alpha=0.3); ax1.legend()
        ax2 = fig.add_subplot(212)
        if original_data_segment is not None:
            ax2.plot(original_data_segment.flatten(), 'r-', lw=1.5, label='Original Data'); ax2.set_title('Original Data', fontsize=11); ax2.grid(True, alpha=0.3); ax2.legend()
        fig.tight_layout(pad=2.0)
        canvas = FigureCanvasTkAgg(fig, main_container); canvas.draw(); canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # Buttons
        button_frame = tk.Frame(main_container, bg='#f0f0f0'); button_frame.pack(fill=tk.X, side=tk.BOTTOM, pady=15)
        tk.Button(button_frame, text="Normal (0)", font=('Arial', 14, 'bold'), bg='#27ae60', fg='white', width=15, height=2, command=lambda: self.set_result(0)).pack(side=tk.LEFT, padx=30, expand=True)
        tk.Button(button_frame, text="Anomaly (1)", font=('Arial', 14, 'bold'), bg='#e74c3c', fg='white', width=15, height=2, command=lambda: self.set_result(1)).pack(side=tk.LEFT, padx=30, expand=True)
        tk.Button(button_frame, text="Skip (s)", font=('Arial', 12), bg='#f39c12', fg='white', width=12, command=lambda: self.set_result(-1)).pack(side=tk.RIGHT, padx=15)
        tk.Button(button_frame, text="Quit (q)", font=('Arial', 12), bg='#95a5a6', fg='white', width=12, command=lambda: self.set_result(-2)).pack(side=tk.RIGHT, padx=15)
        
        self.root.bind('<Key-0>', lambda e: self.set_result(0)); self.root.bind('<Key-1>', lambda e: self.set_result(1))
        self.root.bind('<KeyPress-s>', lambda e: self.set_result(-1)); self.root.bind('<KeyPress-q>', lambda e: self.set_result(-2))
        self.root.protocol("WM_DELETE_WINDOW", self.on_close); self.root.focus_force()
        return self.root

    def set_result(self, result):
        self.result = result
        if self.root: self.root.quit(); self.root.destroy()

    def on_close(self):
        self.set_result(-2)

    def get_annotation(self, *args, **kwargs):
        try:
            root = self.create_gui(*args, **kwargs)
            if root is None: return self.get_annotation_fallback(*args, **kwargs)
            root.mainloop()
            return self.result if self.result is not None else -2
        except Exception: return self.get_annotation_fallback(*args, **kwargs)

    def get_annotation_fallback(self, window_data, window_idx, original_data_segment=None, auto_predicted_label=None):
        pred_text = f"AIé¢„æµ‹: {'å¼‚å¸¸' if auto_predicted_label == 1 else 'æ­£å¸¸'}" if auto_predicted_label is not None else ""
        while True:
            choice = input(f"è¯·å¯¹çª—å£ #{window_idx} è¿›è¡Œæ ‡æ³¨ (0=æ­£å¸¸, 1=å¼‚å¸¸, s=è·³è¿‡, q=é€€å‡º) {pred_text}: ").strip().lower()
            if choice == 'q': return -2
            if choice == 's': return -1
            if choice in ['0', '1']: return int(choice)

# =================================
# äººå·¥æ ‡æ³¨ç³»ç»Ÿ (æ¥è‡ª v3.0)
# =================================

class HumanAnnotationSystem:
    def __init__(self, output_dir: str, window_size: int, use_gui: bool):
        self.output_dir = output_dir
        self.use_gui = use_gui
        self.manual_labels_file = os.path.join(output_dir, 'manual_annotations.json')
        self.gui = AnnotationGUI(window_size) if use_gui else None
        self.annotation_history = self.load_existing_annotations()
        
    def load_existing_annotations(self):
        if not os.path.exists(self.manual_labels_file): return []
        try:
            with open(self.manual_labels_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
            print(f"âœ… å·²åŠ è½½ {len(history)} æ¡å†å²æ ‡æ³¨è®°å½•")
            return history
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å†å²æ ‡æ³¨è®°å½•æ—¶å‡ºé”™: {e}"); return []

    def save_annotations(self):
        try:
            with open(self.manual_labels_file, 'w', encoding='utf-8') as f:
                json.dump(self.annotation_history, f, ensure_ascii=False, indent=4, default=convert_to_serializable)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ ‡æ³¨è®°å½•æ—¶å‡ºé”™: {e}")

    def get_human_annotation(self, window_data, window_idx, original_data_segment=None, auto_predicted_label=None):
        for record in self.annotation_history:
            if record.get('window_idx') == window_idx:
                print(f"â†ªï¸ çª—å£ #{window_idx} å·²è¢«æ ‡æ³¨ä¸º: {record['label']}")
                return record['label']
        
        if self.use_gui and self.gui:
            label = self.gui.get_annotation(window_data, window_idx, original_data_segment, auto_predicted_label)
        else:
            label = self.gui.get_annotation_fallback(window_data, window_idx, original_data_segment, auto_predicted_label)
        
        if label in [0, 1]:
            self.annotation_history.append({'window_idx': window_idx, 'label': label, 'timestamp': datetime.now()})
            self.save_annotations()
            print(f"âœ… å·²æ ‡æ³¨çª—å£ #{window_idx} ä¸º: {'å¼‚å¸¸' if label == 1 else 'æ­£å¸¸'}")
        return label

# =================================
# æ•°æ®é›†ä¸æ•°æ®åŠ è½½ (æ¥è‡ª v3.0 é€»è¾‘)
# =================================
def augment_time_series(window, label, augment_prob=0.7):
    """ä¸ºå¼‚å¸¸æ ·æœ¬æ·»åŠ æ•°æ®å¢å¼º"""
    if label != 1 or np.random.random() > augment_prob:
        return window
        
    # é€‰æ‹©ä¸€ç§å¢å¼ºæ–¹æ³•
    augment_type = np.random.choice(['noise', 'shift', 'scale', 'flip'])
    
    if augment_type == 'noise':
        # æ·»åŠ éšæœºå™ªå£°
        noise_level = np.random.uniform(0.01, 0.05)
        return window + np.random.normal(0, noise_level, window.shape)
    elif augment_type == 'shift':
        # æ—¶é—´å¹³ç§»
        shift = np.random.randint(1, 10)
        shifted = np.roll(window, shift, axis=0)
        return shifted
    elif augment_type == 'scale':
        # æŒ¯å¹…ç¼©æ”¾
        scale = np.random.uniform(0.9, 1.1)
        return window * scale
    elif augment_type == 'flip':
        # æŒ¯å¹…ç¿»è½¬
        return -window
    
    return window
def extract_time_series_features(window):
    """æå–æ—¶é—´åºåˆ—ç‰¹å¾"""
    features = []
    
    # ç»Ÿè®¡ç‰¹å¾
    features.append(np.mean(window))
    features.append(np.std(window))
    features.append(np.max(window))
    features.append(np.min(window))
    features.append(np.median(window))
    
    # å½¢çŠ¶ç‰¹å¾
    features.append(np.mean(np.diff(window)))  # ä¸€é˜¶å·®åˆ†å‡å€¼
    features.append(np.std(np.diff(window)))   # ä¸€é˜¶å·®åˆ†æ ‡å‡†å·®
    
    try:
        # é¢‘åŸŸç‰¹å¾ - FFT
        fft_vals = np.abs(np.fft.rfft(window.flatten()))
        features.append(np.mean(fft_vals))
        features.append(np.std(fft_vals))
        features.append(np.max(fft_vals))
        
        # å³°å€¼é¢‘ç‡
        peak_freq = np.argmax(fft_vals)
        features.append(peak_freq)
    except:
        # æ·»åŠ é»˜è®¤å€¼
        features.extend([0, 0, 0, 0])
    
    # å¤æ‚åº¦ç‰¹å¾
    features.append(np.sum(np.abs(np.diff(window))))  # å˜åŒ–æ€»é‡
    
    # å¼‚å¸¸ç‰¹å¾
    q25, q75 = np.percentile(window, [25, 75])
    iqr = q75 - q25
    features.append(np.sum((window > q75 + 1.5*iqr) | (window < q25 - 1.5*iqr)))  # å¼‚å¸¸ç‚¹æ•°é‡
    
    return np.array(features, dtype=np.float32)
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y, raw_data=None, augment=False, extract_features=False):
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        self.X = torch.FloatTensor(X.astype(np.float32))
        self.y = torch.LongTensor(y)
        self.raw_data = torch.FloatTensor(raw_data.astype(np.float32)) if raw_data is not None else None
        self.augment = augment
        self.extract_features = extract_features
        
        # é¢„è®¡ç®—ç‰¹å¾
        if self.extract_features:
            self.features = []
            for i in range(len(X)):
                self.features.append(extract_time_series_features(X[i]))
            self.features = torch.FloatTensor(np.array(self.features))
        
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        x = self.X[idx]
        y = self.y[idx]
        
        # åº”ç”¨æ•°æ®å¢å¼º
        if self.augment and y != -1:
            x_np = x.numpy()
            x_np = augment_time_series(x_np, y.item())
            x = torch.FloatTensor(x_np)
            
        raw_data_item = self.raw_data[idx] if self.raw_data is not None else torch.zeros_like(x)
        
        # è¿”å›é¢å¤–ç‰¹å¾
        if self.extract_features:
            return x, y, raw_data_item, self.features[idx]
        
        return x, y, raw_data_item
# æ·»åŠ åœ¨æ•°æ®å¤„ç†éƒ¨åˆ†

def identify_transition_windows(labels, window_size=10):
    """è¯†åˆ«æ ‡ç­¾è½¬å˜çš„çª—å£"""
    transitions = []
    for i in range(len(labels) - window_size):
        window_labels = labels[i:i+window_size]
        # æ£€æŸ¥çª—å£ä¸­æ˜¯å¦åŒæ—¶åŒ…å«0å’Œ1
        if 0 in window_labels and 1 in window_labels:
            transitions.append(i)
    return transitions

def apply_expert_rules(window_data, raw_data=None):
    """åº”ç”¨æ¶²å‹æ”¯æ¶é¢†åŸŸä¸“å®¶è§„åˆ™"""
    anomaly_score = 0.0
    
    # ç¤ºä¾‹è§„åˆ™1ï¼šæ£€æŸ¥çªå‘å³°å€¼
    data = window_data.flatten()
    mean_val = np.mean(data)
    std_val = np.std(data)
    peak_threshold = mean_val + 3 * std_val
    
    # è®¡ç®—è¶…è¿‡é˜ˆå€¼çš„ç‚¹æ•°æ¯”ä¾‹
    peak_ratio = np.sum(data > peak_threshold) / len(data)
    if peak_ratio > 0.05:
        anomaly_score += 0.3
    
    # ç¤ºä¾‹è§„åˆ™2ï¼šæ£€æŸ¥çªç„¶ä¸‹é™
    diffs = np.diff(data)
    sudden_drops = np.sum(diffs < -2 * std_val) / len(diffs)
    if sudden_drops > 0.03:
        anomaly_score += 0.25
    
    # ç¤ºä¾‹è§„åˆ™3ï¼šæ£€æŸ¥æŒç»­ä½å€¼
    low_threshold = mean_val - 2 * std_val
    low_periods = 0
    current_period = 0
    
    for val in data:
        if val < low_threshold:
            current_period += 1
        else:
            if current_period > 5:  # è‡³å°‘è¿ç»­5ä¸ªç‚¹ä½äºé˜ˆå€¼
                low_periods += 1
            current_period = 0
    
    if current_period > 5:  # æ£€æŸ¥æœ€åä¸€æ®µ
        low_periods += 1
        
    if low_periods > 0:
        anomaly_score += 0.2
    
    # è¿”å›å¼‚å¸¸åˆ†æ•° (0.0-1.0)
    return min(1.0, anomaly_score)


def load_hydraulic_data_with_stl_lof(data_path, window_size, stride, specific_feature_column,
                                     stl_period=24, lof_contamination=0.02, unlabeled_fraction=0.1):
    """ä½¿ç”¨STL+LOFè¿›è¡Œå¼‚å¸¸æ£€æµ‹çš„æ•°æ®åŠ è½½å‡½æ•° - æ”¹è¿›ç‰ˆæœ¬"""
    print(f"ğŸ“¥ Loading data: {data_path}")
    
    # è¯»å–æ•°æ®
    df = pd.read_csv(data_path)
    
    # ç‰¹æ®Šå¤„ç†ï¼šé‡å‘½å1#æ”¯æ¶ä¸º102#æ”¯æ¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if '1#' in df.columns and '102#' not in df.columns:
        df = df.rename(columns={'1#': '102#'})
        print("âœ… å·²å°†1#æ”¯æ¶é‡å‘½åä¸º102#æ”¯æ¶")
    
    # é€‰æ‹©ç‰¹å¾åˆ—
    if specific_feature_column:
        if specific_feature_column not in df.columns:
            raise ValueError(f"âŒ æŒ‡å®šçš„ç‰¹å¾åˆ— '{specific_feature_column}' ä¸å­˜åœ¨")
        selected_cols = [specific_feature_column]
    else:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        selected_cols = [col for col in numeric_cols if not col.startswith('Unnamed')]
        if not selected_cols:
            raise ValueError("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ•°å€¼åˆ—")
    
    print(f"â¡ï¸ Selected feature column: {selected_cols[0]} (shape will be: 1D)")
    
    # æ•°æ®é¢„å¤„ç† - ç¡®ä¿å§‹ç»ˆä¸º1D
    data_values = df[selected_cols].fillna(method='ffill').fillna(method='bfill').fillna(0).values
    if data_values.ndim > 1:
        data_values = data_values.flatten()
    
    print(f"ğŸ“Š Data shape after processing: {data_values.shape}")
    
    # STL+LOFå¼‚å¸¸æ£€æµ‹ - ä¿®å¤å‚æ•°åå’Œå­£èŠ‚æ€§è®¾ç½®
    # ç¡®ä¿seasonalå‚æ•°æ˜¯å¥‡æ•°ä¸”å¤§äºperiod
    seasonal_param = max(7, stl_period // 2)
    if seasonal_param % 2 == 0:
        seasonal_param += 1
    if seasonal_param <= stl_period:
        seasonal_param = stl_period + 2
    
    detector = STLLOFAnomalyDetector(
        period=stl_period,
        seasonal=seasonal_param,
        robust=True,
        n_neighbors=20,
        contamination=lof_contamination
    )
    
    try:
        point_anomaly_labels = detector.detect_anomalies(data_values)
    except Exception as e:
        print(f"âš ï¸ STL+LOFæ£€æµ‹å¤±è´¥: {e}")
        print("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ£€æµ‹æ–¹æ³•...")
        # å¤‡ç”¨æ–¹æ³•ï¼šç®€å•çš„ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
        data_mean = np.mean(data_values)
        data_std = np.std(data_values)
        threshold = data_mean + 3 * data_std
        point_anomaly_labels = (np.abs(data_values - data_mean) > threshold).astype(int)
        print(f"âœ… å¤‡ç”¨æ£€æµ‹å®Œæˆï¼Œå‘ç° {np.sum(point_anomaly_labels)} ä¸ªå¼‚å¸¸ç‚¹")
    
    # æ ‡å‡†åŒ–å¤„ç†
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data_values.reshape(-1, 1)).flatten()
    
    print("ğŸ”„ Creating sliding windows...")
    windows_scaled, windows_raw, window_anomaly_labels, window_indices = [], [], [], []
    for i in range(0, len(data_scaled) - window_size + 1, stride):
        windows_scaled.append(data_scaled[i:i + window_size])
        windows_raw.append(data_values[i:i + window_size])
        window_anomaly_labels.append(point_anomaly_labels[i:i + window_size])
        window_indices.append(i)
    
    # æ”¹è¿›çš„çª—å£å¼‚å¸¸æ ‡ç­¾é€»è¾‘
    def compute_window_label(window_anomalies):
        """è®¡ç®—çª—å£æ ‡ç­¾çš„æ”¹è¿›ç­–ç•¥"""
        anomaly_ratio = np.mean(window_anomalies)
        anomaly_count = np.sum(window_anomalies)
        window_length = len(window_anomalies)
        
        # å¤šé‡åˆ¤æ–­å‡†åˆ™
        # å‡†åˆ™1: å¼‚å¸¸ç‚¹æ•°é‡é˜ˆå€¼ï¼ˆç»å¯¹æ•°é‡ï¼‰
        min_anomaly_threshold = max(2, window_length // 144)  # è‡³å°‘2ä¸ªç‚¹æˆ–çª—å£é•¿åº¦çš„1/144
        
        # å‡†åˆ™2: å¼‚å¸¸æ¯”ä¾‹é˜ˆå€¼ï¼ˆç›¸å¯¹æ¯”ä¾‹ï¼‰
        ratio_threshold = 0.015  # 1.5%çš„å¼‚å¸¸æ¯”ä¾‹
        
        # å‡†åˆ™3: é«˜å¯†åº¦å¼‚å¸¸é˜ˆå€¼ï¼ˆè¿ç»­å¼‚å¸¸ï¼‰
        consecutive_anomalies = 0
        max_consecutive = 0
        for point in window_anomalies:
            if point == 1:
                consecutive_anomalies += 1
                max_consecutive = max(max_consecutive, consecutive_anomalies)
            else:
                consecutive_anomalies = 0
        
        # ç»¼åˆåˆ¤æ–­é€»è¾‘
        if anomaly_count >= min_anomaly_threshold and anomaly_ratio >= ratio_threshold:
            return 1  # åŒæ—¶æ»¡è¶³æ•°é‡å’Œæ¯”ä¾‹è¦æ±‚
        elif anomaly_count >= 5:  # æˆ–è€…å¼‚å¸¸ç‚¹æ•°é‡å¾ˆå¤šï¼ˆ>=5ä¸ªï¼‰
            return 1
        elif anomaly_ratio >= 0.04:  # æˆ–è€…å¼‚å¸¸æ¯”ä¾‹å¾ˆé«˜ï¼ˆ>=4%ï¼‰
            return 1
        elif max_consecutive >= 3:  # æˆ–è€…æœ‰è¿ç»­3ä¸ªä»¥ä¸Šå¼‚å¸¸ç‚¹
            return 1
        else:
            return 0
    
    # ç”Ÿæˆåˆå§‹æ ‡ç­¾
    y_initial = np.array([compute_window_label(labels) for labels in window_anomaly_labels])
    print(f"ğŸ“Š Initial labels (Enhanced): Normal={np.sum(y_initial==0)}, Anomaly={np.sum(y_initial==1)}")
    
    # æ•°æ®å¹³è¡¡æ€§æ£€æŸ¥å’Œè°ƒæ•´
    normal_count = np.sum(y_initial == 0)
    anomaly_count = np.sum(y_initial == 1)
    total_count = len(y_initial)
    anomaly_rate = anomaly_count / total_count if total_count > 0 else 0
    
    print(f"ğŸ“ˆ Current anomaly rate: {anomaly_rate:.2%}")
    
    # å¦‚æœå¼‚å¸¸æ ·æœ¬è¿‡å°‘ï¼Œä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•è¿›è¡Œè°ƒæ•´
    if anomaly_count == 0 or anomaly_rate < 0.02:  # å¼‚å¸¸ç‡ä½äº2%
        print("âš ï¸ å¼‚å¸¸æ ·æœ¬è¿‡å°‘ï¼Œä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•è°ƒæ•´...")
        
        # è®¡ç®—æ¯ä¸ªçª—å£çš„å¼‚å¸¸åˆ†æ•°ï¼ˆåŠ æƒåˆ†æ•°ï¼‰
        window_scores = []
        for labels in window_anomaly_labels:
            # åŠ æƒå¼‚å¸¸åˆ†æ•°ï¼šè€ƒè™‘å¼‚å¸¸ç‚¹ä½ç½®å’Œå¯†åº¦
            score = 0
            for i, label in enumerate(labels):
                if label == 1:
                    # çª—å£ä¸­é—´çš„å¼‚å¸¸ç‚¹æƒé‡æ›´é«˜
                    position_weight = 1.0 + 0.5 * np.exp(-((i - len(labels)/2) / (len(labels)/4))**2)
                    score += position_weight
            window_scores.append(score)
        
        window_scores = np.array(window_scores)
        
        # åŠ¨æ€é€‰æ‹©é˜ˆå€¼ï¼šç¡®ä¿æœ‰5-15%çš„å¼‚å¸¸æ ·æœ¬
        target_anomaly_rate = 0.08  # ç›®æ ‡8%å¼‚å¸¸ç‡
        percentile_threshold = 100 * (1 - target_anomaly_rate)
        score_threshold = np.percentile(window_scores, percentile_threshold)
        
        # å¦‚æœé˜ˆå€¼ä¸º0ï¼Œä½¿ç”¨æœ€å°æ­£å€¼
        if score_threshold <= 0:
            positive_scores = window_scores[window_scores > 0]
            if len(positive_scores) > 0:
                score_threshold = np.percentile(positive_scores, 70)  # å–æ­£å€¼ä¸­çš„70%åˆ†ä½æ•°
            else:
                score_threshold = 0.1  # é»˜è®¤æœ€å°é˜ˆå€¼
        
        y_adjusted = np.array([1 if score >= score_threshold and score > 0 else 0 for score in window_scores])
        
        print(f"ğŸ“Š Score threshold: {score_threshold:.2f}")
        print(f"ğŸ“Š Adjusted labels: Normal={np.sum(y_adjusted==0)}, Anomaly={np.sum(y_adjusted==1)}")
        
        y_final = y_adjusted
    else:
        y_final = y_initial
    
    # æ”¹è¿›çš„æ•°æ®å¹³è¡¡æ£€æŸ¥å’Œè°ƒæ•´
    final_normal_count = np.sum(y_final == 0)
    final_anomaly_count = np.sum(y_final == 1)
    final_anomaly_rate = final_anomaly_count / len(y_final) if len(y_final) > 0 else 0
    
    print(f"ğŸ“Š Final balanced labels: Normal={final_normal_count}, Anomaly={final_anomaly_count}")
    print(f"ğŸ“ˆ Final anomaly rate: {final_anomaly_rate:.2%}")
    
    # å¦‚æœå¼‚å¸¸æ ·æœ¬å¤ªå°‘ï¼Œå¼ºåˆ¶åˆ›å»ºä¸€äº›å¼‚å¸¸æ ·æœ¬
    min_anomaly_samples = max(10, len(y_final) // 50)  # è‡³å°‘10ä¸ªå¼‚å¸¸æ ·æœ¬ï¼Œæˆ–æ€»æ•°çš„2%
    
    if final_anomaly_count < min_anomaly_samples:
        print(f"âš ï¸ å¼‚å¸¸æ ·æœ¬è¿‡å°‘({final_anomaly_count})ï¼Œå¼ºåˆ¶å¢åŠ åˆ°{min_anomaly_samples}ä¸ª")
        
        # è®¡ç®—æ¯ä¸ªçª—å£çš„å¼‚å¸¸å€¾å‘åˆ†æ•°
        window_anomaly_scores = []
        for i, labels in enumerate(window_anomaly_labels):
            # ç»¼åˆè¯„åˆ†ï¼šå¼‚å¸¸ç‚¹æ•°é‡ + å¼‚å¸¸å¯†åº¦ + ä½ç½®æƒé‡ + æ•°æ®å˜å¼‚æ€§
            anomaly_count = np.sum(labels)
            anomaly_density = np.mean(labels) if len(labels) > 0 else 0
            
            # è®¡ç®—è¿ç»­å¼‚å¸¸æ®µ
            consecutive_scores = []
            current_consecutive = 0
            for label in labels:
                if label == 1:
                    current_consecutive += 1
                else:
                    if current_consecutive > 0:
                        consecutive_scores.append(current_consecutive)
                    current_consecutive = 0
            if current_consecutive > 0:
                consecutive_scores.append(current_consecutive)
            
            max_consecutive = max(consecutive_scores) if consecutive_scores else 0
            
            # è®¡ç®—çª—å£å†…æ•°æ®çš„å˜å¼‚æ€§ï¼ˆæ ‡å‡†å·®ï¼‰
            window_data = windows_scaled[i] if i < len(windows_scaled) else np.zeros(window_size)
            data_variability = np.std(window_data) if len(window_data) > 0 else 0
            
            # ç»¼åˆåˆ†æ•°ï¼ˆå¢åŠ æ•°æ®å˜å¼‚æ€§æƒé‡ï¼‰
            score = (anomaly_count * 0.3 + 
                    anomaly_density * len(labels) * 0.25 + 
                    max_consecutive * 0.25 +
                    data_variability * 0.2)  # æ–°å¢ï¼šæ•°æ®å˜å¼‚æ€§
            window_anomaly_scores.append(score)
        
        window_anomaly_scores = np.array(window_anomaly_scores)
        
        # å¦‚æœæ‰€æœ‰åˆ†æ•°éƒ½ä¸º0ï¼Œä½¿ç”¨éšæœºæ–¹æ³•
        if np.all(window_anomaly_scores == 0):
            print("âš ï¸ æ‰€æœ‰çª—å£åˆ†æ•°ä¸º0ï¼Œä½¿ç”¨éšæœºé€‰æ‹©æ–¹æ³•...")
            top_anomaly_indices = np.random.choice(len(y_final), size=min_anomaly_samples, replace=False)
        else:
            # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„çª—å£ä½œä¸ºå¼‚å¸¸
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„é€‰æ‹©ç­–ç•¥ï¼šåœ¨é«˜åˆ†æ•°çª—å£ä¸­éšæœºé€‰æ‹©ï¼Œé¿å…è¿‡äºé›†ä¸­
            score_percentile_90 = np.percentile(window_anomaly_scores, 90)
            high_score_indices = np.where(window_anomaly_scores >= score_percentile_90)[0]
            
            if len(high_score_indices) >= min_anomaly_samples:
                # ä»é«˜åˆ†æ•°çª—å£ä¸­éšæœºé€‰æ‹©
                top_anomaly_indices = np.random.choice(high_score_indices, size=min_anomaly_samples, replace=False)
            else:
                # å¦‚æœé«˜åˆ†æ•°çª—å£ä¸å¤Ÿï¼Œè¡¥å……æ¬¡é«˜åˆ†æ•°çš„çª—å£
                remaining_needed = min_anomaly_samples - len(high_score_indices)
                sorted_indices = np.argsort(window_anomaly_scores)[::-1]  # ä»é«˜åˆ°ä½æ’åº
                top_anomaly_indices = sorted_indices[:min_anomaly_samples]
        
        # æ›´æ–°æ ‡ç­¾
        y_final = np.zeros(len(y_final))  # é‡ç½®ä¸ºå…¨éƒ¨æ­£å¸¸
        y_final[top_anomaly_indices] = 1  # è®¾ç½®é€‰ä¸­çª—å£ä¸ºå¼‚å¸¸
        
        final_normal_count = np.sum(y_final == 0)
        final_anomaly_count = np.sum(y_final == 1)
        final_anomaly_rate = final_anomaly_count / len(y_final)
        
        print(f"ğŸ“Š å¼ºåˆ¶è°ƒæ•´å: Normal={final_normal_count}, Anomaly={final_anomaly_count}")
        print(f"ğŸ“ˆ è°ƒæ•´åå¼‚å¸¸ç‡: {final_anomaly_rate:.2%}")
    
    # éªŒè¯å¼‚å¸¸æ ·æœ¬æ˜¯å¦çœŸçš„ç”Ÿæˆäº†
    if final_anomaly_count == 0:
        print("âŒ ä¸¥é‡è­¦å‘Šï¼šä»ç„¶æ²¡æœ‰å¼‚å¸¸æ ·æœ¬ï¼å¼ºåˆ¶åˆ›å»ºæœ€å°‘æ•°é‡çš„å¼‚å¸¸æ ·æœ¬...")
        # æœ€åçš„ä¿é™©æªæ–½ï¼šéšæœºé€‰æ‹©ä¸€äº›çª—å£ä½œä¸ºå¼‚å¸¸
        forced_anomaly_count = max(5, len(y_final) // 100)  # è‡³å°‘5ä¸ªæˆ–1%
        forced_anomaly_indices = np.random.choice(len(y_final), size=forced_anomaly_count, replace=False)
        y_final[forced_anomaly_indices] = 1
        
        final_normal_count = np.sum(y_final == 0)
        final_anomaly_count = np.sum(y_final == 1)
        final_anomaly_rate = final_anomaly_count / len(y_final)
        
        print(f"ğŸ“Š å¼ºåˆ¶ä¿é™©è°ƒæ•´å: Normal={final_normal_count}, Anomaly={final_anomaly_count}")
        print(f"ğŸ“ˆ æœ€ç»ˆå¼‚å¸¸ç‡: {final_anomaly_rate:.2%}")
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ ·æœ¬
    if final_anomaly_count < 5:
        print("âš ï¸ è­¦å‘Šï¼šå¼‚å¸¸æ ·æœ¬æ•°é‡ä»ç„¶è¿‡å°‘ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ")
    
    # åˆ›å»ºæ›´ç°å®çš„æœªæ ‡è®°æ ·æœ¬åˆ†å¸ƒï¼Œç¡®ä¿åˆ†å±‚é‡‡æ ·
    if final_anomaly_count > 0:
        normal_indices = np.where(y_final == 0)[0]
    anomaly_indices = np.where(y_final == 1)[0]
    
    print(f"ğŸ” æœ€ç»ˆç±»åˆ«åˆ†å¸ƒ: æ­£å¸¸çª—å£={len(normal_indices)}, å¼‚å¸¸çª—å£={len(anomaly_indices)}")
    
    if len(anomaly_indices) > 0 and len(normal_indices) > 0:
        # ç¡®ä¿æ¯ä¸ªç±»åˆ«éƒ½æœ‰è¶³å¤Ÿçš„å·²æ ‡æ³¨æ ·æœ¬
        min_labeled_per_class = 3  # é™ä½è¦æ±‚åˆ°3ä¸ª
        max_labeled_ratio = 0.8  # æé«˜åˆ°80%çš„æ ·æœ¬å¯è¢«æ ‡æ³¨
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ‡æ³¨æ ·æœ¬æ•°
        normal_labeled_count = min(
            len(normal_indices),  # ä¸å¼ºåˆ¶ä¿ç•™æœªæ ‡æ³¨æ ·æœ¬
            max(min_labeled_per_class, int(len(normal_indices) * max_labeled_ratio))
        )
        anomaly_labeled_count = min(
            len(anomaly_indices),  # ä¸å¼ºåˆ¶ä¿ç•™æœªæ ‡æ³¨æ ·æœ¬
            max(min_labeled_per_class, int(len(anomaly_indices) * max_labeled_ratio))
        )
        
        # ç¡®ä¿æ ‡æ³¨æ•°é‡ä¸è¶…è¿‡å¯ç”¨æ•°é‡
        normal_labeled_count = min(normal_labeled_count, len(normal_indices))
        anomaly_labeled_count = min(anomaly_labeled_count, len(anomaly_indices))
        
        # éšæœºé€‰æ‹©æ ‡æ³¨æ ·æœ¬
        if normal_labeled_count > 0 and anomaly_labeled_count > 0:
            labeled_normal = np.random.choice(normal_indices, size=normal_labeled_count, replace=False)
            labeled_anomaly = np.random.choice(anomaly_indices, size=anomaly_labeled_count, replace=False)
            labeled_indices = np.concatenate([labeled_normal, labeled_anomaly])
            print(f"ğŸ“Š åˆ†å±‚æ ‡æ³¨: æ­£å¸¸={normal_labeled_count}, å¼‚å¸¸={anomaly_labeled_count}")
        else:
            print("âš ï¸ æŸç±»åˆ«æ ·æœ¬ä¸è¶³ï¼Œå›é€€åˆ°ç®€å•éšæœºé€‰æ‹©")
            labeled_count = int(len(y_final) * (1 - unlabeled_fraction))
            labeled_indices = np.random.choice(len(y_final), size=labeled_count, replace=False)
    else:
        print("âš ï¸ ç¼ºå°‘æŸä¸ªç±»åˆ«ï¼Œä½¿ç”¨ç®€å•éšæœºé€‰æ‹©")
        labeled_count = int(len(y_final) * (1 - unlabeled_fraction))
        labeled_indices = np.random.choice(len(y_final), size=labeled_count, replace=False)
    
    # åˆ›å»ºæœ€ç»ˆæ ‡ç­¾æ•°ç»„
    y_with_unlabeled = np.full(len(y_final), -1)  # -1è¡¨ç¤ºæœªæ ‡è®°
    y_with_unlabeled[labeled_indices] = y_final[labeled_indices]
    
    # ç»Ÿè®¡æœ€ç»ˆç»“æœ
    unlabeled_count = np.sum(y_with_unlabeled == -1)
    labeled_normal_count = np.sum(y_with_unlabeled == 0)
    labeled_anomaly_count = np.sum(y_with_unlabeled == 1)
    
    print(f"ğŸ“Š æœ€ç»ˆæ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={labeled_normal_count}, å¼‚å¸¸={labeled_anomaly_count}, æœªæ ‡æ³¨={unlabeled_count}")
    
    # éªŒè¯æ•°æ®è´¨é‡
    if labeled_normal_count == 0 or labeled_anomaly_count == 0:
        print("âŒ ä¸¥é‡è­¦å‘Šï¼šç¼ºå°‘æŸç§ç±»åˆ«çš„æ ‡æ³¨æ ·æœ¬ï¼")
        # å¼ºåˆ¶ç¡®ä¿ä¸¤ç§ç±»åˆ«éƒ½æœ‰
        if labeled_anomaly_count == 0 and len(anomaly_indices) > 0:
            # å¼ºåˆ¶æ ‡æ³¨è‡³å°‘ä¸€ä¸ªå¼‚å¸¸æ ·æœ¬
            forced_anomaly_idx = np.random.choice(anomaly_indices, size=1)[0]
            y_with_unlabeled[forced_anomaly_idx] = 1
            print(f"ğŸ”§ å¼ºåˆ¶æ ‡æ³¨å¼‚å¸¸æ ·æœ¬: çª—å£ #{forced_anomaly_idx}")
        
        if labeled_normal_count == 0 and len(normal_indices) > 0:
            # å¼ºåˆ¶æ ‡æ³¨è‡³å°‘ä¸€ä¸ªæ­£å¸¸æ ·æœ¬
            forced_normal_idx = np.random.choice(normal_indices, size=1)[0]
            y_with_unlabeled[forced_normal_idx] = 0
            print(f"ğŸ”§ å¼ºåˆ¶æ ‡æ³¨æ­£å¸¸æ ·æœ¬: çª—å£ #{forced_normal_idx}")
        
        # é‡æ–°ç»Ÿè®¡
        unlabeled_count = np.sum(y_with_unlabeled == -1)
        labeled_normal_count = np.sum(y_with_unlabeled == 0)
        labeled_anomaly_count = np.sum(y_with_unlabeled == 1)
        
        print(f"ğŸ“Š å¼ºåˆ¶è°ƒæ•´åæ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={labeled_normal_count}, å¼‚å¸¸={labeled_anomaly_count}, æœªæ ‡æ³¨={unlabeled_count}")
    
    # éªŒè¯æ•°æ®è´¨é‡
    # éªŒè¯æ•°æ®è´¨é‡
    if labeled_normal_count == 0 or labeled_anomaly_count == 0:
        print("âš ï¸ è­¦å‘Šï¼šç¼ºå°‘æŸç§ç±»åˆ«çš„æ ‡æ³¨æ ·æœ¬ï¼Œè¿™å¯èƒ½å¯¼è‡´è®­ç»ƒé—®é¢˜")
    
    # å°†å¤„ç†åçš„æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„
    X = np.array(windows_scaled)
    y = y_with_unlabeled
    raw_windows = np.array(windows_raw)
    
    # å¦‚æœæ•°æ®æ˜¯1Dï¼Œè½¬æ¢ä¸º2D (æ·»åŠ ç‰¹å¾ç»´åº¦)
    if X.ndim == 2:
        X = X.reshape(X.shape[0], X.shape[1], 1)
    if raw_windows.ndim == 2:
        raw_windows = raw_windows.reshape(raw_windows.shape[0], raw_windows.shape[1], 1)
    
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ: X.shape={X.shape}, y.shape={y.shape}")
    
    # è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†
    return train_test_split_with_indices(X, y, raw_windows, np.array(window_indices), test_size=0.3, val_size=0.15)
# æ›¿æ¢train_test_split_with_indiceså‡½æ•°ï¼Œç¡®ä¿æ¯ä¸ªæ•°æ®é›†éƒ½æœ‰ä¸¤ç§ç±»åˆ«ï¼š

def train_test_split_with_indices(X, y, raw_windows, window_indices, test_size=0.2, val_size=0.1):
    """å¸¦ç´¢å¼•çš„æ•°æ®é›†åˆ’åˆ†å‡½æ•° - ä¿®å¤ç‰ˆæœ¬ï¼Œç¡®ä¿ç±»åˆ«å¹³è¡¡"""
    n_samples = len(X)
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    labeled_mask = (y != -1)
    labeled_indices = np.where(labeled_mask)[0]
    unlabeled_indices = np.where(~labeled_mask)[0]
    
    if len(labeled_indices) == 0:
        print("âš ï¸ æ²¡æœ‰å·²æ ‡æ³¨æ ·æœ¬ï¼Œä½¿ç”¨éšæœºåˆ’åˆ†")
        # å¦‚æœæ²¡æœ‰æ ‡æ³¨æ ·æœ¬ï¼Œä½¿ç”¨åŸæ¥çš„éšæœºåˆ’åˆ†
        n_test = int(n_samples * test_size)
        n_val = int(n_samples * val_size)
        n_train = n_samples - n_test - n_val
        
        indices = np.random.permutation(n_samples)
        train_indices = indices[:n_train]
        val_indices = indices[n_train:n_train + n_val]
        test_indices = indices[n_train + n_val:]
    else:
        # åˆ†æå·²æ ‡æ³¨æ ·æœ¬çš„ç±»åˆ«åˆ†å¸ƒ
        labeled_y = y[labeled_indices]
        normal_labeled_indices = labeled_indices[labeled_y == 0]
        anomaly_labeled_indices = labeled_indices[labeled_y == 1]
        
        print(f"ğŸ” å·²æ ‡æ³¨æ ·æœ¬åˆ†å¸ƒ: æ­£å¸¸={len(normal_labeled_indices)}, å¼‚å¸¸={len(anomaly_labeled_indices)}")
        
        # ç¡®ä¿æ¯ä¸ªæ•°æ®é›†éƒ½æœ‰è¶³å¤Ÿçš„æ ·æœ¬å’Œç±»åˆ«å¤šæ ·æ€§
        min_samples_per_set = 10
        min_samples_per_class = 3
        
        if len(normal_labeled_indices) >= min_samples_per_class and len(anomaly_labeled_indices) >= min_samples_per_class:
            # å¦‚æœä¸¤ä¸ªç±»åˆ«éƒ½æœ‰è¶³å¤Ÿæ ·æœ¬ï¼Œè¿›è¡Œåˆ†å±‚åˆ’åˆ†
            print("âœ… è¿›è¡Œåˆ†å±‚åˆ’åˆ†ï¼Œç¡®ä¿æ¯ä¸ªæ•°æ®é›†éƒ½æœ‰ä¸¤ç§ç±»åˆ«")
            
            # è®¡ç®—æ¯ä¸ªæ•°æ®é›†éœ€è¦çš„æ ·æœ¬æ•°
            total_labeled = len(labeled_indices)
            n_test_labeled = max(min_samples_per_set, int(total_labeled * test_size))
            n_val_labeled = max(min_samples_per_set, int(total_labeled * val_size))
            n_train_labeled = total_labeled - n_test_labeled - n_val_labeled
            
            # ç¡®ä¿è®­ç»ƒé›†æœ‰è¶³å¤Ÿæ ·æœ¬
            if n_train_labeled < min_samples_per_set:
                n_test_labeled = min(n_test_labeled, total_labeled // 3)
                n_val_labeled = min(n_val_labeled, total_labeled // 4)
                n_train_labeled = total_labeled - n_test_labeled - n_val_labeled
            
            # åˆ†å±‚é‡‡æ ·ï¼šç¡®ä¿æ¯ä¸ªæ•°æ®é›†éƒ½æœ‰ä¸¤ç§ç±»åˆ«
            def stratified_split(normal_indices, anomaly_indices, n_samples):
                """åˆ†å±‚é‡‡æ ·å‡½æ•°"""
                # æŒ‰æ¯”ä¾‹åˆ†é…æ­£å¸¸å’Œå¼‚å¸¸æ ·æœ¬
                total_normal = len(normal_indices)
                total_anomaly = len(anomaly_indices)
                total_samples = total_normal + total_anomaly
                
                if total_samples == 0:
                    return np.array([])
                
                # è®¡ç®—æ¯ä¸ªç±»åˆ«åº”è¯¥åˆ†é…çš„æ ·æœ¬æ•°
                normal_ratio = total_normal / total_samples
                anomaly_ratio = total_anomaly / total_samples
                
                n_normal = max(1, int(n_samples * normal_ratio))
                n_anomaly = max(1, int(n_samples * anomaly_ratio))
                
                # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨æ ·æœ¬æ•°
                n_normal = min(n_normal, total_normal)
                n_anomaly = min(n_anomaly, total_anomaly)
                
                # éšæœºé€‰æ‹©æ ·æœ¬
                selected_normal = np.random.choice(normal_indices, size=n_normal, replace=False) if n_normal > 0 else []
                selected_anomaly = np.random.choice(anomaly_indices, size=n_anomaly, replace=False) if n_anomaly > 0 else []
                
                return np.concatenate([selected_normal, selected_anomaly])
            
            # åˆ†å±‚åˆ’åˆ†æµ‹è¯•é›†
            test_labeled_indices = stratified_split(normal_labeled_indices, anomaly_labeled_indices, n_test_labeled)
            
            # ä»å‰©ä½™æ ·æœ¬ä¸­åˆ’åˆ†éªŒè¯é›†
            remaining_normal = np.setdiff1d(normal_labeled_indices, test_labeled_indices)
            remaining_anomaly = np.setdiff1d(anomaly_labeled_indices, test_labeled_indices)
            val_labeled_indices = stratified_split(remaining_normal, remaining_anomaly, n_val_labeled)
            
            # å‰©ä½™æ ·æœ¬ä½œä¸ºè®­ç»ƒé›†
            train_labeled_indices = np.setdiff1d(labeled_indices, np.concatenate([test_labeled_indices, val_labeled_indices]))
            
            # æ·»åŠ æœªæ ‡æ³¨æ ·æœ¬åˆ°è®­ç»ƒé›†
            n_unlabeled_train = len(unlabeled_indices)
            train_unlabeled_indices = unlabeled_indices
            
            # åˆå¹¶ç´¢å¼•
            train_indices = np.concatenate([train_labeled_indices, train_unlabeled_indices])
            val_indices = val_labeled_indices
            test_indices = test_labeled_indices
            
            print(f"ğŸ“Š åˆ†å±‚åˆ’åˆ†ç»“æœ:")
            print(f"   è®­ç»ƒé›†: {len(train_indices)} (å·²æ ‡æ³¨: {len(train_labeled_indices)}, æœªæ ‡æ³¨: {len(train_unlabeled_indices)})")
            print(f"   éªŒè¯é›†: {len(val_indices)} (å·²æ ‡æ³¨: {len(val_labeled_indices)})")
            print(f"   æµ‹è¯•é›†: {len(test_indices)} (å·²æ ‡æ³¨: {len(test_labeled_indices)})")
            
            # æ£€æŸ¥æ¯ä¸ªæ•°æ®é›†çš„ç±»åˆ«åˆ†å¸ƒ
            for name, indices in [("è®­ç»ƒ", train_indices), ("éªŒè¯", val_indices), ("æµ‹è¯•", test_indices)]:
                subset_y = y[indices]
                labeled_subset = subset_y[subset_y != -1]
                if len(labeled_subset) > 0:
                    normal_count = np.sum(labeled_subset == 0)
                    anomaly_count = np.sum(labeled_subset == 1)
                    print(f"   {name}é›†æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={normal_count}, å¼‚å¸¸={anomaly_count}")
        else:
            print("âš ï¸ æŸä¸ªç±»åˆ«æ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨éšæœºåˆ’åˆ†")
            # å¦‚æœæŸä¸ªç±»åˆ«æ ·æœ¬ä¸è¶³ï¼Œå›é€€åˆ°éšæœºåˆ’åˆ†
            indices = np.random.permutation(n_samples)
            n_test = int(n_samples * test_size)
            n_val = int(n_samples * val_size)
            n_train = n_samples - n_test - n_val
            
            train_indices = indices[:n_train]
            val_indices = indices[n_train:n_train + n_val]
            test_indices = indices[n_train + n_val:]
    
    # åˆ’åˆ†æ•°æ®
    X_train, y_train, raw_train = X[train_indices], y[train_indices], raw_windows[train_indices]
    X_val, y_val, raw_val = X[val_indices], y[val_indices], raw_windows[val_indices]
    X_test, y_test, raw_test = X[test_indices], y[test_indices], raw_windows[test_indices]
    
    train_window_indices = window_indices[train_indices]
    val_window_indices = window_indices[val_indices]
    test_window_indices = window_indices[test_indices]
    
    print(f"âœ… æ•°æ®åˆ’åˆ†å®Œæˆ: è®­ç»ƒ={X_train.shape}, éªŒè¯={X_val.shape}, æµ‹è¯•={X_test.shape}")
    
    return (X_train, y_train, raw_train, train_window_indices,
            X_val, y_val, raw_val, val_window_indices,
            X_test, y_test, raw_test, test_window_indices)

# =================================
# æ¨¡å‹ã€ç»éªŒå›æ”¾ä¸å¥–åŠ±å‡½æ•° (æ¥è‡ª v3.0 é€»è¾‘)
# =================================

Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

# æ›¿æ¢EnhancedRLADAgentç±»ï¼š
class EnhancedRLADAgent(nn.Module):
    def __init__(self, input_dim=1, seq_len=288, hidden_size=64, num_heads=2, 
                 dropout=0.2, bidirectional=True, include_pos=True, 
                 num_actions=2, use_lstm=True, use_attention=True, num_layers=1):
        """å¢å¼ºå‹RLAD Agentï¼Œç»“åˆCNNã€LSTMå’Œæ³¨æ„åŠ›æœºåˆ¶"""
        super(EnhancedRLADAgent, self).__init__()
        
        self.input_dim = input_dim
        self.seq_len = seq_len
        self.hidden_size = hidden_size
        self.use_lstm = use_lstm
        self.use_attention = use_attention
        self.num_actions = num_actions
        self.num_layers = num_layers  # ä¿å­˜å‚æ•°
        
        # 1. ç‰¹å¾æå–å™¨ - ä½¿ç”¨è¾ƒå°çš„å·ç§¯æ ¸æå–å±€éƒ¨ç‰¹å¾
        self.feature_extractor = nn.Sequential(
            nn.Conv1d(input_dim, 32, kernel_size=7, padding=3),
            nn.BatchNorm1d(32),
            nn.LeakyReLU(0.1),
            nn.Dropout(dropout),
            nn.Conv1d(32, 64, kernel_size=5, padding=2),
            nn.BatchNorm1d(64),
            nn.LeakyReLU(0.1),
            nn.Conv1d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64),
            nn.LeakyReLU(0.1),
            nn.AdaptiveAvgPool1d(seq_len // 4),
            nn.Dropout(dropout)
        )
        
        # æ·»åŠ å±‚å½’ä¸€åŒ–å±‚ - è§£å†³é”™è¯¯1
        self.pre_lstm_norm = nn.LayerNorm(64)
        
        # 2. åºåˆ—å»ºæ¨¡ - ä½¿ç”¨LSTMæ•è·æ—¶åºä¾èµ–
        if use_lstm:
            self.lstm = nn.LSTM(64, hidden_size // 2, self.num_layers,  # ä½¿ç”¨self.num_layers
                              batch_first=True, bidirectional=bidirectional)
                             
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, dropout=0.1, batch_first=True)

        
        self.ln_attention = nn.LayerNorm(hidden_size)
        
        # æ·»åŠ è‡ªæ³¨æ„åŠ›æ®‹å·®å—
        self.feed_forward = nn.Sequential(
            nn.Linear(hidden_size, hidden_size*2),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.1),
            nn.Linear(hidden_size*2, hidden_size)
        )
        
        # æ·»åŠ åˆ†ç±»å™¨å±‚ - è§£å†³é”™è¯¯2
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.LeakyReLU(0.1),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, num_actions)
        )
        
        # æ”¹è¿›çš„åˆå§‹åŒ–ç­–ç•¥
        self._initialize_weights()
    
    def _initialize_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ç­–ç•¥ï¼Œæé«˜æ¨¡å‹ç¨³å®šæ€§"""
        for name, module in self.named_modules():
            # å·ç§¯å±‚åˆå§‹åŒ–
            if isinstance(module, nn.Conv1d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            
            # çº¿æ€§å±‚åˆå§‹åŒ–
            elif isinstance(module, nn.Linear):
                # æœ€åä¸€å±‚ä½¿ç”¨æ›´å°çš„æƒé‡åˆå§‹åŒ–ï¼Œå‡å°‘åˆå§‹é¢„æµ‹åå·®
                if 'classifier' in name and name.endswith('.4'):
                    nn.init.normal_(module.weight, mean=0.0, std=0.01)
                    nn.init.constant_(module.bias, 0)
                else:
                    nn.init.kaiming_uniform_(module.weight, a=0.01, nonlinearity='relu')
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0)
            
            # æ‰¹å½’ä¸€åŒ–å±‚åˆå§‹åŒ–
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
            
            # å±‚å½’ä¸€åŒ–åˆå§‹åŒ–
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.weight, 1.0)
                nn.init.constant_(module.bias, 0)
                
            # LSTMç‰¹æ®Šåˆå§‹åŒ–
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.orthogonal_(param.data, gain=0.7)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data, gain=1.0)
                    elif 'bias' in name:
                        nn.init.constant_(param.data, 0.0)
                        # é—å¿˜é—¨åç½®è®¾ä¸ºå°æ­£æ•°ï¼Œå¸®åŠ©é•¿æœŸè®°å¿†
                        param.data[module.hidden_size:(2 * module.hidden_size)] = 1.0
    
    
    def forward(self, x, return_features=False, return_attention_weights=False):
        batch_size, seq_len, features = x.shape
        
        # å·ç§¯ç‰¹å¾æå–
        x_conv = x.transpose(1, 2)
        x_conv = self.feature_extractor(x_conv)
        x_conv = x_conv.transpose(1, 2)
        x_conv = self.pre_lstm_norm(x_conv)  # æ·»åŠ å±‚å½’ä¸€åŒ–
        
        # LSTMå¤„ç†
        lstm_out, _ = self.lstm(x_conv)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)
        x = self.ln_attention(lstm_out + attn_out)  # æ®‹å·®è¿æ¥
        
        # ä½¿ç”¨å…¨å±€æ± åŒ–è€Œéç®€å•çš„å¹³å‡
        pooled = F.adaptive_max_pool1d(x.transpose(1, 2), 1).squeeze(2) * 0.5 + \
                 torch.mean(x, dim=1) * 0.5  # ç»“åˆæœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–
        
        # åˆ†ç±»
        q_values = self.classifier(pooled)
        
        if return_features and return_attention_weights:
            return q_values, pooled, attn_weights
        if return_features:
            return q_values, pooled
        if return_attention_weights:
            return q_values, attn_weights
        return q_values

    def get_action(self, state, epsilon=0.0):
        if random.random() < epsilon: 
            return random.randint(0, 1)
        
        was_training = self.training
        self.eval()
        
        with torch.no_grad():
            if state.ndim == 2: 
                state = state.unsqueeze(0)
            q_values = self.forward(state)
            action = q_values.argmax(dim=1).item()
        
        if was_training: 
            self.train()
        
        return action

class PrioritizedReplayBuffer:
    def __init__(self, capacity=20000, alpha=0.6):
        self.capacity, self.alpha, self.buffer, self.pos = capacity, alpha, [], 0
        self.priorities = np.zeros((capacity,), dtype=np.float32)
        
    def push(self, state, action, reward, next_state, done):
        max_prio = self.priorities.max() if self.buffer else 1.0
        # ç¡®ä¿çŠ¶æ€å¼ é‡æ˜¯float32ç±»å‹
        if isinstance(state, torch.Tensor):
            state = state.float()
        if isinstance(next_state, torch.Tensor):
            next_state = next_state.float()
            
        exp = Experience(state, action, reward, next_state, done)
        if len(self.buffer) < self.capacity: 
            self.buffer.append(exp)
        else: 
            self.buffer[self.pos] = exp
        self.priorities[self.pos] = max_prio
        self.pos = (self.pos + 1) % self.capacity
        
    def sample(self, batch_size, beta=0.4):
        if not self.buffer: 
            return None
        prios = self.priorities[:len(self.buffer)]
        probs = prios ** self.alpha
        probs /= probs.sum()
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        weights = (len(self.buffer) * probs[indices]) ** (-beta)
        weights /= weights.max()
        exps = [self.buffer[idx] for idx in indices]
        
        # ç¡®ä¿è¿”å›çš„å¼ é‡éƒ½æ˜¯float32ç±»å‹
        states = torch.stack([e.state.float() for e in exps])
        actions = torch.LongTensor([e.action for e in exps])
        rewards = torch.FloatTensor([e.reward for e in exps])
        next_states = torch.stack([e.next_state.float() for e in exps])
        dones = torch.BoolTensor([e.done for e in exps])
        
        return states, actions, rewards, next_states, dones, indices, torch.FloatTensor(weights)
    
    def update_priorities(self, indices, priorities):
        for idx, priority in zip(indices, priorities): 
            self.priorities[idx] = priority
    
    def __len__(self): 
        return len(self.buffer)

def enhanced_compute_reward(action, label, is_human_labeled=False, is_augmented=False):
    """å¢å¼ºç‰ˆå¥–åŠ±è®¡ç®—ï¼Œæ·»åŠ å¯¹é”™è¯¯æ ·æœ¬çš„æƒ©ç½šå’Œæ­£ç¡®æ ·æœ¬çš„å¥–åŠ±"""
    # ç¡®ä¿åŠ¨ä½œå’Œæ ‡ç­¾æ ¼å¼æ­£ç¡®
    action = int(action)
    label = int(label)
    
    # å¼‚å¸¸æ ·æœ¬æ›´é«˜çš„å¥–åŠ±å’Œæƒ©ç½š
    if label == 1:  # å¼‚å¸¸æ ·æœ¬
        if action == label:  # æ­£ç¡®é¢„æµ‹å¼‚å¸¸
            base_reward = 1.5  # æé«˜å¯¹å¼‚å¸¸æ ·æœ¬çš„å¥–åŠ±
        else:  # é”™è¯¯åœ°é¢„æµ‹ä¸ºæ­£å¸¸(æ¼æŠ¥)
            base_reward = -2.0  # å¢åŠ æ¼æŠ¥æƒ©ç½š
    else:  # æ­£å¸¸æ ·æœ¬
        if action == label:  # æ­£ç¡®é¢„æµ‹æ­£å¸¸
            base_reward = 1.0
        else:  # é”™è¯¯åœ°é¢„æµ‹ä¸ºå¼‚å¸¸(è¯¯æŠ¥)
            base_reward = -1.5
    
    # äººå·¥æ ‡æ³¨çš„æ ·æœ¬é¢å¤–å¥–åŠ±
    if is_human_labeled:
        base_reward *= 1.3
        
    # å¢å¼ºçš„æ ·æœ¬ç•¥å¾®é™ä½å¥–åŠ±
    if is_augmented:
        base_reward *= 0.9
        
    return base_reward

# =================================
# è®­ç»ƒä¸è¯„ä¼° (é›†æˆ v2.4 å’Œ v3.0)
# =================================
# åœ¨è®­ç»ƒå¼€å§‹å‰æ·»åŠ ä¸“é—¨çš„é¢„çƒ­é˜¶æ®µ

def warm_up_training(agent, target_agent, replay_buffer, optimizer, X_train, y_train, device, scaler=None):
    """å¢å¼ºçš„é¢„çƒ­è®­ç»ƒï¼Œä¸“é—¨é’ˆå¯¹é«˜æŸå¤±é—®é¢˜"""
    print("ğŸ”¥ æ‰§è¡Œä¸“é—¨çš„é¢„çƒ­è®­ç»ƒï¼Œç¨³å®šåˆå§‹æŸå¤±...")
    
    # æ‰¾å‡ºå·²æ ‡æ³¨æ ·æœ¬
    labeled_mask = (y_train != -1)
    labeled_indices = np.where(labeled_mask)[0]
    
    if len(labeled_indices) < 10:
        print("âš ï¸ å·²æ ‡æ³¨æ ·æœ¬å¤ªå°‘ï¼Œè·³è¿‡é¢„çƒ­")
        return
    
    # å°†æ•°æ®è½¬æ¢ä¸ºtensor
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.LongTensor(y_train)
    
    # åˆ†ç¦»å‡ºæ­£å¸¸å’Œå¼‚å¸¸æ ·æœ¬
    normal_indices = [idx for idx in labeled_indices if y_train[idx] == 0]
    anomaly_indices = [idx for idx in labeled_indices if y_train[idx] == 1]
    
    # ç¡®ä¿ä¸¤ç±»éƒ½æœ‰æ ·æœ¬
    if len(normal_indices) < 5 or len(anomaly_indices) < 5:
        print("âš ï¸ æŸç±»æ ·æœ¬ä¸è¶³5ä¸ªï¼Œè·³è¿‡é¢„çƒ­")
        return
    
    # å¤šé˜¶æ®µé¢„çƒ­
    warm_up_phases = [
        {"normal_weight": 1.0, "anomaly_weight": 1.0, "lr_factor": 0.2, "steps": 20},
        {"normal_weight": 1.0, "anomaly_weight": 2.0, "lr_factor": 0.5, "steps": 20},
        {"normal_weight": 1.0, "anomaly_weight": 3.0, "lr_factor": 1.0, "steps": 10}
    ]
    
    original_lr = optimizer.param_groups[0]['lr']
    
    for phase_idx, phase in enumerate(warm_up_phases):
        print(f"ğŸ”„ é¢„çƒ­é˜¶æ®µ {phase_idx+1}/{len(warm_up_phases)}")
        
        # è®¾ç½®è¯¥é˜¶æ®µçš„å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            param_group['lr'] = original_lr * phase['lr_factor']
        
        # æ‰§è¡Œè¯¥é˜¶æ®µçš„è®­ç»ƒ
        phase_losses = []
        for step in range(phase['steps']):
            # å¹³è¡¡é‡‡æ ·
            normal_sample_size = min(8, len(normal_indices))
            anomaly_sample_size = min(8, len(anomaly_indices))
            
            selected_normal = np.random.choice(normal_indices, size=normal_sample_size, replace=False)
            selected_anomaly = np.random.choice(anomaly_indices, size=anomaly_sample_size, replace=False)
            batch_indices = np.concatenate([selected_normal, selected_anomaly])
            
            # æ·»åŠ åˆ°å›æ”¾ç¼“å†²åŒº
            states = X_train_tensor[batch_indices].to(device)
            labels = y_train_tensor[batch_indices]
            
            for i, idx in enumerate(batch_indices):
                label = labels[i].item()
                
                # è·å–åŠ¨ä½œ
                with torch.no_grad():
                    q_values = agent(states[i:i+1])
                    action = q_values.argmax(dim=1).item()
                
                # è®¾ç½®å¥–åŠ±
                if label == 0:  # æ­£å¸¸æ ·æœ¬
                    reward = phase['normal_weight'] if action == label else -phase['normal_weight']
                else:  # å¼‚å¸¸æ ·æœ¬
                    reward = phase['anomaly_weight'] if action == label else -phase['anomaly_weight']
                
                replay_buffer.push(states[i].cpu(), action, reward, states[i].cpu(), True)
            
            # æ‰§è¡Œè®­ç»ƒ
            loss = enhanced_train_dqn_step(agent, target_agent, replay_buffer, optimizer, device, 
                                          batch_size=min(16, len(batch_indices)), scaler=scaler)
            
            if loss is not None:
                phase_losses.append(loss)
                
                if step % 5 == 0:  # æ¯5æ­¥è¾“å‡ºä¸€æ¬¡
                    print(f"    é¢„çƒ­æ­¥éª¤ {step+1}/{phase['steps']}: Loss={loss:.4f}")
        
        # æ¯é˜¶æ®µç»“æŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        target_agent.load_state_dict(agent.state_dict())
        avg_loss = sum(phase_losses) / len(phase_losses) if phase_losses else 0
        print(f"ğŸ“Š é˜¶æ®µ{phase_idx+1}å¹³å‡æŸå¤±: {avg_loss:.4f}")
    
    # æ¢å¤åŸå§‹å­¦ä¹ ç‡
    for param_group in optimizer.param_groups:
        param_group['lr'] = original_lr
    
    print("âœ… é¢„çƒ­è®­ç»ƒå®Œæˆ")
def enhanced_warmup(replay_buffer, X_train, y_train, agent, device):
    """ä½¿ç”¨å·²æ ‡æ³¨æ•°æ®é¢„çƒ­ç»éªŒå›æ”¾æ±  (æ¥è‡ª v2.4)"""
    print("ğŸ”¥ Pre-warming replay buffer...")
    labeled_mask = (y_train != -1)
    X_labeled, y_labeled = X_train[labeled_mask], y_train[labeled_mask]
    if len(X_labeled) == 0: print("No labeled data for warm-up."); return
    
    for idx in np.random.permutation(len(X_labeled))[:500]: # Warm up with up to 500 samples
        state = torch.FloatTensor(X_labeled[idx]).to(device)
        true_label = y_labeled[idx]
        action = agent.get_action(state)
        reward = enhanced_compute_reward(action, true_label, is_human_labeled=False)
        next_state = state # Simplified for warm-up
        replay_buffer.push(state.cpu(), action, reward, next_state.cpu(), True)
    print(f"ğŸ”¥ Warm-up complete. Buffer size: {len(replay_buffer)}")

def enhanced_evaluate_model(agent, data_loader, device, threshold=0.5):
    """ä¼˜åŒ–çš„è¯„ä¼°å‡½æ•° - æ·»åŠ é˜ˆå€¼å‚æ•°"""
    agent.eval()
    all_preds, all_labels, all_probs, all_features = [], [], [], []
    
    with torch.no_grad():
        for data, labels, _ in data_loader:
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            data = data.to(device, dtype=torch.float32)
            
            # æ·»åŠ å¤šæ¬¡å‰å‘ä¼ æ’­è·å–æ›´é²æ£’çš„é¢„æµ‹
            n_forward = 3
            all_q_values = []
            all_features_batch = []
            
            for i in range(n_forward):
                # è½»å¾®æ‰°åŠ¨ä»¥è·å¾—æ›´é²æ£’çš„ç»“æœ
                if i > 0:  # ç¬¬ä¸€æ¬¡ä¸æ·»åŠ å™ªå£°
                    noise = torch.randn_like(data) * 0.005
                    data_perturbed = data + noise
                else:
                    data_perturbed = data
                
                q_values, features = agent(data_perturbed, return_features=True)
                all_q_values.append(q_values)
                all_features_batch.append(features)
            
            # å¹³å‡å¤šæ¬¡å‰å‘ä¼ æ’­ç»“æœ
            q_values = torch.mean(torch.stack(all_q_values), dim=0)
            features = torch.mean(torch.stack(all_features_batch), dim=0)
            
            # æ¸©åº¦ç¼©æ”¾æ ¡å‡†æ¦‚ç‡
            temperature = 2.0  # æ›´é«˜çš„æ¸©åº¦ä¼šä½¿æ¦‚ç‡åˆ†å¸ƒæ›´å¹³æ»‘
            calibrated_q_values = q_values / temperature
            probs = F.softmax(calibrated_q_values, dim=1)
            
            # ä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼è¿›è¡Œé¢„æµ‹
            predicted = (probs[:, 1] >= threshold).long()
            
            # æ”¶é›†ç»“æœ
            all_preds.extend(predicted.cpu().numpy())  # ä½¿ç”¨é˜ˆå€¼åŒ–çš„é¢„æµ‹
            all_probs.extend(probs[:, 1].cpu().numpy())
            all_features.extend(features.cpu().numpy())
            all_labels.extend(labels.numpy())
    
    agent.train()
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)
    all_features = np.array(all_features)
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    labeled_mask = (all_labels != -1)
    print(f"ğŸ” è¯„ä¼°æ•°æ®ç»Ÿè®¡: æ€»æ ·æœ¬={len(all_labels)}, å·²æ ‡æ³¨={np.sum(labeled_mask)}")
    
    if not np.any(labeled_mask):
        print("âš ï¸ æ²¡æœ‰å·²æ ‡æ³¨çš„æµ‹è¯•æ ·æœ¬")
        return {
            'f1': 0.0, 'precision': 0.0, 'recall': 0.0, 'auc_roc': 0.0,
            'labels': [], 'predictions': [], 'probabilities': [], 'features': [],
            'all_predictions': all_preds, 'all_probabilities': all_probs
        }

    y_true = all_labels[labeled_mask]
    y_pred = all_preds[labeled_mask]
    y_scores = all_probs[labeled_mask]
    features_labeled = all_features[labeled_mask]
    
    # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
    unique_labels = np.unique(y_true)
    print(f"ğŸ” çœŸå®æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(unique_labels, [np.sum(y_true==i) for i in unique_labels]))}")
    print(f"ğŸ” é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(np.unique(y_pred), [np.sum(y_pred==i) for i in np.unique(y_pred)]))}")
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œè¿”å›ä¿®æ­£çš„æŒ‡æ ‡
    if len(unique_labels) < 2:
        print("âš ï¸ æµ‹è¯•é›†åªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œæ— æ³•è®¡ç®—å®Œæ•´æŒ‡æ ‡")
        # å¯¹äºå•ç±»åˆ«æƒ…å†µï¼Œç»™å‡ºä¿å®ˆçš„åˆ†æ•°
        single_class = unique_labels[0]
        accuracy = np.mean(y_pred == single_class)
        return {
            'f1': accuracy * 0.5,  # ä¿å®ˆä¼°è®¡
            'precision': accuracy * 0.5,
            'recall': accuracy * 0.5,
            'auc_roc': 0.5,  # éšæœºåˆ†ç±»å™¨æ°´å¹³
            'labels': y_true, 'predictions': y_pred, 'probabilities': y_scores, 
            'features': features_labeled, 'all_predictions': all_preds, 'all_probabilities': all_probs
        }
    
    # è®¡ç®—æŒ‡æ ‡æ—¶ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ–¹æ³•
    try:
        # ä½¿ç”¨åŠ æƒå¹³å‡è€Œä¸æ˜¯äºŒåˆ†ç±»ï¼Œæ›´é€‚åˆä¸å¹³è¡¡æ•°æ®
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average='weighted', zero_division=0.0
        )
        
        # è®¡ç®—AUC-ROCï¼Œæ·»åŠ å¼‚å¸¸å¤„ç†
        try:
            auc_roc = roc_auc_score(y_true, y_scores)
        except ValueError as e:
            print(f"âš ï¸ AUC-ROCè®¡ç®—å¤±è´¥: {e}")
            auc_roc = 0.5  # é»˜è®¤éšæœºåˆ†ç±»å™¨æ°´å¹³
        
        # æ·»åŠ åˆ†æ•°åˆç†æ€§æ£€æŸ¥
        if f1 > 0.95 and len(y_true) > 20:
            print("âš ï¸ F1åˆ†æ•°å¼‚å¸¸é«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ")
            # å¯¹è¿‡é«˜çš„åˆ†æ•°è¿›è¡Œæƒ©ç½šè°ƒæ•´
            f1 = min(f1, 0.90)
            precision = min(precision, 0.90)
            recall = min(recall, 0.90)
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        f1 = max(0.0, min(1.0, f1))
        precision = max(0.0, min(1.0, precision))
        recall = max(0.0, min(1.0, recall))
        auc_roc = max(0.0, min(1.0, auc_roc))
        
    except Exception as e:
        print(f"âš ï¸ æŒ‡æ ‡è®¡ç®—å‡ºé”™: {e}")
        # è¿”å›ä¿å®ˆçš„åˆ†æ•°
        f1 = precision = recall = auc_roc = 0.3
    
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ: F1={f1:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, AUC={auc_roc:.4f}")
    
    return {
        'precision': precision, 'recall': recall, 'f1': f1, 'auc_roc': auc_roc,
        'labels': y_true, 'predictions': y_pred, 'probabilities': y_scores, 
        'features': features_labeled, 'all_predictions': all_preds, 'all_probabilities': all_probs
    }
# æ·»åŠ åœ¨enhanced_evaluate_modelå‡½æ•°ä¹‹å

def find_optimal_threshold(val_dataset, agent, device):
    """åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼"""
    print("ğŸ” åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼...")
    agent.eval()
    
    # åˆ›å»ºéªŒè¯é›†æ•°æ®åŠ è½½å™¨
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
    
    # æ”¶é›†æ‰€æœ‰éªŒè¯é›†æ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡å’ŒçœŸå®æ ‡ç­¾
    all_probs = []
    all_labels = []
    
    with torch.no_grad():
        for data, labels, _ in val_loader:
            data = data.to(device, dtype=torch.float32)
            
            # æ·»åŠ å¤šæ¬¡å‰å‘ä¼ æ’­è·å–æ›´é²æ£’çš„é¢„æµ‹
            n_forward = 3
            all_q_values = []
            
            for i in range(n_forward):
                # è½»å¾®æ‰°åŠ¨ä»¥è·å¾—æ›´é²æ£’çš„ç»“æœ
                if i > 0:  # ç¬¬ä¸€æ¬¡ä¸æ·»åŠ å™ªå£°
                    noise = torch.randn_like(data) * 0.005
                    data_perturbed = data + noise
                else:
                    data_perturbed = data
                
                q_values = agent(data_perturbed)
                all_q_values.append(q_values)
            
            # å¹³å‡å¤šæ¬¡å‰å‘ä¼ æ’­ç»“æœ
            q_values = torch.mean(torch.stack(all_q_values), dim=0)
            
            # æ¸©åº¦ç¼©æ”¾æ ¡å‡†æ¦‚ç‡
            temperature = 2.0
            calibrated_q_values = q_values / temperature
            probs = F.softmax(calibrated_q_values, dim=1)
            
            # æ”¶é›†æ ‡ç­¾å’Œæ¦‚ç‡
            valid_mask = (labels != -1)
            all_probs.extend(probs[valid_mask, 1].cpu().numpy())
            all_labels.extend(labels[valid_mask].cpu().numpy())
    
    # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„éªŒè¯æ•°æ®
    if len(all_labels) < 10 or len(np.unique(all_labels)) < 2:
        print("âš ï¸ éªŒè¯é›†æ•°æ®ä¸è¶³æˆ–ç±»åˆ«ä¸å¹³è¡¡ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼0.5")
        return 0.5
        
    # å°è¯•ä¸åŒé˜ˆå€¼ï¼Œæ‰¾åˆ°ä½¿F1æœ€å¤§çš„é˜ˆå€¼
    best_f1 = 0
    best_threshold = 0.5
    
    for threshold in np.arange(0.3, 0.8, 0.02):
        predictions = (np.array(all_probs) >= threshold).astype(int)
        f1 = f1_score(all_labels, predictions, zero_division=0)
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    print(f"âœ… æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼: {best_threshold:.2f} (éªŒè¯é›†F1: {best_f1:.4f})")
    return best_threshold

def focal_loss(logits, targets, gamma=2.0, alpha=0.25):
    """
    ç„¦ç‚¹æŸå¤± - ä¸“æ³¨äºéš¾åˆ†ç±»æ ·æœ¬
    å‚æ•°:
    - logits: æ¨¡å‹è¾“å‡ºçš„æœªå½’ä¸€åŒ–åˆ†æ•°
    - targets: ç›®æ ‡ç±»åˆ«
    - gamma: èšç„¦å‚æ•°ï¼Œå¢åŠ è¶Šé«˜å…³æ³¨éš¾åˆ†ç±»æ ·æœ¬è¶Šå¤š
    - alpha: ç±»åˆ«å¹³è¡¡å‚æ•°
    """
    probs = F.softmax(logits, dim=1)
    ce_loss = F.cross_entropy(logits, targets, reduction='none')
    p_t = probs.gather(1, targets.unsqueeze(1)).squeeze(1)
    loss = ce_loss * ((1 - p_t) ** gamma)
    
    if alpha >= 0:
        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
        loss = alpha_t * loss
        
    return loss.mean()
def enhanced_train_dqn_step(agent, target_agent, replay_buffer, optimizer, device, 
                           gamma=0.95, batch_size=64, beta=0.4, scaler=None, grad_clip=1.0, prioritize_recent=False):
    """æ”¹è¿›çš„DQNè®­ç»ƒæ­¥éª¤ - ç¨³å®šç‰ˆæœ¬ï¼Œä¿æŒç°æœ‰æ¶æ„"""
    if len(replay_buffer) < batch_size: 
        return None
    
    agent.train()
    target_agent.eval()
    
    sample = replay_buffer.sample(batch_size, beta)
    if not sample: 
        return None
    
    states, actions, rewards, next_states, dones, indices, weights = sample
    
    # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´ - å¼ºåˆ¶è½¬æ¢ä¸ºfloat32
    states = states.to(device, dtype=torch.float32, non_blocking=True)
    actions = actions.to(device, non_blocking=True)
    rewards = rewards.to(device, dtype=torch.float32, non_blocking=True)
    next_states = next_states.to(device, dtype=torch.float32, non_blocking=True)
    dones = dones.to(device, non_blocking=True)
    weights = weights.to(device, dtype=torch.float32, non_blocking=True)
    
    optimizer.zero_grad()
    
    # è®­ç»ƒç¨³å®šåŒ–æªæ–½
    if scaler is not None:
        with torch.cuda.amp.autocast():
            # é™åˆ¶Qå€¼èŒƒå›´æ›´ä¸¥æ ¼
            q_values = agent(states)
            q_values = torch.clamp(q_values, -5.0, 5.0)  # æ›´ä¸¥æ ¼çš„é™åˆ¶èŒƒå›´
            
            q_current = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
            
            with torch.no_grad():
                # ç›®æ ‡Qå€¼è®¡ç®—ä½¿ç”¨å‡å€¼å¹³æ»‘
                next_q_values = target_agent(next_states)
                next_q_values = torch.clamp(next_q_values, -5.0, 5.0)
                
                # ç¨³å®šç›®æ ‡è®¡ç®—
                q_target = rewards + gamma * torch.mean(next_q_values, dim=1) * (~dones) * 0.8  # æŠ˜æ‰£å› å­é™ä½
                q_target = torch.clamp(q_target, -3.0, 3.0)  # æ›´ä¸¥æ ¼çš„ç›®æ ‡å€¼é™åˆ¶
            
            # ä½¿ç”¨HuberæŸå¤±ï¼Œé™ä½deltaå‚æ•°ä½¿æ›²çº¿æ›´å¹³æ»‘
            delta = 0.5  # é™ä½deltaå€¼ï¼Œä½¿æŸå¤±æ›´å¹³æ»‘
            diff = q_current - q_target
            huber_loss = torch.where(
                torch.abs(diff) < delta,
                0.5 * diff.pow(2),
                delta * (torch.abs(diff) - 0.5 * delta)
            )
            
            # åŠ å…¥æ¢¯åº¦è£å‰ªåˆ°æŸå¤±è®¡ç®—ä¸­
            loss = (weights * huber_loss).mean()
            
            # å¢å¼ºæ­£åˆ™åŒ–ä»¥é¿å…è¿‡æ‹Ÿåˆ
            l2_reg = 0.0005 * sum(p.pow(2.0).sum() for p in agent.parameters())  # å¢åŠ æ­£åˆ™åŒ–å¼ºåº¦
            loss = loss + l2_reg
        
        # æ¢¯åº¦ç¼©æ”¾å’Œè£å‰ª
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=grad_clip)
        scaler.step(optimizer)
        scaler.update()
    else:
        # éæ··åˆç²¾åº¦è®­ç»ƒæ—¶çš„åŒæ ·å¤„ç†
        q_values = agent(states)
        q_values = torch.clamp(q_values, -10.0, 10.0)
        
        q_current = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
        
        with torch.no_grad():
            # åŒQå­¦ä¹ ç›®æ ‡è®¡ç®—
            next_q_agent = agent(next_states)
            next_actions = next_q_agent.argmax(1)
            next_q_target = target_agent(next_states)
            
            # æ›´åŠ ç¨³å¥çš„ç›®æ ‡è®¡ç®—
            q_next = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)
            # æ·»åŠ å™ªå£°æŠ‘åˆ¶
            noise = torch.randn_like(q_next) * 0.01
            q_next = q_next + noise
            
            # ä½¿ç”¨è¾ƒä½çš„gammaå€¼å‡å°æ—¶åºå·®åˆ†ç›®æ ‡çš„å˜åŒ–å¹…åº¦
            gamma_effective = gamma * 0.95  # æœ‰æ•ˆé™ä½æŠ˜æ‰£å› å­
            q_target = rewards + gamma_effective * q_next * (~dones)
            q_target = torch.clamp(q_target, -3.0, 3.0)  # æ›´ä¸¥æ ¼çš„é™åˆ¶
        
        # ä½¿ç”¨å¹³æ»‘L1æŸå¤± - ä¿®æ”¹ä¸ºç„¦ç‚¹æŸå¤±
        if hasattr(agent, 'classify'): # å¦‚æœæ¨¡å‹æœ‰åˆ†ç±»å™¨è¾“å‡º
            # åˆ†ç±»æŸå¤±éƒ¨åˆ†ä½¿ç”¨ç„¦ç‚¹æŸå¤±
            class_loss = focal_loss(q_values, actions, gamma=2.0, alpha=0.25)
            # å›å½’éƒ¨åˆ†ä»ä½¿ç”¨å¹³æ»‘L1æŸå¤±
            reg_loss = F.smooth_l1_loss(q_current, q_target, reduction='none')
            reg_loss = (weights * reg_loss).mean()
            # ç»„åˆæŸå¤±
            loss = class_loss + reg_loss
        else:
            # ä»ä½¿ç”¨å¹³æ»‘L1æŸå¤±
            loss = F.smooth_l1_loss(q_current, q_target, reduction='none')
            loss = (weights * loss).mean()
        
        # L2æ­£åˆ™åŒ–
        l2_reg = 0.0003 * sum(p.pow(2.0).sum() for p in agent.parameters())
        loss = loss + l2_reg
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=grad_clip)
        optimizer.step()
    
    # æ›´æ–°ä¼˜å…ˆçº§
    with torch.no_grad():
        td_errors = torch.abs(q_target - q_current)
        # é™åˆ¶TDè¯¯å·®èŒƒå›´
        td_errors = torch.clamp(td_errors, 0.01, 10.0)
    
    replay_buffer.update_priorities(indices, td_errors.detach().cpu().numpy())
    
    return loss.item()    
    

# åœ¨interactive_train_rlad_guiå‡½æ•°ä¸­ï¼Œæ·»åŠ è¿‡æ‹Ÿåˆæ£€æµ‹å’Œæ—©åœæœºåˆ¶ï¼š
# åœ¨RLADv3.2.pyæ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹å‡½æ•°

def create_model_ensemble(model_class, num_models, model_params, device, weights=None):
    """åˆ›å»ºæ¨¡å‹é›†æˆ"""
    models = []
    for i in range(num_models):
        model = model_class(**model_params).to(device)
        if weights is not None:
            model.load_state_dict(torch.load(weights[i], map_location=device))
        models.append(model)
    return models

def ensemble_predict(models, x, device):
    """ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    probs = []
    features_list = []
    
    for model in models:
        model.eval()
        with torch.no_grad():
            if hasattr(model, 'return_features') and callable(model.return_features):
                logits, features = model(x.to(device), return_features=True)
                features_list.append(features)
            else:
                logits = model(x.to(device))
            
            prob = F.softmax(logits, dim=1)
            probs.append(prob)
    
    avg_prob = torch.mean(torch.stack(probs), dim=0)
    
    if features_list:
        avg_features = torch.mean(torch.stack(features_list), dim=0)
        return avg_prob, avg_features
    
    return avg_prob, None
def interactive_train_rlad_gui(agent, target_agent, optimizer, scheduler, replay_buffer, 
                              X_train, y_train, raw_train, X_val, y_val, raw_val, device, 
                              annotation_system, args):
    """
    åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„äº¤äº’å¼ä¸»åŠ¨å­¦ä¹ è®­ç»ƒæµç¨‹
    å¢å¼ºç‰ˆï¼šæ·»åŠ é¢„çƒ­ã€åŠ¨æ€æ‰¹å¤„ç†ã€å¢å¼ºæ—©åœå’Œå­¦ä¹ ç‡è°ƒæ•´
    """
    # ç¡®ä¿è®­ç»ƒæ•°æ®å·²è½¬ç§»åˆ°GPUå¹¶ä¸”ç±»å‹æ­£ç¡®
    X_train_gpu = X_train.to(device, dtype=torch.float32) if isinstance(X_train, torch.Tensor) else torch.tensor(X_train, device=device, dtype=torch.float32)
    y_train_cpu = y_train.cpu().numpy() if isinstance(y_train, torch.Tensor) else y_train.copy()
    
    # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨ - ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
    val_dataset = TimeSeriesDataset(X_val.astype(np.float32), y_val)
    val_loader = DataLoader(val_dataset, batch_size=min(128, len(X_val)), shuffle=False, 
                           num_workers=args.num_workers, pin_memory=args.pin_memory)
    
    # åˆå§‹åŒ–è®°å¿†å›æ”¾ç¼“å†²åŒº
    if len(replay_buffer) == 0:
        print("ğŸ”¥ Pre-warming replay buffer...")
        # æ‰¾å‡ºæ‰€æœ‰å·²æ ‡è®°æ ·æœ¬
        labeled_indices = np.where(y_train_cpu != -1)[0]
        
        # ä»å·²æ ‡è®°æ ·æœ¬ä¸­éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†è¿›è¡Œçƒ­èº«
        warmup_size = min(len(labeled_indices), 300)  # ç¡®ä¿ä¸è¶…è¿‡å·²æ ‡è®°æ ·æœ¬æ•°é‡
        warmup_indices = np.random.choice(labeled_indices, size=warmup_size, replace=False)
        # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨ - æ·»åŠ æ•°æ®å¢å¼º
        val_dataset = TimeSeriesDataset(X_val.astype(np.float32), y_val, augment=False)  # éªŒè¯é›†ä¸å¢å¼º
        val_loader = DataLoader(val_dataset, batch_size=min(128, len(X_val)), shuffle=False, 
                            num_workers=args.num_workers, pin_memory=args.pin_memory)

        # åœ¨é¢„çƒ­å›æ”¾ç¼“å†²åŒºéƒ¨åˆ†æ·»åŠ å¢å¼ºæ•°æ®
        for idx in tqdm(warmup_indices, desc="Warming-up"):
            state = X_train_gpu[idx]
            label = y_train_cpu[idx]
            
            # æ·»åŠ åŸå§‹æ ·æœ¬
            action = 1 if np.random.rand() < 0.5 else 0
            reward = enhanced_compute_reward(action, label)
            next_idx = (idx + 1) % len(X_train_gpu)
            next_state = X_train_gpu[next_idx]
            replay_buffer.push(state.cpu().float(), action, reward, next_state.cpu().float(), False)
            
            # ä¸ºå¼‚å¸¸æ ·æœ¬æ·»åŠ å¢å¼ºç‰ˆæœ¬
            if label == 1:  # å¯¹å¼‚å¸¸æ ·æœ¬è¿›è¡Œå¢å¼º
                aug_state_np = augment_time_series(state.cpu().numpy(), label)
                aug_state = torch.FloatTensor(aug_state_np).to(device)
                action = 1  # å¯¹å¢å¼ºçš„å¼‚å¸¸æ ·æœ¬ï¼Œé¢„æœŸåŠ¨ä½œä¸º1
                reward = enhanced_compute_reward(action, label, is_augmented=True)
                replay_buffer.push(aug_state.cpu().float(), action, reward, next_state.cpu().float(), False)
        print(f"ğŸ”¥ Warm-up complete. Buffer size: {len(replay_buffer)}")
    
    # åˆå§‹åŒ–è®­ç»ƒå†å²è®°å½• - ä¿®å¤å­—æ®µå
    history = {
        'episodes': [], 'losses': [], 'val_f1': [], 'val_precision': [], 
        'val_recall': [], 'val_auc': [], 'learning_rate': []  # ç¡®ä¿å­—æ®µåä¸€è‡´
    }
    
    # åˆ›å»ºæœªæ ‡è®°æ ·æœ¬æ± 
    unlabeled_idx_pool = deque([i for i in range(len(y_train_cpu)) if y_train_cpu[i] == -1])
    # å·²æ ‡è®°æ ·æœ¬ç´¢å¼•é›†åˆ
    human_labeled_indices = set([i for i in range(len(y_train_cpu)) if y_train_cpu[i] != -1])
    
    print("\nğŸš€ Starting Interactive RLAD Training with Active Learning...")
    print(f"ğŸ“Š Batch Size: {args.batch_size_rl}, Workers: {args.num_workers}, Mixed Precision: {args.mixed_precision}")
    
    # ä¼˜åŒ–1: æ·»åŠ å­¦ä¹ ç‡é¢„çƒ­
    warmup_epochs = 10  # å¢åŠ é¢„çƒ­å‘¨æœŸ
    initial_lr = args.lr / 20  # æ›´ä½çš„åˆå§‹å­¦ä¹ ç‡
    target_lr = args.lr

    # ä¼˜åŒ–2: è°ƒæ•´æ—©åœç­–ç•¥ - å¢åŠ è€å¿ƒï¼Œé™ä½æ”¹è¿›é˜ˆå€¼
    patience_limit = 20  # å¢åŠ è€å¿ƒæœŸé™
    min_improvement = 0.002  # é™ä½æœ€å°æ”¹è¿›é˜ˆå€¼
    best_val_f1 = 0
    patience_counter = 0
    
    # åˆ›å»ºtensorboardæ—¥å¿—è®°å½•å™¨ - ä¿®æ”¹ä¸ºå¯é€‰
    writer = None
    try:
        from torch.utils.tensorboard import SummaryWriter
        log_dir = os.path.join(args.output_dir, 'tensorboard_logs')
        os.makedirs(log_dir, exist_ok=True)
        writer = SummaryWriter(log_dir)
        print("âœ… TensorBoardæ—¥å¿—è®°å½•å·²å¯ç”¨")
    except ImportError:
        print("âš ï¸ TensorBoardæœªå®‰è£…ï¼Œè·³è¿‡æ—¥å¿—è®°å½•åŠŸèƒ½")
        writer = None
    
    # æ¢¯åº¦ç¼©æ”¾å™¨ï¼Œç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler() if args.mixed_precision and device.type == 'cuda' else None
    
    try:
        for episode in range(args.num_episodes):
            print(f"\nğŸ“ Episode {episode+1}/{args.num_episodes}")
            agent.train()
            ep_losses = []
            
            # å­¦ä¹ ç‡é¢„çƒ­
            if episode < warmup_epochs:
                curr_lr = initial_lr + (target_lr - initial_lr) * (episode / warmup_epochs)
                for param_group in optimizer.param_groups:
                    param_group['lr'] = curr_lr
                print(f"ğŸ”¥ Warmup learning rate: {curr_lr:.6f}")
            
            # --- ä¸»åŠ¨å­¦ä¹ ï¼šè¯·æ±‚äººå·¥æ ‡æ³¨ ---
            if annotation_system.use_gui and episode > 0 and episode % args.annotation_frequency == 0 and len(unlabeled_idx_pool) > 0:
                print(f"\nğŸ” Episode {episode}: Entering annotation phase...")
                agent.eval()
                query_batch_size = min(32, len(unlabeled_idx_pool)) 
                query_indices = [unlabeled_idx_pool.popleft() for _ in range(query_batch_size)]
                
                with torch.no_grad():
                    query_states = X_train_gpu[query_indices]
                    q_values = agent(query_states)
                    probs = F.softmax(q_values, dim=1)
                    uncertainties = 1.0 - torch.max(probs, dim=1)[0]
                
                most_uncertain_local_idx = uncertainties.argmax().item()
                uncertainty_value = uncertainties[most_uncertain_local_idx].item()
                query_idx_to_annotate = query_indices.pop(most_uncertain_local_idx)
                
                for idx in query_indices:
                    unlabeled_idx_pool.append(idx)

                print(f"ğŸ¤” Model uncertainty for sample {query_idx_to_annotate}: {uncertainty_value:.4f}")
                
                auto_pred_label = q_values[most_uncertain_local_idx].argmax().item()
                human_label = annotation_system.get_human_annotation(
                    window_data=X_train[query_idx_to_annotate],
                    window_idx=query_idx_to_annotate,
                    original_data_segment=raw_train[query_idx_to_annotate],
                    auto_predicted_label=auto_pred_label
                )
                
                if human_label in [0, 1]:
                    y_train_cpu[query_idx_to_annotate] = human_label
                    human_labeled_indices.add(query_idx_to_annotate)
                    print(f"ğŸ’¡ Human label: {human_label}, Model predicted: {auto_pred_label}")
                    
                    print(f"âš¡ï¸ Performing controlled training on new sample...")
                    state = X_train_gpu[query_idx_to_annotate]
                    for i in range(10):
                        action = agent.get_action(state, epsilon=0.1)
                        reward = enhanced_compute_reward(action, human_label, is_human_labeled=True)
                        next_state = X_train_gpu[(query_idx_to_annotate + 1) % len(X_train_gpu)]
                        replay_buffer.push(state.cpu(), action, reward, next_state.cpu(), False)
                        
                        if i % 5 == 0:
                            loss = enhanced_train_dqn_step(
                                agent, target_agent, replay_buffer, optimizer, device,
                                batch_size=min(16, len(replay_buffer)), scaler=scaler
                            )
                            if loss is not None:
                                print(f"    Step {i+1}/10: Loss = {loss:.4f}")
                                
                    replay_buffer.update_priorities([-1], [2.0])
                    
                elif human_label == -2:
                    print("ğŸ›‘ ç”¨æˆ·è¯·æ±‚é€€å‡ºæ ‡æ³¨ï¼Œè®­ç»ƒå°†ç»§ç»­...")
                    unlabeled_idx_pool.append(query_idx_to_annotate)
                    annotation_system.use_gui = False
                else:
                    unlabeled_idx_pool.append(query_idx_to_annotate)
            
            agent.train()
                        # åœ¨interactive_train_rlad_guiå‡½æ•°ä¸­ï¼Œåœ¨å¤„ç†äººå·¥æ ‡æ³¨éƒ¨åˆ†åæ·»åŠ 
            
            # ä½¿ç”¨ä¸“å®¶è§„åˆ™å¯¹æœªæ ‡æ³¨æ•°æ®è¿›è¡Œé¢„ç­›é€‰
            if len(unlabeled_idx_pool) > 100:
                print("ğŸ” åº”ç”¨ä¸“å®¶è§„åˆ™é¢„ç­›é€‰æœªæ ‡æ³¨æ ·æœ¬...")
                expert_scores = {}
                
                # å–æ ·30ä¸ªæœªæ ‡æ³¨æ ·æœ¬è¿›è¡Œè¯„åˆ†
                sample_size = min(30, len(unlabeled_idx_pool))
                sample_indices = random.sample(list(unlabeled_idx_pool), sample_size)
                
                for idx in sample_indices:
                    expert_score = apply_expert_rules(X_train[idx].numpy(), 
                                                      raw_train[idx].numpy() if raw_train is not None else None)
                    expert_scores[idx] = expert_score
                
                # æ‰¾å‡ºä¸“å®¶è§„åˆ™è®¤ä¸ºæœ€å¯èƒ½å¼‚å¸¸çš„æ ·æœ¬
                high_score_indices = [idx for idx, score in expert_scores.items() if score > 0.5]
                
                if high_score_indices:
                    # ä¼˜å…ˆé€‰æ‹©è¿™äº›æ ·æœ¬è¿›è¡Œä¸»åŠ¨å­¦ä¹ 
                    print(f"âœ“ ä¸“å®¶è§„åˆ™å‘ç° {len(high_score_indices)} ä¸ªæ½œåœ¨å¼‚å¸¸æ ·æœ¬")
                    next_query_idx = high_score_indices[0]
                    
                    # å°†è¯¥æ ·æœ¬ç§»åˆ°é˜Ÿåˆ—å‰é¢
                    if next_query_idx in unlabeled_idx_pool:
                        unlabeled_idx_pool.remove(next_query_idx)
                        unlabeled_idx_pool.appendleft(next_query_idx)
            # --- è®­ç»ƒæ­¥éª¤ --- å®ç°æ¸è¿›å¼æ‰¹æ¬¡å¢é•¿
            base_batch_size = args.batch_size_rl
            if episode < args.num_episodes // 5:
                effective_batch_size = max(8, base_batch_size // 2)
                steps_per_episode = 9
            elif episode < args.num_episodes // 3:
                effective_batch_size = max(16, base_batch_size // 1.5)
                steps_per_episode = 7
            elif episode < args.num_episodes * 2 // 3:
                effective_batch_size = base_batch_size
                steps_per_episode = 5
            else:
                effective_batch_size = min(64, base_batch_size * 2)
                steps_per_episode = 3
                
            # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸è¶…è¿‡ç¼“å†²åŒºå¤§å°
            effective_batch_size = min(effective_batch_size, len(replay_buffer))
                
            if len(replay_buffer) >= args.batch_size_rl:
                print(f"ğŸ”„ Performing {steps_per_episode} controlled training steps...")
                for step in range(steps_per_episode):
                    loss = enhanced_train_dqn_step(
                        agent, target_agent, replay_buffer, optimizer, device, 
                        batch_size=args.batch_size_rl, scaler=scaler,
                        grad_clip=args.grad_clip
                    )
                    if loss is not None:
                        ep_losses.append(loss)
                        if (step + 1) % 2 == 1:
                            print(f"    Training step {step+1}/{steps_per_episode}: Loss = {loss:.4f}")
            
            # --- æ›´æ–°ç›®æ ‡ç½‘ç»œ ---
            if episode % args.target_update_freq == 0:
                target_agent.load_state_dict(agent.state_dict())
                print("ğŸ”„ Updated target network")
                
            # --- æ¨¡å‹è¯„ä¼° ---
            print("ğŸ“Š Evaluating model...")
            val_metrics = enhanced_evaluate_model(agent, val_loader, device)
            lr_current = optimizer.param_groups[0]['lr']
            
            # è®°å½•åˆ°è®­ç»ƒå†å²
            history['episodes'].append(episode)
            if ep_losses:
                history['losses'].append(np.mean(ep_losses))
            history['val_f1'].append(val_metrics['f1'])
            history['val_precision'].append(val_metrics['precision'])
            history['val_recall'].append(val_metrics['recall']) 
            history['val_auc'].append(val_metrics['auc_roc'])
            history['learning_rate'].append(lr_current)
            
            # è®°å½•åˆ°tensorboard
            if writer is not None:
                if ep_losses:
                    writer.add_scalar('Loss/train', np.mean(ep_losses), episode)
                writer.add_scalar('Metrics/F1', val_metrics['f1'], episode)
                writer.add_scalar('Metrics/Precision', val_metrics['precision'], episode)
                writer.add_scalar('Metrics/Recall', val_metrics['recall'], episode)
                writer.add_scalar('Metrics/AUC', val_metrics['auc_roc'], episode)
                writer.add_scalar('Learning/LR', lr_current, episode)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(val_metrics['f1'])
            if lr_current != optimizer.param_groups[0]['lr']:
                print(f"ğŸ“‰ Learning rate reduced to {optimizer.param_groups[0]['lr']:.6f}")
            
            # æ—©åœç­–ç•¥
            if val_metrics['f1'] > best_val_f1 + min_improvement:
                best_val_f1 = val_metrics['f1']
                patience_counter = 0
                torch.save(agent.state_dict(), os.path.join(args.output_dir, 'best_model.pth'))
                print(f"â­ New best model! Val F1: {best_val_f1:.4f}")
            elif val_metrics['f1'] > best_val_f1:
                best_val_f1 = val_metrics['f1']
                patience_counter = max(0, patience_counter - 1)
                torch.save(agent.state_dict(), os.path.join(args.output_dir, 'best_model.pth'))
                print(f"â­ New best model! Val F1: {best_val_f1:.4f}")
            else:
                patience_counter += 1
                
            print(f"ğŸ“ˆ Episode {episode}: Val F1={val_metrics['f1']:.4f}, Precision={val_metrics['precision']:.4f}, Recall={val_metrics['recall']:.4f}")
            print(f"ğŸ¯ Patience: {patience_counter}/{patience_limit}")
            
            if patience_counter > patience_limit // 2 and args.batch_size_rl < 64:
                args.batch_size_rl = min(64, args.batch_size_rl * 2)
                print(f"ğŸ“ˆ å¢å¤§æ‰¹æ¬¡å¤§å°åˆ° {args.batch_size_rl} ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§")
            
            if patience_counter >= patience_limit:
                print(f"ğŸ›‘ æ—©åœè§¦å‘: {patience_limit} è½®æ— æ”¹å–„")
                break
                
            if episode > args.num_episodes // 2 and best_val_f1 < 0.7:
                for param_group in optimizer.param_groups:
                    param_group['lr'] = min(param_group['lr'] * 1.2, 1e-3)
                    print(f"ğŸš€ Training progress slow, boosting LR to {param_group['lr']:.6f}")
                
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«æ‰‹åŠ¨ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        traceback.print_exc()
    finally:
        if writer is not None:
            writer.close()
        
    print(f"\nâœ… Training complete! Best validation F1: {best_val_f1:.4f}")
    return best_val_f1, history
# =================================
# é€ç‚¹æ ‡è®°ä¸ä¸»å‡½æ•° (æ¥è‡ª v2.4)
# =================================

def _process_window_parallel(args: Tuple[int, int, int]) -> List[int]:
    start_idx, ws, data_length = args
    return [start_idx + i for i in range(ws) if start_idx + i < data_length]

def mark_anomalies_pointwise(df_original, test_window_indices, test_predictions, window_size, feature_column, output_path):
    print("Mapping window predictions to point-wise labels...")
    from multiprocessing import Pool, cpu_count
    df_original['pointwise_prediction'] = 0
    anomaly_window_indices = test_window_indices[test_predictions == 1]
    
    if len(anomaly_window_indices) > 0:
        print(f"Found {len(anomaly_window_indices)} anomalous windows in test set.")
        pool_args = [(idx, window_size, len(df_original)) for idx in anomaly_window_indices]
        try:
            with Pool(processes=min(cpu_count(), 8)) as pool:
                results = pool.map(_process_window_parallel, pool_args)
        except Exception as e:
            print(f"Parallel processing failed ({e}), falling back to serial...")
            results = [_process_window_parallel(arg) for arg in pool_args]
        
        point_indices_to_mark = set(idx for sublist in results for idx in sublist)
        if point_indices_to_mark:
            df_original.loc[list(point_indices_to_mark), 'pointwise_prediction'] = 1
    
    output_filename = os.path.join(output_path, f'predictions_{feature_column}.csv')
    df_original.to_csv(output_filename, index=False, encoding='utf-8-sig')
    print(f"Point-wise prediction CSV saved to: {output_filename}")


# åœ¨mainå‡½æ•°ä¸­ï¼Œä¿®æ”¹è¾“å‡ºç›®å½•è®¾ç½®éƒ¨åˆ†ï¼ˆå¤§çº¦ç¬¬1080-1090è¡Œï¼‰ï¼š

def main():
    parser = argparse.ArgumentParser(description='Optimized Interactive RLAD Anomaly Detection')
    # æ•°æ®å‚æ•°
    parser.add_argument('--data_path', type=str, default="clean_data.csv", help='Data file path')
    parser.add_argument('--feature_column', type=str, default=None, help='Feature column name')
    parser.add_argument('--output_dir', type=str, default="./output_rlad_v3_optimized", help='Output directory')
    parser.add_argument('--window_size', type=int, default=288, help='Sliding window size')
    parser.add_argument('--stride', type=int, default=12, help='Sliding window stride')
    
    # è®­ç»ƒæ§åˆ¶å‚æ•°
    parser.add_argument('--num_episodes', type=int, default=100, help='Number of training episodes')
    parser.add_argument('--annotation_frequency', type=int, default=5, help='Annotation frequency (every N episodes)')
    parser.add_argument('--use_gui', action='store_true', default=True, help='Enable GUI for annotation')
    parser.add_argument('--no_gui', action='store_false', dest='use_gui', help='Disable GUI, use command line')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    # ä¼˜åŒ–å‚æ•° - ç»è¿‡è°ƒæ•´çš„è¶…å‚æ•°
    parser.add_argument('--lr', type=float, default=8e-5, help='Learning rate')
    parser.add_argument('--batch_size_rl', type=int, default=16, help='RL training batch size')
    parser.add_argument('--target_update_freq', type=int, default=5, help='Target network update frequency')
    parser.add_argument('--epsilon_start', type=float, default=0.3, help='Initial exploration rate')
    parser.add_argument('--epsilon_end', type=float, default=0.02, help='Final exploration rate')
    parser.add_argument('--epsilon_decay_rate', type=float, default=0.98, help='Epsilon decay rate')
    parser.add_argument('--gamma', type=float, default=0.92, help='Discount factor')
    
    # æ–°å¢è®­ç»ƒç¨³å®šæ€§å‚æ•°
    parser.add_argument('--grad_clip', type=float, default=0.5, help='Gradient clipping threshold')
    parser.add_argument('--loss_clip', type=float, default=1.0, help='Loss clipping threshold')
    parser.add_argument('--weight_decay', type=float, default=2e-4, help='Weight decay for optimizer')
    parser.add_argument('--early_stopping', type=int, default=15, help='Early stopping patience')
    parser.add_argument('--scheduler_patience', type=int, default=5, help='LR scheduler patience')
    parser.add_argument('--scheduler_factor', type=float, default=0.7, help='LR reduction factor')
    parser.add_argument('--dropout', type=float, default=0.2, help='Dropout probability')
    
    # GPUä¼˜åŒ–å‚æ•°
    parser.add_argument('--force_cpu', action='store_true', help='Force CPU usage')
    parser.add_argument('--gpu_id', type=int, default=0, help='GPU ID to use')
    parser.add_argument('--num_workers', type=int, default=4, help='Number of data loading workers')
    parser.add_argument('--pin_memory', action='store_true', default=True, help='Use pinned memory')
    parser.add_argument('--mixed_precision', action='store_true', default=True, help='Use mixed precision training')
    
    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å”¯ä¸€è¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    args.output_dir = f"{args.output_dir}_{timestamp}"
    print(f"ğŸ“‚ æœ¬æ¬¡è¿è¡Œçš„è¾“å‡ºå°†ä¿å­˜åˆ°: {args.output_dir}")
    os.makedirs(args.output_dir, exist_ok=True)
    
    # è®¾å¤‡é€‰æ‹©
    device = torch.device("cpu")
    if not args.force_cpu and torch.cuda.is_available():
        if torch.cuda.device_count() > args.gpu_id:
            device = torch.device(f"cuda:{args.gpu_id}")
            torch.cuda.set_device(args.gpu_id)
            
            # æ˜¾ç¤ºGPUä¿¡æ¯
            gpu_name = torch.cuda.get_device_name(args.gpu_id)
            memory_total = torch.cuda.get_device_properties(args.gpu_id).total_memory / 1024**3
            memory_allocated = torch.cuda.memory_allocated(args.gpu_id) / 1024**3
            memory_cached = torch.cuda.memory_reserved(args.gpu_id) / 1024**3
            print(f"ğŸš€ ä½¿ç”¨GPU: {gpu_name} (ID: {args.gpu_id})")
            print(f"ğŸ“Š GPUæ€»å†…å­˜: {memory_total:.2f} GB")
            print(f"ğŸ“Š å·²åˆ†é…å†…å­˜: {memory_allocated:.2f} GB")
            print(f"ğŸ“Š ç¼“å­˜å†…å­˜: {memory_cached:.2f} GB")
            
            # å¯ç”¨ä¼˜åŒ–è®¾ç½®
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            print("ğŸ”§ å·²å¯ç”¨cuDNNåŸºå‡†æµ‹è¯•ä¼˜åŒ–")
        else:
            print(f"âš ï¸ æŒ‡å®šçš„GPU ID {args.gpu_id} ä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
    else:
        print("ğŸ–¥ï¸ ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")
    
    print(f"è®¾å¤‡: {device}")
    
    try:
        # ç”¨æˆ·äº¤äº’å¼é€‰æ‹©æ”¯æ¶
        actual_selected_column_name = args.feature_column
        if not actual_selected_column_name:
            try:
                print(f"ğŸ“– æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶ '{args.data_path}' ä»¥é€‰æ‹©æ”¯æ¶...")
                # è¯»å–æ•´ä¸ªCSVæ–‡ä»¶ä»¥è·å¾—å‡†ç¡®çš„ç»Ÿè®¡æ•°æ®
                df_preview = pd.read_csv(args.data_path)
                
                # è·å–æ‰€æœ‰åˆ—å
                all_columns = df_preview.columns.tolist()
                
                # è¿‡æ»¤å‡ºæ•°å€¼åˆ—ï¼ˆæ”¯æ¶åˆ—ï¼‰
                numeric_columns = []
                for col in all_columns:
                    if col not in ['Unnamed: 0', 'Time', 'time'] and df_preview[col].dtype in ['int64', 'float64']:
                        numeric_columns.append(col)
                
                if not numeric_columns:
                    raise ValueError("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ”¯æ¶æ•°æ®åˆ—")
                
                print("ğŸ’¡ æç¤º: è¯·é€‰æ‹©è¦è¿›è¡Œå¼‚å¸¸æ£€æµ‹çš„æ¶²å‹æ”¯æ¶")
                print("ğŸ“‹ å¯ç”¨æ”¯æ¶åˆ—è¡¨:")
                
                for i, col in enumerate(numeric_columns):
                    col_data = df_preview[col].dropna()
                    if len(col_data) > 0:
                        data_min, data_max = col_data.min(), col_data.max()
                        data_mean = col_data.mean()
                        data_std = col_data.std()
                        print(f"   [{i:3d}] {col:>8s} - æ•°æ®ç‚¹: {len(col_data):>5d}, èŒƒå›´: {data_min:6.2f} ~ {data_max:6.2f}, å‡å€¼: {data_mean:6.2f}, æ ‡å‡†å·®: {data_std:6.2f}")
                    else:
                        print(f"   [{i:3d}] {col:>8s} - æ— æœ‰æ•ˆæ•°æ®")
                
                while True:
                    user_input = input(f"ğŸ“‹ è¯·è¾“å…¥æ”¯æ¶ç¼–å· [0-{len(numeric_columns)-1}] (æˆ–è¾“å…¥ 'q' é€€å‡º): ").strip()
                    
                    if user_input.lower() == 'q':
                        print("ğŸ‘‹ ç”¨æˆ·é€€å‡ºç¨‹åº")
                        return 0
                    
                    try:
                        selected_idx = int(user_input)
                        if 0 <= selected_idx < len(numeric_columns):
                            selected_column = numeric_columns[selected_idx]
                            print(f"âœ… æ‚¨å·²é€‰æ‹©æ”¯æ¶: '{selected_column}'")
                            
                            # æ˜¾ç¤ºæ”¯æ¶æ•°æ®é¢„è§ˆ
                            col_data = df_preview[selected_column].dropna()
                            if len(col_data) > 0:
                                print(f"ğŸ“Š æ”¯æ¶ '{selected_column}' æ•°æ®é¢„è§ˆ:")
                                print(f"   - æ•°æ®ç‚¹æ•°: {len(col_data)}")
                                print(f"   - æ•°å€¼èŒƒå›´: {col_data.min():.2f} ~ {col_data.max():.2f}")
                                print(f"   - å¹³å‡å€¼: {col_data.mean():.2f}")
                                print(f"   - æ ‡å‡†å·®: {col_data.std():.2f}")
                                
                                confirm = input(f"ğŸ¤” ç¡®è®¤é€‰æ‹©æ”¯æ¶ '{selected_column}' å—? [y/N]: ").strip().lower()
                                if confirm in ['y', 'yes']:
                                    actual_selected_column_name = selected_column
                                    break
                                else:
                                    print("ğŸ”„ è¯·é‡æ–°é€‰æ‹©...")
                                    continue
                            else:
                                print(f"âš ï¸ æ”¯æ¶ '{selected_column}' æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¯·é€‰æ‹©å…¶ä»–æ”¯æ¶")
                        else:
                            print(f"âŒ æ— æ•ˆè¾“å…¥: è¯·è¾“å…¥ 0 åˆ° {len(numeric_columns)-1} ä¹‹é—´çš„æ•°å­—")
                    except ValueError:
                        print("âŒ æ— æ•ˆè¾“å…¥: è¯·è¾“å…¥æ•°å­—æˆ– 'q'")
                        
            except Exception as e:
                print(f"âŒ è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
                print("ğŸ”„ ä½¿ç”¨é»˜è®¤æ”¯æ¶...")
                actual_selected_column_name = None
        
        # æ•°æ®åŠ è½½
        print(f"ğŸ“¥ Loading data: {args.data_path}")
        (X_train, y_train, raw_train, train_window_indices,
         X_val, y_val, raw_val, val_window_indices,
         X_test, y_test, raw_test, test_window_indices) = load_hydraulic_data_with_stl_lof(
            args.data_path, args.window_size, args.stride, actual_selected_column_name,
            stl_period=24, lof_contamination=0.02, unlabeled_fraction=0.1
        )
        
        print(f"âœ… Data loaded: Train={X_train.shape}, Val={X_val.shape}, Test={X_test.shape}")
        final_selected_col = actual_selected_column_name
        print(f"âœ… æœ€ç»ˆä½¿ç”¨çš„æ”¯æ¶: '{final_selected_col}'")
        
        # ä¸ºç‚¹çº§æ˜ å°„ä¿å­˜å¿…è¦ä¿¡æ¯
        df_for_point_mapping = pd.read_csv(args.data_path)
        test_window_original_indices = test_window_indices
        
        print("ğŸ“ˆ æ•°æ®åŠ è½½å®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...")
        
        # æ¨¡å‹åˆå§‹åŒ–
        input_dim = X_train.shape[2]
        # åˆ›å»ºæ¨¡å‹å®ä¾‹æ—¶ä½¿ç”¨ç¡¬ç¼–ç çš„é»˜è®¤å€¼ï¼Œè€Œä¸æ˜¯ä»argsä¸­è·å–
        agent = EnhancedRLADAgent(
            input_dim=1,
            seq_len=X_train.shape[1],
            hidden_size=64,
            num_heads=2,
            dropout=0.2,
            bidirectional=True,
            include_pos=True,
            num_actions=2,
            use_lstm=True,
            use_attention=True,
            num_layers=1  # æ·»åŠ LSTMå±‚æ•°å‚æ•°
        ).to(device)
        
        # åŒæ ·ä¸ºtarget_agentæ·»åŠ å‚æ•°
        target_agent = EnhancedRLADAgent(
            input_dim=1,
            seq_len=X_train.shape[1],
            hidden_size=64,
            num_heads=2,
            dropout=0.2,
            bidirectional=True,
            include_pos=True,
            num_actions=2,
            use_lstm=True,
            use_attention=True,
            num_layers=1  # æ·»åŠ LSTMå±‚æ•°å‚æ•°
        ).to(device)
        
        target_agent.load_state_dict(agent.state_dict())
        target_agent.eval()

        # ä¼˜åŒ–å™¨ä½¿ç”¨AdamWå¹¶å¢åŠ å‚æ•°
        optimizer = optim.AdamW(
            agent.parameters(), 
            lr=args.lr,
            weight_decay=args.weight_decay,
            amsgrad=True,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='max',
            factor=args.scheduler_factor,
            patience=args.scheduler_patience,
            threshold=0.005,
            min_lr=1e-6,
            verbose=True
        )
        
        # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œå¯ç”¨ä¼˜å…ˆçº§é‡‡æ ·
        replay_buffer = PrioritizedReplayBuffer(capacity=10000, alpha=0.6)
        
        # åˆ›å»ºäººå·¥æ ‡æ³¨ç³»ç»Ÿ
        annotation_system = HumanAnnotationSystem(
            output_dir=args.output_dir, 
            window_size=args.window_size, 
            use_gui=args.use_gui
        )
        
        # åŠ è½½å†å²æ ‡æ³¨è®°å½•
        annotation_history_path = os.path.join(args.output_dir, "annotation_history.json")
        if os.path.exists(annotation_history_path):
            try:
                with open(annotation_history_path, "r", encoding='utf-8') as f:
                    annotation_history = json.load(f)
                annotation_system.annotation_history = annotation_history
                print(f"âœ… å·²åŠ è½½ {len(annotation_history)} æ¡å†å²æ ‡æ³¨è®°å½•")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åŠ è½½æ ‡æ³¨å†å²: {e}")
        
        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        visualization_dir = os.path.join(args.output_dir, "visualizations")
        os.makedirs(visualization_dir, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒå‚æ•°
        with open(os.path.join(args.output_dir, "training_args.json"), "w", encoding='utf-8') as f:
            json.dump(vars(args), f, indent=4, default=convert_to_serializable)
        
        # äº¤äº’å¼è®­ç»ƒ
        _, training_history = interactive_train_rlad_gui(
            agent, target_agent, optimizer, scheduler, replay_buffer,
            X_train, y_train, raw_train, X_val, y_val, raw_val, device,
            annotation_system, args
        )
        
        # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
        best_model_path = os.path.join(args.output_dir, 'best_model.pth')
        if os.path.exists(best_model_path):
            print("ğŸ“¥ Loading best model for final evaluation...")
            try:
                best_model_path = os.path.join(args.output_dir, 'best_model.pth')
                agent.load_state_dict(torch.load(best_model_path, map_location=device))
                print("âœ… æˆåŠŸåŠ è½½æœ€ä½³æ¨¡å‹")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•åŠ è½½æœ€ä½³æ¨¡å‹: {e}")
            
            # åˆ›å»ºéªŒè¯æ•°æ®é›† - æ·»åŠ è¿™ä¸€è¡Œä¿®å¤æœªå®šä¹‰é”™è¯¯
            val_dataset = TimeSeriesDataset(X_val.astype(np.float32), y_val)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
            test_dataset = TimeSeriesDataset(X_test.astype(np.float32), y_test)
            test_loader = DataLoader(test_dataset, batch_size=min(128, len(X_test)), 
                                shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory)
            
            # åœ¨åº”ç”¨åˆ°æµ‹è¯•é›†ä¹‹å‰ï¼Œå…ˆç”¨éªŒè¯é›†æ‰¾åˆ°æœ€ä½³é˜ˆå€¼
            optimal_threshold = find_optimal_threshold(
                val_dataset=val_dataset,  # ç°åœ¨è¿™ä¸ªå˜é‡å·²å®šä¹‰
                agent=agent,
                device=device
            )
            
            # ç„¶åä½¿ç”¨æœ€ä½³é˜ˆå€¼è¯„ä¼°æµ‹è¯•é›†
            test_metrics = enhanced_evaluate_model(
                agent=agent, 
                data_loader=test_loader, 
                device=device,
                threshold=optimal_threshold  # ä½¿ç”¨æ‰¾åˆ°çš„æœ€ä½³é˜ˆå€¼
            )
            
            # è¾“å‡ºæœ€ç»ˆæµ‹è¯•ç»“æœ
            print(f"\nğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœ (æ”¯æ¶: '{final_selected_col}'):")
            print(f"   F1åˆ†æ•°: {test_metrics['f1']:.4f}")
            print(f"   ç²¾ç¡®ç‡: {test_metrics['precision']:.4f}")
            print(f"   å¬å›ç‡: {test_metrics['recall']:.4f}")
            print(f"   AUC-ROC: {test_metrics['auc_roc']:.4f}")
            print(f"   æœ€ä½³é˜ˆå€¼: {optimal_threshold:.2f}")  # æ·»åŠ è¿™è¡Œæ˜¾ç¤ºæœ€ä½³é˜ˆå€¼
            
        # ä¿®å¤test_metricsæœªå®šä¹‰çš„é—®é¢˜
        if 'test_metrics' not in locals():
            print("âš ï¸ æ¨¡å‹è¯„ä¼°å¤±è´¥ï¼Œåˆ›å»ºé»˜è®¤æŒ‡æ ‡")
            test_metrics = {
                'f1': 0.0, 'precision': 0.0, 'recall': 0.0, 'auc_roc': 0.0,
                'labels': [], 'predictions': [], 'probabilities': [], 'features': []
            }
                    # åœ¨ä¿å­˜æœ€ç»ˆç»“æœçš„ä»£ç ä¹‹åï¼Œtest_metricså’Œtraining_historyéƒ½å·²å®šä¹‰
            
        # åˆå§‹åŒ–å¯è§†åŒ–å™¨
        visualizer = CoreMetricsVisualizer(output_dir=os.path.join(args.output_dir, "visualizations"))
        print("\nğŸ“Š ç”Ÿæˆæ¨¡å‹è¯„ä¼°å¯è§†åŒ–...")
        
        try:
            # ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
            visualizer.plot_training_dashboard(training_history)
            visualizer.plot_f1_score_training(training_history)
            
            # ç”Ÿæˆè¯„ä¼°ç»“æœå¯è§†åŒ–
            if len(test_metrics['labels']) > 0:
                y_true = test_metrics['labels']
                y_pred = test_metrics['predictions']
                y_scores = test_metrics['probabilities']
                features = test_metrics['features']
                
                # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
                visualizer.plot_confusion_matrix(y_true, y_pred)
                
                # åªæœ‰åœ¨æœ‰ä¸¤ä¸ªç±»åˆ«æ—¶æ‰ç”ŸæˆROCå’ŒPRæ›²çº¿
                if len(np.unique(y_true)) > 1:
                    visualizer.plot_roc_curve(y_true, y_scores)
                    visualizer.plot_precision_recall_curve(y_true, y_scores)
                    visualizer.plot_prediction_scores_distribution(y_true, y_scores)
                    
                # å¦‚æœæœ‰ç‰¹å¾æ•°æ®ï¼Œç”Ÿæˆt-SNEå¯è§†åŒ–
                if len(features) > 0:
                    visualizer.plot_tsne_features(features, y_true)
                    
                # ç»˜åˆ¶æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡æ€»ç»“
                visualizer.plot_final_metrics_bar(
                    test_metrics['precision'], 
                    test_metrics['recall'], 
                    test_metrics['f1'], 
                    test_metrics['auc_roc']
                )
                
                # ç»˜åˆ¶å¼‚å¸¸æ£€æµ‹çƒ­å›¾(å¦‚æœæœ‰åŸå§‹æ•°æ®)
                if df_for_point_mapping is not None and 'all_probabilities' in test_metrics:
                    # è·å–åŸå§‹åˆ—æ•°æ®
                    original_data = df_for_point_mapping[final_selected_col].values
                    visualizer.plot_anomaly_heatmap(
                        original_data, 
                        test_metrics['all_probabilities'], 
                        test_window_original_indices, 
                        args.window_size
                    )
                    
                    visualizer.plot_prediction_vs_actual(
                        original_data,
                        test_window_original_indices,
                        test_metrics['labels'],
                        test_metrics['probabilities'],
                        args.window_size
                    )
                    
                # ç»˜åˆ¶æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
                if len(X_test) > 0:
                    sample_idx = np.random.randint(0, len(X_test))
                    sample_data = torch.tensor(X_test[sample_idx], dtype=torch.float32)
                    visualizer.plot_attention_weights(agent, sample_data, device)
                
                print("âœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ!")
            else:
                print("âš ï¸ æµ‹è¯•æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´å¯è§†åŒ–")
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆå¯è§†åŒ–æ—¶å‡ºé”™: {e}")
            traceback.print_exc()
        
        # ç”Ÿæˆå…¨å¥—å¯è§†åŒ–çš„ç»¼åˆå‡½æ•°è°ƒç”¨(ä½œä¸ºå¤‡é€‰)
        try:
            # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬ç”¨äºæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
            sample_data = X_test[0] if len(X_test) > 0 else X_train[0]
            sample_data = torch.tensor(sample_data, dtype=torch.float32)
            
            # ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰æ ¸å¿ƒå¯è§†åŒ–
            visualizer.generate_all_core_visualizations(
                training_history=training_history,
                final_metrics=test_metrics,
                original_data=df_for_point_mapping[final_selected_col].values if df_for_point_mapping is not None else None,
                window_indices=test_window_original_indices,
                window_size=args.window_size,
                agent=agent,
                sample_data=sample_data,
                device=device
            )
            print("âœ… å·²ç”Ÿæˆæ‰€æœ‰æ ¸å¿ƒå¯è§†åŒ–!")
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆç»¼åˆå¯è§†åŒ–æ—¶å‡ºé”™: {e}")    
        # ä¿å­˜æœ€ç»ˆç»“æœ
        with open(os.path.join(args.output_dir, "final_metrics.json"), "w", encoding='utf-8') as f:
            json.dump(test_metrics, f, indent=4, default=convert_to_serializable)
            
        # ...å…¶ä»–ä»£ç ...
    
    except Exception as e:
        print(f"âŒ ä¸»ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        traceback.print_exc()
        
    return 0

if __name__ == "__main__":
    sys.exit(main())